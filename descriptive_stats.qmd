---
editor: visual
---

# Descriptive Statistics {.unnumbered}

## Measures of central tendency

We described in the lecture three main measures of central tendency: the **mean**, the **median** and the **mode**.

### Mean

The mean is described in mathematically as:

$$
\overline{X}=\frac{\sum_{i=1}^{N}{X_i}}{N}
$$

If we have a list of numbers, we could calculate it using the function `sum()` as in

```{r}
x <- c(1,3,5,6,8,2)
sum(x) / 6
```

There is however a built in function `mean()` that does this calculation:

```{r}
mean(x)
```

### Median

Similarly, the median can be calculated by ranking the values and selecting those in the middle, but there is a convenience function `median()`

```{r}
median(x)
```

If we have a dataset with a large outlier, the median is a more robust measurement of central tendency compared to the mean.

```{r}
y <- c(1,3,5,6,8,2,200)
mean(y)
median(y)
```

### Mode

There is no built in function to calculate the mode in R. It is simply the value repeated the most in a dataset. The function `table()` can be used to create a count of the number of elements in a dataframe as a function of variable (so-called *contingency tables*). We will see in the next sections how to do that with tidyverse.

## 5-point summaries

There are several ways to calculate the five point summaries discussed in the lecture for a dataset. Using individual functions:

-   `min()` : minimum of a set of numbers

-   `max()` : maximum of a set of numbers

-   `quantile()` : calculates the quantile based on a threshold.

    -   `quantile(x, prob=0.25)` provides the first quartile Q1

    -   `quantile(x, prob=0.75)` provides the third quartile Q3

    -   `quantile(x, prob=0.5)` corresponds to the median

-   `IQR()` : provides the Inter-Quartile Range, that is equivalent to Q3 - Q1

-   `range()` : difference between maximum and minimum values

-   mean()

-   median()

Taking again the example of the `lexdec` dataset and the Reaction Time `RT`

```{r}
library(languageR)

data(lexdec)

min(lexdec$RT)
max(lexdec$RT)

range(lexdec$RT)
```

```{r}
#| echo: true
quantile(lexdec$RT, prob = 0.25)
quantile(lexdec$RT, prob = 0.75)

#IQR calculated from quantiles
quantile(lexdec$RT, prob = 0.75) - quantile(lexdec$RT, prob = 0.25)

IQR(lexdec$RT)
```

However, most of the values above can be calculated using the convenient `summary()` function

```{r}
summary(lexdec$RT)
```

## Measures of dispersion

### Standard deviation and variance

As described in the lecture the standard deviation of a dataset is calculated as follows:

-   for the full population: $\sigma = \sqrt{\frac{\sum_{i=1}^{N}{(X_i-\mu)^2}}{N}}$

-   for a sample of the population with the un-biased estimator: $s = \sqrt{\frac{\sum_{i=1}^{N}{(X_i-\overline{X})^2}}{N-1}}$

In R, the function `sd()` calculates the [sample standard deviation]{.underline}*.* There is no built in function to calculate the full population $\sigma$ , but in general, for N \> 30 they are very close

```{r}
sd(lexdec$RT)
```

## Data summaries with `tidyverse`

In most cases, we have a most complex data structure where we want to calculate the summary statistics not only globally but in several cases of conditions.

For example, if we want to calculate the average and number of cases of the Reaction Time in the lexdec dataset for Male and Female and also between Native and non-native English speakers. We could perform it by individually filtering each group combination and calculating the mean and number of cases (`length()` provides the number of rows) as below:

```{r}
library(tidyverse)

# Male and Native English Speaker
lexdec_M_english <- lexdec %>% filter( (Sex=="M") & (NativeLanguage=="English"))
mean(lexdec_M_english$RT)
length(lexdec_M_english$RT)


```

And this should be repeated for each of the four groups (Male-English, Male-Other, Female-English, Female-Other)

R has several ways to simplify these calculations. We will look at the approach with tidyverse packages by making use of the functions `group_by()` and `summarize()`.

`group_by()` allows to specify grouping variables that would be applied to the next operation on the pipe. The function `summarize()` allows to create summary variables with the statistics of choice, including mean, sd, IQR, median, min, max, etc... (look at the online help).

The two functions can be combined as in the example below, where we group by Sex and NativeLanguage and ask to create three columns with names `numObs` , `avgRT` and `sdRT` containing the number of observations, mean Reaction Time and Standard Deviation Time respectively.

```{r}
lexdec %>% group_by(Sex, NativeLanguage) %>% 
           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT))
```

When reporting summary descriptive statistics, it is common practice to provide the Standard Error (SE) that is the error on the sampling of the mean, which is calculated as

$$
s_{\overline{X}} = \frac{s}{\sqrt{N}}
$$

We can extend the function above to add a column with the SE calculation, which can be done on the basis of the new columns sdRT and numObs:

```{r}
lexdec %>% group_by(Sex, NativeLanguage) %>% 
           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT), seRT = sdRT / sqrt(numObs))
```

## Reporting Summary Descriptive Statistics

For the reporting of each group of data, either a tabular form or descriptive paragraph can be used, providing the values as calculated. An example for the *`lexdec`* example we used could be as follows:

::: callout-note
*"The logarithmically transformed reaction time of Female English Native Speakers (M = 6.29, SE = 0.01, n = 553) was faster than Male English Native Speakers (M=6.36, SE = 0.01, n=395), which was faster than both Female (M=6.47, SE=0.01, n=553) and Male (M=6.50, SE=0.02, n=158) non-native Speakers."*
:::

Note that the mean is denoted by a capital *M*. In this case we don't have yet a statistical analysis to determine if the difference observed is significant. That should be reported as well and we will address it in the next lectures.

## Data Normality checks

We mentioned in the lectures the importance to be able to check the normality of data or, as we will see of the residuals of a model fit, to ensure that our conclusions based on the statistical assessment of the results of a linear model are valid. In this section we briefly present how to perform in R some of the measures to determine deviation from normality.

### Skewness and Kurtosis

Skewness and Kurtosis of a distribution can be computed using functions in the `moments` package. Before using it, we have to install the package

```{r}
#install.packages("moments")
library(moments)
```

The functions are easily called, `kurtosis()` and `skewness()` . Let's see an example based on the `lexdec` dataset:

```{r}
skewness(lexdec$RT)
```

A positive number implies a right-tailed distribution. If we plot again the histogram for the RT data, this is clearly visible.

```{r}
lexdec %>% ggplot(aes(x=RT)) + geom_histogram(bins = 70)
```

If now we calculate the kurtosis:

```{r}
kurtosis(lexdec$RT)
```

A positive value indicates a distribution that is more concentrated in the center than a normal distribution.

So both measures point to a distribution deviating from normality on the data.

### Q-Q plots

A Q-Q plot or "Quantile-Quantile" Plot, can be used to compare two distributions. It plots the quantiles from the measured data against what a theoretical or other distribution quantile would look like.

A straight line implies that the two distributions compared are very similar. this is done in R using the `qqplot()` function. To test for normality, we want to compare the data against a thoretical normal distribution. This a particular case implemented with the function `qqnorm()` .

If we look in our running example

```{r}
qqnorm(lexdec$RT)
qqline(lexdec$RT)
```

`qqline()` adds a reference line for comparison. Clearly the data deviates from the line noting again the non-normality of the data.

An alternative to the above two commands is to use the `qqPlot()` function in the `car` package that produces charts with an error area more adequate for publication and reporting.

```{r}
library(car)

qqPlot(lexdec$RT)
```

### Shapiro-Wilk tests

Finally, a number of tests can be used to provide a quantified assessment of the normality or not of the data.

Here we present the Shapiro-Wilk test. This test considers a null hypothesis that the data is normally distributed. A significant outcome implies that the null hypothesis is not maintained and that the data is not normally distributed. It is implemented by the `shapiro.test()` function

```{r}
shapiro.test(lexdec$RT)
```

A p-value lower than a defined threshold implies that the null hypothesis is rejected. In this case, p\<0.001 so we will assess the data as not being normally distributed.

## 
