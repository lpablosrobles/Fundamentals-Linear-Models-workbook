---
editor: visual
---

# Linear Model output {.unnumbered}

We start from where we left in workgroup #3, looking at the last section of the output generated by the `lm()` function, that provides the statistical assessment of the [**model as a whole**]{.underline}

![](images/clipboard-3283260128.png){fig-align="center" width="400"}

Let's look at the different elements of the output

## Model fit quality {#sec-modelfit .unnumbered}

We know now how to create linear models fitting the data of interest, but how do we quantify how good is that fit, and if it adequately represents the data?

### $R^2$ coefficient

------------------------------------------------------------------------

As seen in the lecture, the $R^2$ value of a model represents the level of data variance explained by the model and can be expressed as:

$$R^2 = \frac{SS_M}{SS_T}=\frac{SS_T-SS_R}{SS_T} = 1-\frac{SS_R}{SS_T}$$ where:

-   $SS_T$: Total sum of squares (deviation of data points from the data mean)

-   $SS_M$: Model sum of squares (deviation of regression line from the data mean)

-   $SS_R$: Residual sum of squares (deviation of data points from regression line)

As an exercise let's calculate manually the $R^2$ value of the linear model created in Assignment #3. We load the data again and fit the model for the vowel `Duration` as a function of `logFreqs`

```{r}
#| warning: false
library(tidyverse)

dfVowel<-read.csv('./data/extended_vowel_duration.csv')

mod <- lm(Duration~1+logFreqs, data=dfVowel)
summary(mod)
```

To calculate $SS_T$, we just add the squared difference between all of Duration points from their mean value:

```{r}
SS_t <- sum( (dfVowel$Duration - mean(dfVowel$Duration))^2 )
SS_t
```

For $SS_R$, the residuals from the model are available in the object returned by the `lm()` function:

```{r}
SS_r <- sum (mod$residuals^2)
SS_r
```

The resulting value of R2 is then:

```{r}
R2 <- (SS_t - SS_r)/SS_t
R2
```

This is exactly the value returned in the `summary(mod)` output below and labelled [***Multiple R squared***]{.underline}

The value indicates that the model explains 45.5% of the data variance.

::: callout-note
#### **Relationship with Pearson's correlation coefficient (r)**

Note that in the case of a single continuous variable, the $R^2$ value corresponds to the square of the correlation coefficient r:

```{r}
r<-cor(dfVowel$Duration,dfVowel$logFreqs)
r^2
```
:::

#### Adjusted $R^2$

In the output of the model summary function there is another value labelled [**Adjusted R-squared**]{.underline}

![](images/clipboard-3805672897.png){fig-align="center" width="500"}

The $R^2_{Adjusted}$ value is a correction applied to the case where more than one predictor variable is included in the model as we will see in the next lectures. It prevents the $R^2$ value to increase with additional predictors while not improving the model fit. The adjustment is based on the number of data points (*N*) and the number of predictors (*K*):

$$
R^2_{Adjusted} = 1-\left( \frac{SS_R}{SS_T}\times \frac{N-1}{N-K-1}\right)
$$

For the cases we have seen so far with $K=1$, the values are not too different.

------------------------------------------------------------------------

## Model F-statistic

The F-statistic displayed at the end of the output is from the so-called [**F-test for regression**]{.underline}.

![](images/clipboard-2088562208.png){fig-align="center" width="500"}

In essence, the test follows a NHST to identify if any regression slope coefficient (other than the intercept) is different from 0.

Following our general NHST process:

![](images/clipboard-1217780110.png){fig-align="center" width="600"}

-   **Step 1**: Define statistical hypotheses

    The null hypothesis here is that for a model, no slope coefficient is different from 0

    $$
    H_0: b_1=b_2=b_3=...=0 \\\\
    H_a: b_k \neq 0, \text{for at least one value}
    $$

    In the case of single linear regression as we are looking at by now, this simplifies to

$$
H_0: b_1=0 \\\\
H_a: b_1\neq 0
$$

-   Step 2: Define sampling distribution: in an F-test, if $H_0$ is true the ratio of explained and unexplained variance follow an F-distribution.

-   Step 3: F-test statistic can be defined as the ratio of the mean squared errors

$$
F = \frac{MS_{mod}}{MS_{res}}
$$

The mean standard error are calculated from the Sum-squared errors and the degrees of freedom. The degrees of freedom of a model is the number of predictors (*K*), while for the residuals, the degrees of freedom depends on the number of data points (*N*-K-1).

$$
MS_{mod} = \frac{SS_{mod}}{df_{mod}} = \frac{SS_{mod}}{K}
$$

$$
MS_{res} = \frac{SS_{res}}{df_{res}}=\frac{SS_{res}}{N-K-1}
$$

In our example, $N=600, K=1$, and $SS_{mod}$ we can calculate:

```{r}
SS_mod <- SS_t-SS_r
MS_mod <- SS_mod / 1
MS_res <- SS_r / (600 - 1 -1)

F_val <- MS_mod / MS_res
F_val
```

As you can see, the value is the same as in the output from model fit in the figure above.

-   Step 4: determine the critical value. Here the F-distribution in R can be calculated with the function `qf()` (similar to the function `qt()` that we used to determine the critical t-value in the Pearson's coefficient significant testing). Three arguments are required: the probability (1- confidence level, so 1-0.05), and the two degrees of freedom (k, N-k-1), where k is the number of predictors and N the number of samples.

    ```{r}
    F_critical<-qf(1-0.05,1,600-1-1)
    F_critical
    ```

-   Step 5: reach statistical conclusion

The calculated F-value is much higher than the threshold F_critical, therefore, the null hypothesis is rejected and the model is better than the model with only an intercept.

::: callout-important
Note that the example above was intended for you to know where the numbers come from in the R output. You will not have to make the calculations step by step, as the `lm()` function already provides it in the output.
:::
