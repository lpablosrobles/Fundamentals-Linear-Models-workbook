[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Linear Models 2025-2026: Workbook",
    "section": "",
    "text": "Welcome\nThis is the Workbook for the Fundamentals of Linear Models Course\nThis Workbook was designed as a companion to the Workgroup lectures and aims to provide you with basics of statistical computing using the R language programming, an explanation of the exercises we will follow during the class as well as the assignments to be performed after the lecture.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Workbook organization",
    "section": "",
    "text": "The workbook is organized in six chapters to cover the Workgroup sessions.\nFor each workgroup, a number of descriptive sections are included explaining the main concepts and including code examples. The idea is that you read through the description and try the code examples and exercises yourself as you go through in the RStudio environment.\nAt the end of each workgroup section there is an introduction and description of an Assignment, to be started during the class and finished at home (if required).\nAnswer keys to the assignments will be included in the relevant Workgroup section after the due date of each of them.",
    "crumbs": [
      "Workbook organization"
    ]
  },
  {
    "objectID": "Workgroup1.html",
    "href": "Workgroup1.html",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "",
    "text": "üß† Learning Objectives\nIn this session we will familiarize with the basics on the R computing language and the RStudio environment.\nBy the end of this lesson, you will be able to:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-r",
    "href": "Workgroup1.html#what-is-r",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üõ†Ô∏è What is R?",
    "text": "üõ†Ô∏è What is R?\nR is a powerful, open-source programming language designed for statistical computing, data analysis and visualization. It is widely used among statisticians, data analysts, and researchers.\nIt was created by statisticians Ross Ihaka and Robert Gentleman in the early 1990s and has since become a standard tool in academia, research, and industry.\n\nKey Features of R\n\nStatistical Analysis: R allows to perform analysis using a wide range of statistical techniques including linear and nonlinear modeling, time-series analysis, classification, clustering, bayesian methods.\nData Visualization: R allows the creation of high-quality plots and graphics using specific packages (we will explain below what packages are) like ggplot2, lattice, and plotly.\nExtensibility: Thousands of community provided packages are available via CRAN (Comprehensive R Archive Network), covering several fields.\nData Handling: R includes robust tools for importing, cleaning, transforming, and manipulating data.\nCommunity Support: A large and active user community contributes to its development and provides extensive documentation and tutorials.\n\n\n\nInstalling R\nTo work with the programming language, the R interpreter needs to be installed. You can download it from the R homepage, which is:\n\nhttp://cran.r-project.org/\n\nThere are versions available for Windows, Mac and Linux (several distributions). Select the current version (version 4.5.1 at the start of this course) and install it in your personal computer if you want to use it at home.\nR installation includes a basic interface environment with a Console to enter commands and write scripts that can be launched using the R.exe or R.app.\n\nThis interface is nonetheless quite limited and it is not normally used for data analysis and script development. You can use this language through lots of different applications and environments (e.g.¬†VSCode, JupyterLabs, etc..) . For this course we will introduce the most commonly use development environment for R: RStudio.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-rstudio",
    "href": "Workgroup1.html#what-is-rstudio",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üíª What is RStudio?",
    "text": "üíª What is RStudio?\nRStudio is an Integrated Development Environment (IDE) for R, that makes coding in R and management of data analysis projects easier.\nRStudio is a product developed by a company called posit, but that provides a free, open source RStudio Desktop version that can be downloaded here:\n\nhttps://posit.co/download/rstudio-desktop/\n\nAs with R, there are versions available for Windows, Mac and several Linux distributions.\nOnce we launch RStudio, you can distinguish four different areas:\n\nConsole: Where R code is executed, you can type commands and see their output.\nSource: Where you write and save scripts, Notebooks.\nEnvironments: Includes tabs to inspect variables and inspect the command history as well as access tutorials.\nFiles/Plots/Packages/Help/Viewer: Includes tabs for file navigation, plotting, package management, and help.\n\n\n\n\nRStudio Environment (extracted from RStudio User Guide)\n\n\nWe will familiarize with the interface during the exercises in the Workgroup sessions, but you can find a full description and information the following resources provided in the Posit website:\n\nRStudio IDE User Guide",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "href": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üåêUsing R and RStudio on the web",
    "text": "üåêUsing R and RStudio on the web\nIf you don‚Äôt want to install in your own computer, you can use the Posit Cloud environment. that provides a Cloud Free option. This is the approach we will use in this course, so that you can access your work from anywhere.\nAll assignments will be performed in Posit Cloud where I can follow your progress and assist in case of issues.\n\n\nüìù Exercise1: Connect to Posit Cloud\nIn order to use Posit Cloud you need to register for this course following the link below.\nhttps://posit.cloud/spaces/681791/join?access_code=Da9RyPvqyx7Jymq_aHZpKYwBuJJ-45h5bjS1Y7tq\nYou will be directed to the following page. Select the sign-up option and create your account.\n\n\n\nPosit Cloud Sign-in page\n\n\nOnce you sign-in you should see the course project as in the image below:\n\n\n\n\n\nOpen it and explore the RStudio interface.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Commands and operators\nR is an interpreted language, which means that you can write commands to an interpreter in a console that will execute them and return the results. This is in comparison with compiled languages that require to compile the source code to translate it to a machine understandable code.\nIn the RStudio console, you can see the symbol below. This is the command prompt, indicating that the system us ready to execute an instruction or command.\nR syntax is relatively simple and, although daunting if you have never programmed before when you first encounter it, you will quickly get acquainted with it.\nHere‚Äôs a simple example of how R code looks. In the following sections we explain some basic concepts on the syntax and notation.\nBefore going forward, note that lines of code starting with the symbol # are not interpreted. This is use to introduce comments in your code for readability and documentation. We will come to that later when we talk about scripts and notebooks.\nEvery instruction to enter in the command prompt is called a command.\nA simple command is to perform an arithmetic operations like for example:\n1 + 2\n\n[1] 3\nThe command just calculated the addition of the two numbers. In this example, we used the operator + to do so.\nWe include a list below of the basic operators in R, grouped by category. Do not worry if not all are understandable yet:\n1. Arithmetic Operators\nUsed for basic mathematical operations:\n2. Relational (Comparison) Operators\nUsed to compare values:\n3. Logical Operators\nUsed for logical operations (return TRUE/FALSE):\n4. Assignment Operators\nUsed to assign values to variables:\n5. Miscellaneous Operators",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#commands-and-operators",
    "href": "intro_to_r.html#commands-and-operators",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Tip\n\n\n\nCopy the code as you read along in this workbook and try it yourself in RStudio to become familiar with using the tool and environment.\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n+\nAddition\n2 + 3\n\n\n-\nSubtraction\n5 - 2\n\n\n*\nMultiplication\n4 * 3\n\n\n/\nDivision\n10 / 2\n\n\n^ or **\nExponentiation\n2^3 or 2**3\n\n\n%%\nModulus (remainder)\n10 %% 3\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == y\n\n\n!=\nNot equal to\nx != y\n\n\n&gt;\nGreater than\nx &gt; y\n\n\n&lt;\nLess than\nx &lt; y\n\n\n&gt;=\nGreater than or equal to\nx &gt;= y\n\n\n&lt;=\nLess than or equal to\nx &lt;= y\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&\nElement-wise AND\nx &gt; 1 & x &lt; 5\n\n\n|\nElement-wise OR\nx &lt; 1 | x &gt; 5\n\n\n!\nNOT\n!TRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\nComment\n\n\n\n\n&lt;-\nPreferred assignment\nx &lt;- 5\nMost commonly used\n\n\n=\nAlternative assignment\nx = 5\nUsed in the assignment of values to function arguments (see function section below)\n\n\n-&gt;\nAssign right to left\n5 -&gt; x\nAlthough syntactically valid in R, not used often\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n%in%\nMembership test\n3 %in% c(1, 2, 3) will return TRUE\n\n\n:\nSequence\n1:5 returns 1 2 3 4 5",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#variables",
    "href": "intro_to_r.html#variables",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Variables",
    "text": "Variables\nA variable is a name that stores a value or data object. You can think of it as a labeled container that holds information you want to use or manipulate in your program.\nIn R, you assign values to variables mostly using the operator &lt;- .\n\nx &lt;- 5       # Assign 5 to variable x  \ny &lt;- 10      # Assign 10 to variable y  \nz &lt;- x + y   # Add x and y  \nprint(z)     # Print the result \n\n[1] 15\n\n\nVariables naming in R can be anything, but follow a few rules:\n\nMust start with a letter\nCan contain letters, numbers, underscores (_) or periods (.)\nAre case-sensitive (Name and name are different)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#data-types",
    "href": "intro_to_r.html#data-types",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Data types",
    "text": "Data types\n\nBasic types\nR supports several basic data types. Some of the most common are:\n\nNumeric: x &lt;- 3.14\nInteger: x &lt;- 5L (note the L)\nCharacter: name &lt;- \"Leticia\"\nLogical: flag &lt;- TRUE\n\n\n\nVectors and lists\nVectors are the most basic data structure in R. They are a groups of values built using the combine function,¬†c(). For example,¬†c(1, 2, 3, 4)¬†creates a four element series of positive integer values\n\nnumbers &lt;- c(1, 2, 3, 4)\nnumbers\n\n[1] 1 2 3 4\n\n\nYou can also perform operations on vectors.\n\nnumbers^2\n\n[1]  1  4  9 16\n\n\n\n\nDataframes\nstructures can be thought of as sets of data organized in a table format in rows and columns. They can be created using the dataframe() function.\n\ndf &lt;- data.frame(\n  SubjectID = c(\"S1\", \"S2\",\"S3\"),\n  age = c(25, 30, 28)\n)\ndf\n\n  SubjectID age\n1        S1  25\n2        S2  30\n3        S3  28\n\n\nYou can access the individual columns on a dataframe using the $ operator. Try to start typing the code below on the console. You will see that R provides suggested completions, displaying the available columns in the dataframe.\n\ndf$SubjectID\n\n[1] \"S1\" \"S2\" \"S3\"\n\n\nMany of the functions we will use in this course require a dataframe as an input or produce one as output, so it is the data structure you will use the most.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#par-functions",
    "href": "intro_to_r.html#par-functions",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Functions",
    "text": "Functions\nFunctions perform tasks in R. They take in inputs called arguments and return outputs. They are called using parentheses. For example, the function mean() in R calculates the mean of the elements we provide as input.\n\n# We can specify a list of numbers\nmean(c(1,2,3,4))\n\n[1] 2.5\n\n\nOr, more useful, we can provide a variable containing values:\n\n# we defined the variable numbers before\nmean(numbers)\n\n[1] 2.5\n\n\nThe parameters of a function and normally called arguments. You can either manually specify a function‚Äôs arguments or use the function‚Äôs default values. In the examples above mean() and sum() are simple functions with not many arguments, but this is not normally the case.\nTo know the arguments of a function you can use the R help. There are two ways to access the help pages for a function:\n\nUse the operator ? followed by the function name in the console, or use the help() function.\nFor example, type the instruction ?mean in the console. The help page for the mean() function will open in the RStudio Help panel as in the image below:\n\nYou can of course directly open the Help tab and search for the function of interest.\n\nAs you can see above, mean() actually has two other arguments, trim and na.rm . The arguments have a default value, so if we don‚Äôt explicitly include them in the function call, they will use that value. Let‚Äôs look at an example using na.rm :\nR has a special value called NA , which means ‚ÄúNot Available‚Äù and it is used to represent missing values on the data. In experimental work is often the case that some data point is lost or corrupted and we have incomplete datasets. Let‚Äôs assume you had performed an online experiment that computed the reaction time in miliseconds of 10 participants, and one value was not available as per the vector below:\n\nrt &lt;- c(234.2, 127.5, 256.2, NA, 287.1, 145.6, 358.9, 200.1, 398.3, 178.3)\n\nLet‚Äôs try to calculate the average reaction time of your data using mean():\n\nmean(rt)\n\n[1] NA\n\n\nThe function tries to calculate the average, but when one value is not available (NA) the result is also NA . In the help in the image below we see there is an argument na.rm that we can use to ignore the missing elements in the data:\n\nmean(rt, na.rm = TRUE)\n\n[1] 242.9111\n\n\nNow the function worked and summed all the numbers and divided them by 9, ignoring the missing data, in the calculation of the average.\nWe‚Äôll work with functions a lot throughout this book and you‚Äôll get lots of practice in understanding their behaviors, so don‚Äôt panic.\nFinally an advanced note to make you aware that you can define your own functions. The following code defines a new function called add() that, well, adds two numbers:\n\nadd &lt;- function(a, b) {\n  return(a + b)\n}\n\nOnce you have defined the function, you can use it as any other in R:\n\nadd(3.5, 2.5)\n\n[1] 6",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#factors",
    "href": "intro_to_r.html#factors",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Factors",
    "text": "Factors\nIn¬†R, a¬†factor¬†is a data structure used to represent¬†categorical data. Categorical data consists of variables that have a fixed number of unique values, known as¬†levels. These are typically used for any variable that classifies observations into groups. We will use factors extensively in the analysis of data\nWe convert a vector into a factor by using the factor() function. Let‚Äôs look at one example.\n\n\nüìù Exercise 2: Creating a dataframe with factors\nWe want to create a dataset of six words, collecting data of the animacy, gender, length and frequency of the word.\nFirst, we create individual variables with vectors including the values\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\") \nNounAnimacy \n\n[1] \"animate\"   \"inanimate\" \"inanimate\" \"animate\"   \"animate\"   \"animate\"  \n\n\n\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\") \nNounGender\n\n[1] \"feminine\"  \"masculine\" \"neuter\"    \"masculine\" \"feminine\"  \"feminine\" \n\n\n\nNounLength&lt;-c(6,7,4,5,8,6) \nNounLength \n\n[1] 6 7 4 5 8 6\n\nNounFrequency&lt;-c(638,799,390,569,567,665) \nNounFrequency\n\n[1] 638 799 390 569 567 665\n\n\nAs you can see from the output above, data in the variables NounAnimacy and NounGender are considered as words, or literal strings. The next step is to indicate they are factors.\n\nNounAnimacy&lt;- factor(NounAnimacy) \nNounAnimacy \n\n[1] animate   inanimate inanimate animate   animate   animate  \nLevels: animate inanimate\n\nNounGender&lt;- factor(NounGender) \nNounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter\n\n\nAs you can see by the output produced, now the variables are considered as factors, and Levels indicate the unique values that the each takes.\nWith the variables above, we can now create a dataframe.\n\nDataexample&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency) \nDataexample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nA few useful functions when working with dataframes:\n\nnrow() : returns the number of rows of a dataframe. Normally corresponding to observations in experimental data files.\nncol(): returns the number of columns of a dataframe.\nhead() : displays the first rows of a dataframe or vector. By default, it displays the first 6 items, but you can specify how many rows to show with the argument n . Useful when displaying on the screen large datasets.\nView(): Opens the dataset in a excel-like data viewer in RStudio. Same can be done clicking the name of the variable in the Environment window in RStudio.\nstr() : can be used to display the structure of the dataframe.\ncolnames(): returns the names of the columns/variables in a dataframe.\n\nLet‚Äôs look at a few examples with the dataframe we just created.\nFirst we can inspect what is the dataframe structure:\n\nstr(Dataexample)\n\n'data.frame':   6 obs. of  4 variables:\n $ NounAnimacy  : Factor w/ 2 levels \"animate\",\"inanimate\": 1 2 2 1 1 1\n $ NounGender   : Factor w/ 3 levels \"feminine\",\"masculine\",..: 1 2 3 2 1 1\n $ NounLength   : num  6 7 4 5 8 6\n $ NounFrequency: num  638 799 390 569 567 665\n\n\nAs you can see, the output tells us that the dataframe is composed of four variables (columns), two with categorical values and two with numerical elements and provides the names of those columns. It tells us also that it contains 6 observations (rows). You can obtain also the information above using:\n\nnrow(Dataexample)\n\n[1] 6\n\nncol(Dataexample)\n\n[1] 4\n\ncolnames(Dataexample)\n\n[1] \"NounAnimacy\"   \"NounGender\"    \"NounLength\"    \"NounFrequency\"\n\n\nIf we want to display the first 4 observations in the dataset, you could use:\n\nhead(Dataexample, n = 4)\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#additional-reading-material",
    "href": "intro_to_r.html#additional-reading-material",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Additional Reading material",
    "text": "Additional Reading material\nBasics on R programming:\n\nChapter 1, Introduction to R, from Bodo Winter‚Äôs book ((Winter 2019)).\nSections 2 to 8 on R basic Tutorial by UMC Utrecht\nA good starters reference for R is the book ‚ÄúR for Data Science‚Äù ((Wickham, √áetinkaya-Rundel, and Grolemund 2024)) ). The book is available online freely at R for Data Science\nExcellent eBook to learn R Basics ((Grolemund 2014)) Hands On Programming with R\n\nDataframes:\n\nYouTube Videos by DataCamp:\n\nhttps://www.youtube.com/watch?v=9f2g7RN5N0I\nhttps://youtu.be/Nh6tSD4i4qs?feature=shared\n\n\n\n\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "using_libraries.html",
    "href": "using_libraries.html",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Installing and using a Package\nOne of the big benefits of R in comparison with other statistics packages is its open nature. The functionality is easily extended by groups all around the world by developing libraries that can be easily installed and used.\nR packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet.\nA package is a collection of R functions, data, and compiled code bundled together. Packages are created by the R community and cover a wide range of topics like data manipulation, visualization, machine learning, etc.\nPackages have to be installed and loaded before using them.\nWe need to install packages only once in our environment. That can be done in two ways:\nFor example, in several exercises in this book we use a library (created by for the book (Baayen 2008)) that contains some utilities and a few sample datasets with linguistics examples.\nLet‚Äôs install the library using the following command:\nAfter installing the package, the contents will still not be available until you load the package, this has to be done in every new work session and it is done with the library() function.\nNow you can access the functions and data in the languageR package.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#installing-and-using-a-package",
    "href": "using_libraries.html#installing-and-using-a-package",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Using the install.packages() function or\nUsing the Packages tab in RStudio:\n\n\n\n\ninstall.packages(\"languageR\")\n\n\n\n\n\n\nWarning\n\n\n\nThe installation of a package can give an error if you try to install a package that is already installed and loaded. If you get an error saying that you have already the package, just cancel.\n\n\n\nlibrary(languageR)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#using-datasets-available-in-packages",
    "href": "using_libraries.html#using-datasets-available-in-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Using datasets available in packages",
    "text": "Using datasets available in packages\n\nAs mentioned, the main objective of a package is to distribute functions, but it is often convenient to include example datasets that can be used to illustrate the use of the functions. In other cases, there are packages that are use mainly to distribute data. The later is the case for example for the languageR package, provided as a companion to the book Baayen (2008) that we will use in some of the examples and assignments in this course.\nYou can explore data sets available from all loaded packages using the data() function. If you want the data from a specific package, specified with the argument package .\nTake a look at the packages datasets included in languageR using the following command.\ndata(package=\"languageR\")\nYou will see that a new tab opens in the Editor area with the content:\n\nIf you want more information more information of a particular dataset, you can use the help operator ? as with any function.\nFor example in today‚Äôs assignment we will use the lexdec dataset containing Lexical decision latencies collected from a group of speakers. Run the following command to read the composition of the dataframe.\n?lexdec",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#useful-packages",
    "href": "using_libraries.html#useful-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Useful packages",
    "text": "Useful packages\n\nThere are by now thousands of community contributed packages available for R and the list grows by the day (see complete list in CRAN website).\nIn practice you will use only a few packages on your data analysis tasks. I list below a number of commonly used packages for further reference (note we will only use a few of those in this course and I will always indicate it in the specific sessions or assignments).\n\nPackages for data input and output\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\nRead Excel files\nPart of tidyverse\n\n\nreadr\nRead information in tabular format from CSV and TAB separated files\nPart of tidyverse\n\n\n\n\n\nPackages for data analysis\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\n\n\n\n\n\n\n\n\n\n\n\n\nPackages for data visualization and manipulation\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nggplot2\nLibrary to create graphics and data visualizations\nPart of tidyverse\n\n\ndplyr\nData manipulation functions\nPart of tidyverse\n\n\ntidyr\nFunctions to transform data from wide to long format.\nPart of tidyverse\n\n\nforcats\nFunctions to modify factors levels and ordering\nPart of tidyverse\n\n\n\nWe will explore functions from the packages above in the Workgroups 1 & 2\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "read_data.html",
    "href": "read_data.html",
    "title": "Reading and Saving data in R",
    "section": "",
    "text": "Working directory and paths in R\nWe have seen how to access datasets contained in packages, but to analyze your own data you will need to load it in the RStudio environment.\nWe will look in what follows at a few examples of loading data in different common formats (text files in general, excel files, files in SPSS format, R files).\nBefore entering into the details on the read and write of files, it is important to understand where the files are located and how to provide paths in R.\nPaths for files in R are relative to the ‚ÄúWorking Directory‚Äù. To know which is the working directory, you can use the function getwd() or select the option ‚ÄúGo to working directory‚Äù in the files tab.\nThe paths of a file are relative to that directory. Considering this, in the examples and assignments in this course, we will load data in the /data folder as follows:\n'./data/FILE_NAME'\nThe ./ in the path above indicates the working directory.\nWhen running commands loading or saving files, take care that you indicate the path correctly if you get an error.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "read_data.html#sec-r-read",
    "href": "read_data.html#sec-r-read",
    "title": "Reading and Saving data in R",
    "section": "Reading data from files",
    "text": "Reading data from files\n\nWe cover below examples for importing data in R, covering the most common methods and packages.\n\nData from text files\nThe most generic function to read data from a text file is read.table() . The arguments allow to define if the data includes a header (i.e.¬†a first row with the names of the columns/variables) and the separator used.\nFor example, let‚Äôs load a file in the /data directory containing the sample dataset from the previous exercise using the function below:\n\ndata_sample &lt;- read.table(\"./data/sample_wg1_text.txt\")\ndata_sample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\n\n\nData from CSV files\nMany experimental data is saved in a text or CSV (Comma separated) file. The best method to read data from these files is to use the read_csv() function in the readr package.\nLet‚Äôs see an example of usage loading the file in the /data directorate ‚ÄúELP_full_length_frequency.csv‚Äù with data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1.\n\nlibrary(readr)\n\ndata_sample_csv &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(data_sample_csv)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\n\n\nAs you can see, the file was loaded with 33,057 observations!\nTo free up memory, let‚Äôs remove the variable from the workspace using the rm() function (remove):\n\nrm(data_sample_csv)\n\nIf your data is tab-separated, you can use the function read_tsv() . By default, the functions expect that the first row contains the names of the columns/variables to be read. If that is not the case, you should modify the argument col_names = FALSE .\n\n\nData from Excel files\nThe best way to read Excel files is using the readxl package. If your file has several worksheets, you can use the sheet argument to specify either an index or the name of the worksheet to read.\n\n# load library\nlibrary(readxl)\n\n# Using readxl\ndata_sample_excel &lt;- read_excel(\"./data/sample_wg1_excel.xlsx\", sheet = 1)\ndata_sample_excel\n\n# A tibble: 6 √ó 4\n  NounAnimacy NounGender NounLength NounFrequency\n  &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 animate     feminine            6           638\n2 inanimate   masculine           7           799\n3 inanimate   neuter              4           390\n4 animate     masculine           5           569\n5 animate     feminine            8           567\n6 animate     feminine            6           665\n\n\n\n\nData in R files\nData can also be saved and loaded in R format directly. This is useful if you are performing your analysis in R to save data intermediate steps as it is a compact and efficient format, even though to share with other researchers and in open access journals you should export it to a more generic format.\nTo illustrate again let‚Äôs load a file in the /data directory containing a sample real dataset from a Event Related Potential (ERP) study that we will use in some of the exercises in the course:\n\ndata_sample_r &lt;- readRDS(\"./data/eegSampleData.Rda\")\nhead(data_sample_r)\n\n       time Subject Condition       Fp1       Fpz       Fp2       AF3      AF4\n1 -0.200000       1     CondA 1.9596113 1.6237565 1.4964751 1.9707278 2.232514\n2 -0.198999       1     CondA 1.7536104 1.4013030 1.3669174 1.7740067 2.084792\n3 -0.197998       1     CondA 1.5272095 1.1631496 1.2091597 1.5532856 1.912671\n4 -0.196997       1     CondA 1.2834086 0.9148962 1.0254020 1.3115644 1.718250\n5 -0.195996       1     CondA 1.0260077 0.6621427 0.8183443 1.0523433 1.504029\n6 -0.194995       1     CondA 0.7589068 0.4100893 0.5914866 0.7800222 1.273008\n          F7          F5        F3        F1        Fz        F2        F4\n1  0.9002055  0.97041392 1.6839135 1.8026822 1.7678664 1.8301742 1.8249546\n2  0.7315209  0.80466543 1.5461834 1.6655317 1.6480326 1.7056625 1.6921552\n3  0.5449364  0.62021693 1.3802533 1.4985812 1.4956989 1.5476508 1.5259558\n4  0.3422518  0.41866843 1.1874231 1.3030307 1.3121651 1.3574391 1.3284565\n5  0.1255673  0.20221994 0.9697930 1.0812802 1.0997314 1.1373274 1.1027571\n6 -0.1025173 -0.02662856 0.7301629 0.8365297 0.8614976 0.8909157 0.8531577\n         F6        F8         FT7       FC5       FC3       FC1       FCz\n1 1.6807566 1.6442472  0.79175532 1.0834129 1.1766302 1.2950722 1.6606659\n2 1.5340938 1.4931450  0.62251420 0.9396381 1.0489977 1.1714287 1.5515010\n3 1.3574309 1.3124428  0.43667307 0.7727634 0.8951652 1.0198851 1.4089360\n4 1.1532681 1.1046406  0.23643195 0.5843886 0.7166327 0.8417415 1.2341711\n5 0.9250052 0.8733384  0.02489083 0.3771138 0.5154001 0.6391979 1.0294061\n6 0.6768424 0.6229362 -0.19425030 0.1538391 0.2946676 0.4151543 0.7976412\n        FC2      FC4       FC6       FT8         T7          C5        C3\n1 1.7052435 1.828386 1.6453833 1.5302436  0.6388098  0.82305823 1.1149519\n2 1.6106100 1.725298 1.5067077 1.3838023  0.4772181  0.66489982 1.0142226\n3 1.4794766 1.593309 1.3373322 1.2091609  0.2951264  0.48534141 0.8858932\n4 1.3130431 1.433921 1.1395566 1.0086196  0.0949347  0.28668300 0.7310638\n5 1.1138097 1.249232 0.9165811 0.7854783 -0.1200570  0.07182459 0.5512344\n6 0.8853762 1.042444 0.6724056 0.5438370 -0.3464487 -0.15573382 0.3492050\n         C1         Cz        C2        C4        C6        T8        TP7\n1 1.1306227 0.63096230 1.3182707 1.4574666 1.4729369 1.3580312 -0.1544125\n2 1.0314048 0.57397554 1.2505935 1.3564641 1.3762993 1.2646492 -0.2040742\n3 0.9031868 0.48648877 1.1480163 1.2251617 1.2511616 1.1442672 -0.2693359\n4 0.7472689 0.36920200 1.0112391 1.0652592 1.0988239 0.9982852 -0.3505976\n5 0.5654510 0.22341524 0.8417619 0.8787567 0.9212863 0.8285032 -0.4477593\n6 0.3606331 0.05132847 0.6424847 0.6691542 0.7215486 0.6379212 -0.5599209\n         CP5       CP3         CP1        CPz        CP2       CP4       CP6\n1 0.79635392 0.7767967  0.48835529 0.58856683 0.50392506 0.8973196 0.8844379\n2 0.69718724 0.6999467  0.42408573 0.53621461 0.46937197 0.8189115 0.8011065\n3 0.57452056 0.5954966  0.33161617 0.45326240 0.40481887 0.7127035 0.6903752\n4 0.42925389 0.4640466  0.21144661 0.33981018 0.31056577 0.5798955 0.5537438\n5 0.26278721 0.3069966  0.06507705 0.19715797 0.18771268 0.4227875 0.3934124\n6 0.07762053 0.1267466 -0.10529251 0.02740575 0.03845958 0.2442795 0.2123810\n        TP8          P7         P5          P3          P1          Pz\n1 0.9127783  0.44240355  0.5012748  0.50060479  0.43724552  0.44452279\n2 0.8270859  0.38499520  0.4463851  0.43989524  0.38284579  0.39195007\n3 0.7139935  0.30168685  0.3652955  0.35238568  0.30064606  0.30987735\n4 0.5753011  0.19277851  0.2583059  0.23837612  0.19084634  0.19880464\n5 0.4131088  0.05907016  0.1263162  0.09886657  0.05454661  0.05993192\n6 0.2306164 -0.09753819 -0.0289734 -0.06424299 -0.10655312 -0.10434080\n            P2        P4        P6        P8         PO7         PO5\n1  0.468759124 0.8508531 0.8558191 0.8671508  0.35547460  0.36233623\n2  0.432358313 0.7741135 0.7808388 0.7798563  0.29344668  0.29959166\n3  0.365657503 0.6698739 0.6783584 0.6649618  0.20641876  0.21124709\n4  0.268956693 0.5394343 0.5496781 0.5242673  0.09449085  0.09760252\n5  0.143555882 0.3848946 0.3967978 0.3598728 -0.04133707 -0.04054205\n6 -0.008344928 0.2089550 0.2223175 0.1750783 -0.19966498 -0.20118662\n          PO3         POz        PO4         PO6         PO8        CB1\n1  0.28232490 -0.03600213 -0.7460777  0.52166620  0.49329201 -0.2268531\n2  0.22316096 -0.06933023 -0.7465746  0.45118577  0.42236878 -0.2837141\n3  0.13819702 -0.13065833 -0.7646715  0.35510534  0.32644555 -0.3584751\n4  0.02763308 -0.21928642 -0.8008684  0.23462491  0.20662231 -0.4507361\n5 -0.10743085 -0.33401452 -0.8546652  0.09154448  0.06479908 -0.5599971\n6 -0.26529479 -0.47284262 -0.9249621 -0.07133595 -0.09642415 -0.6844582\n          O1         Oz         O2        CB2\n1 -0.2783048 -0.6701305 -0.8214062 -0.8811375\n2 -0.3294788 -0.6538060 -0.8241543 -0.8791028\n3 -0.4000527 -0.6546815 -0.8441024 -0.8940680\n4 -0.4900266 -0.6739570 -0.8817505 -0.9264333\n5 -0.5987006 -0.7125325 -0.9365985 -0.9760985\n6 -0.7245745 -0.7706080 -1.0072466 -1.0419638\n\n\n\n\nData from SPSS\nThe haven package can be used to read directly .sav files from SPSS.\nAs an example again, let‚Äôs open a file saved in SPSS from another ERP (Event Related Potential) study of code-switching in Dutch.\n\n# If not yet available, install the package using install.packages(\"haven\")\n\n# Load library\nlibrary(haven)\n\ndata_sample_spss &lt;- read_sav(\"./data/EEG_DataSet_LongFormat_pablos.sav\")\nhead(data_sample_spss)\n\n# A tibble: 6 √ó 6\n  Participant Language CS    Congruency Electrode Amplitude\n        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n1           7 Dutch    no_CS congruent  Fp1           0.712\n2           7 Dutch    no_CS congruent  AF3           0.313\n3           7 Dutch    no_CS congruent  F7            0.982\n4           7 Dutch    no_CS congruent  F3            0.587\n5           7 Dutch    no_CS congruent  FC1           0.711\n6           7 Dutch    no_CS congruent  FC5           2.83 \n\n\nAgain to preserve memory, let‚Äôs clean the workspace by removing the variables we created using :\n\nrm(data_sample_spss, data_sample_r)\n\n\n\nOther formats\nIn your own research, if you have data in other specific formats, you may have to develop your own function to read it, although it is highly likely there is already a solution available. Always search first in the internet‚Ä¶ For example, imagine you have data in a file saved in the software Matlab, typing ‚Äúread Matlab files in R‚Äù in any search engine points to the R package R.matlab that offers the functions readMat() and writeMat() to read and write respectively Matlab files.\nA few examples of useful packages for data reading in linguistics research are listed below:\n\n\n\nPackage\nDescription\n\n\n\n\neegUtils\nUtilities for Electroencephalographic (EEG) Analysis:\nIncludes import functions for EEG files from several EEG acquisition and analysis software suites: ‚ÄòBioSemi‚Äô (.BDF), ‚ÄòNeuroscan‚Äô (.CNT), ‚ÄòBrain Vision Analyzer‚Äô (.VHDR), ‚ÄòEEGLAB‚Äô (.set) and ‚ÄòFieldtrip‚Äô (.mat)\n\n\nrprime\nPackage for parsing¬†.txt¬†generated by E-Prime, a program for running psychological experiments.\nSupport functions to read and clean data created in E-Prime\n\n\nchildesr\nPackage to access data in the childes-db, an open database of child language datasets from the CHILDES (Child Language Data Exchange System) data bank.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "tidyverse_intro.html",
    "href": "tidyverse_intro.html",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "üåê What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. These packages share an underlying philosophy, grammar, and data structures, making it easier to learn and use them together. Tidyverse simplifies tasks like data manipulation, visualization, and modeling.\nA full description of the tidyverse packages and functions is available in https://dplyr.tidyverse.org/",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#what-is-the-tidyverse",
    "href": "tidyverse_intro.html#what-is-the-tidyverse",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "Why Use tidyverse?\nUsing the functions in tidyverse have a number of advantages versus the use of individual packages or the available base R functions:\n\nConsistent and readable syntax.\nPipe operator (%&gt;%) for chaining operations (see later)\nDesigned for tidy data principles as described in Wickham, √áetinkaya-Rundel, and Grolemund (2024).\nIt has a strong user community and documentation.\n\n\n\nüì¶ Core tidyverse Packages\nWhen you install and load the tidyverse package, you get access to the core packages listed in the table below without needing to load them individually\n\n\n\nPackage\nUsage\n\n\n\n\nggplot2\nData visualization\n\n\ndplyr\nData manipulation\n\n\ntidyr\nData tidying\n\n\nreadr\nReading rectangular data (CSV, etc.)\n\n\npurrr\nFunctional programming\n\n\ntibble\nModern data frames\n\n\nstringr\nString manipulation\n\n\nforcats\nWorking with categorical data (factors)\n\n\n\nWe have already seen readr when loading files. We will explore today the tidyr functions.\n\n\nWorkflow Using tidyverse\nA generic workflow using tidyverse can be represented as follows:\n\n\n\n\n\nflowchart LR\n  load[\"Load tidyverse\"]\n  read[\"Read data\"]\n  clean[\"Clean and transform data\"]\n  plot[\"Visualize\"]\n  load --&gt; read\n  read --&gt; clean\n  clean --&gt; plot\n\n\n\n\n\n\nThis flow result in a code as the following example:\n\n\n\n\n\n\nImportant\n\n\n\nYou are not expected to understand all the code below now. We will explain different elements during the course. It is just intended as an example of the workflow.\n\n\n\n# Load tidyverse\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read data\n\ndf &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\", show_col_types = FALSE)\n\n# filter, clean and transform data\n\ncleandata &lt;- df %&gt;%\n  filter(length&gt;3) %&gt;%\n  mutate(rt_per_character = RT / length)\n\n# Visualize\n\nggplot(cleandata, aes(x = Log10Freq, y = RT)) + geom_point(color='grey') + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "href": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "A grammar of data manipulation",
    "text": "A grammar of data manipulation\n\ndplyr is a package within the tidyverse set of functions that allow to manipulate data. You can think of the functions in the package as a sort of ‚Äúgrammar of data manipulation‚Äù, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\nmutate() adds new variables that are functions of existing variables -&gt; operates on columns.\nrelocate(): moves variables to a different position (change order of columns) -&gt; operates on columns.\nselect() picks variables based on their names. -&gt; operates on columns.\nrename(): change variable names -&gt; operates on columns\narrange() changes the ordering of the rows. -&gt; operates on rows.\nfilter() picks cases based on their values. -&gt; operates on rows.\nsummarise() reduces multiple values down to a single summary (we will look at this in next workgroup).\n\n\n\n\n\n\n\nNote\n\n\n\nYou will see that the functions in tidyverse libraries return a so called tibble . The details are beyond the scope of this course, but you can think of a tibble as a version of a data frame.\n\n\nLet‚Äôs explore how to use the functions above with another of the datasets contained in the languageR package by Baayen (2008). We will use the DurationsOnt dataset that contains durational measurement of the Dutch prefix -ont from a study on a Spoken Dutch Corpus ( Pluymaekers, Ernestus, and Baayen (2005)).\n\n#load library\nlibrary(languageR)\n\n#load dataset in the environment\ndata(\"durationsOnt\")\n\n#display the structure of the dataset\nstr(durationsOnt)\n\n'data.frame':   102 obs. of  12 variables:\n $ Word                 : Factor w/ 102 levels \"ontbeten\",\"ontbijt\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Frequency            : num  3.09 4.84 0 3.76 1.95 ...\n $ Speaker              : Factor w/ 63 levels \"N01001\",\"N01002\",..: 44 15 53 55 46 42 25 25 63 4 ...\n $ Sex                  : Factor w/ 2 levels \"female\",\"male\": 1 1 2 1 2 1 2 2 2 1 ...\n $ YearOfBirth          : num  72 80 52 60 74 70 76 76 76 66 ...\n $ DurationOfPrefix     : num  0.113 0.1 0.14 0.161 0.161 ...\n $ DurationPrefixVowel  : num  0.0695 0.0354 0.0474 0.0959 0.064 ...\n $ DurationPrefixNasal  : num  0.0439 0.0651 0.0925 0.0648 0.0973 ...\n $ DurationPrefixPlosive: num  0 0 0 0 0 0 0 0 0 0 ...\n $ NumberOfSegmentsOnset: int  1 1 1 1 1 1 1 1 2 2 ...\n $ PlosivePresent       : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SpeechRate           : num  3.92 6.88 3.53 5.39 6.73 ...\n\n#look at the description of the variables\n?durationsOnt\n\nstarting httpd help server ... done\n\n\n\nSelecting variables: select()\n\nThe function select() allows to choose a subset of variables (columns of interest). The function call includes as first parameter the dataset followed by the list of columns you would like to keep.\nFor this example we would like to look only at the duration of the Prefix in seconds and are not interested in the other variables.\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word, Frequency, Speaker, Sex, YearOfBirth, DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\nThe new dataframe durationsOnt_simplified contains now only the columns we specified.\n\n\n\n\n\n\nTip\n\n\n\nIf you have many columns, there is a simpler way to specify ‚Äúkeep from column X to column Y‚Äù without having to list each one of the individually by using : as in the example below that provides the same result. Of course this does not work if you want to select non-contiguous columns.\n\n\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\n\n\nChanging variable names: rename()\n\nYou can change the naming of the variable using rename as follows - You call the function with as first parameter the dataset and after with the list of variables to be renamed.\\\n\n\n\n\n\n\nWarning\n\n\n\nNote that you write first the new name and then the old one.\n\n\n\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\nhead(durationsOnt_simplified_renamed)\n\n             Word Frequency Speaker Gender YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\n\n\nMoving variable position: relocate()\n\nWith big datasets including many variables (columns), sometimes it is useful to change the order of the columns. This can be done with the relocate() function.\nYou call the function providing again as first parameter the dataset and then the column you want to move. By default the function will move the column specified to become the first. Let‚Äôs say that we want to have the Speaker Identifier as the first column, we will use the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker           Word Frequency Gender YearOfBirth DurationOfPrefix\n24  N01143       ontbeten 3.0910425 female          72         0.113372\n58  N01041        ontbijt 4.8441871 female          80         0.100478\n40  N01157  ontbijtbuffet 0.0000000   male          52         0.139806\n42  N01162      ontbijten 3.7612001 female          60         0.160739\n60  N01145      ontbijtje 1.9459101   male          74         0.161283\n21  N01134 ontbijtservies 0.6931472 female          70         0.176658\n\n\nThe Speaker variable is now the first. If we want to define a specific location, we can use the arguments .after or .before . For instance, if we want to move the Gender to be just after the Speaker column we will call the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_reordered, Gender, .after = Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker Gender           Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female       ontbeten 3.0910425          72         0.113372\n58  N01041 female        ontbijt 4.8441871          80         0.100478\n40  N01157   male  ontbijtbuffet 0.0000000          52         0.139806\n42  N01162 female      ontbijten 3.7612001          60         0.160739\n60  N01145   male      ontbijtje 1.9459101          74         0.161283\n21  N01134 female ontbijtservies 0.6931472          70         0.176658\n\n\nNow that we have selected the variables that we want and in the order that we want them, let‚Äôs filter the data to select some cases\n\n\nSelecting cases: filter()\n\nWe can select cases or observations based on a criteria using the filter() function. The filter function is called providing as a first parameter the dataset, followed by a condition or criteria to use for the selection.\nLet‚Äôs say that we want to make an analysis on the duration of people born after 1970\n\ndurationsOnt_reordered_filtered_age &lt;- filter(durationsOnt_reordered,YearOfBirth &gt; 70)\n\n#check on the minimum value of the Year of Birth with the function min()\nmin(durationsOnt_reordered$YearOfBirth)\n\n[1] 23\n\nmin(durationsOnt_reordered_filtered_age$YearOfBirth)\n\n[1] 71\n\n\nAs you can see on the example, the filter() selected the cases based on the condition specified.\nIf we wanted to select for example only the cases for Females, we will have used the filter as Gender == \"female\" . What about if we want to select the females born after 1970 in a single step? You can combine conditions using the operators & (meaning ‚Äúand‚Äù ) and | (meaning ‚Äúor‚Äù). For example:\n\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\nhead(durationsOnt_reordered_filtered_age_gender)\n\n   Speaker Gender      Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female  ontbeten  3.091042          72         0.113372\n58  N01041 female   ontbijt  4.844187          80         0.100478\n46  N01103 female   onthoud  3.433987          77         0.168078\n43  N01089 female  onthulde  2.079442          75         0.165369\n55  N01065 female onthullen  1.945910          78         0.185638\n78  N01096 female  ontkende  2.833213          78         0.186745\n\n\n\n\nReordering cases: arrange()\nFinally, the arrange() function orders the dataset by a selected value. For example to order the dataframe by Frequency:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, Frequency)\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n    Speaker Gender             Word Frequency YearOfBirth DurationOfPrefix\n102  N01037 female ontstekingsvocht 0.0000000          81         0.099240\n20   N01051 female       ontmaskerd 0.6931472          79         0.190070\n93   N01171 female    ontploffingen 1.3862944          76         0.114520\n32   N01051 female      ontmaskeren 1.6094379          79         0.143948\n88   N01110 female        ontvoeren 1.6094379          77         0.098185\n55   N01065 female        onthullen 1.9459101          78         0.185638\n\n\nWhat about if we want to use a reverse order? This can be done use the desc() modified on the variable to be used:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#pipe-operator-or",
    "href": "tidyverse_intro.html#pipe-operator-or",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "Pipe operator (%>% or |> )",
    "text": "Pipe operator (%&gt;% or |&gt; )\n\nAs you can see, in all the examples above we pass as first argument to the function the dataframe on which we want to make the operation.\nPutting all the calls before together :\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nIn this examples I created on every step a new variable with a different name to save the output of the functions, which created a lot of new variables. This might be useful only in cases when you want to save the intermediate steps. In reality this is rarely the case since anyway you can always execute again the complete flow if you want to change something, and it is a better practice.\nThat being the case, we could just use on variable for example called durationsOnt_processed that we overwrite in every step, resulting in something like this:\ndurationsOnt_processed &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_processed &lt;- rename(durationsOnt_processed, Gender = Sex)\ndurationsOnt_processed &lt;- relocate(durationsOnt_processed, Speaker)\ndurationsOnt_processed &lt;- filter(durationsOnt_processed,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_processed &lt;- arrange(durationsOnt_processed, desc(Frequency))\nIn this process though, still everytime the output of the one call is the input for the next step (in the end it is an analysis workflow).\nIf we were to write textually what we did to the data would be something like:\n\n‚ÄúTake the durationsOnt dataset then\nSelect columns ‚Ä¶ then\nrename column Sex to Gender then\nmove column Speaker to the first column then\nmove column Gender after Speaker then\nselect cases of Females born after 1970 then\narrange by frequency in descending order.‚Äù\n\nThis is where the concept of a pipe was introduced in R. The pipe operator allows you to¬†pass the result of one function directly into the next function¬†as its first argument, without the need to explicitly write it. It‚Äôs widely used in the¬†tidyverse, especially with¬†dplyr.\nThe operator can be used with two syntax %&gt;% or the new |&gt; .\nUsing this, the examples before could be executed as below.\n\ndurationsOnt_processed &lt;- durationsOnt %&gt;% select(Word:DurationOfPrefix) %&gt;%\n                          rename(Gender = Sex) %&gt;%\n                          relocate(Speaker) %&gt;%\n                          relocate(Gender, .after = Speaker) %&gt;%\n                          filter((YearOfBirth &gt; 70) & (Gender==\"female\")) %&gt;%\n                          arrange(desc(Frequency))\n\nhead(durationsOnt_processed)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997\n\n\nAs you can see, the result is the case, but the code and data processing flow is much more readable, and there is no need to introduce intermediate variables.\nYou will become more familiar along the course with the basic usage of the tidyverse data manipulation.s\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005. ‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù The Journal of the Acoustical Society of America 118 (4): 2561‚Äì69.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "using_notebooks.html",
    "href": "using_notebooks.html",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "",
    "text": "R Scripts\nUp to now we have described the usage of R and RStudio as an interactive data analysis environment where we introduce commands and get outputs on the Console.\nAn important part of research and data analysis is being able to reproduce the analysis and work you produce and report.\nIn order to that, reproducible research includes:\nScripts are used to collect the commands and steps used in a data analysis. In the R language, a script is a text file with commands saved with extension .R.\nTo create a script, select File -&gt; New File -&gt; R Script\nThis will open a file in the Editor where you can type a series of commands and comments. Comments are lines starting with # .\nYou can execute the code you want to run highlighting it and pressing¬†Ctrl + Enter¬†(Windows) or¬†Cmd + Enter¬†(Mac) or with the Green ‚ÄúRun‚Äù button on the top right of the Editor window.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#r-markdown-and-notebooks",
    "href": "using_notebooks.html#r-markdown-and-notebooks",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "R Markdown and Notebooks",
    "text": "R Markdown and Notebooks\nAn alternative to scripts are Notebooks, which are a powerful way of documenting work by producing documents that mix plain text and code.¬†\nNotebooks include chunks of code that can be executed and the output displayed and included in the document, together with textual input formatted using R Markdown language. In fact the current workbook is written using this approach.\nA full description of R Markdown is beyond the scope of this course, but also not required to follow the content and exercises. In the section below a basic introduction is given to create a Notebook, use the Visual editor and generate PDF output to hand-in your assignments.\n\nNotebook instructions\n\n\n\n\n\n\nCaution\n\n\n\nBeware that the environment of a Notebook is not the same as the R session environment!\n\n\nTo create a notebook in R Markdown, you can select File -&gt; New File -&gt; R Notebook\n\nTo show the capabilities of the Notebook, we will look at the assignments to be delivered:\n\nOpen the file Assignment1.Rmd by clicking on the file on the name in the file tab.\nIf not active, select ‚ÄúVisual‚Äù in the edit mode on the upper left.\nEnter your name and Student ID in the author field.\n\n\n\nEnter the code to answer each of the questions in the relevant code section, that appear in grey and with a {r} marking.\nAfter entering the code, run it pressing the green arrow on the top right corner of the code section\nRemember to save from time to time!\n\n\nWhen you are ready to hand in your assignment you can generate a PDF to upload in Brightspace. This is the procedure to do it:\n\nClick on the small arrow next to the Knit button (see figure below).\nSelect ‚ÄúKnit to PDF‚Äù\nSave the PDF file generated somewhere in your drive or computer.\nUpload it to Brightspace on the relevant assignment.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#additional-reading",
    "href": "using_notebooks.html#additional-reading",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nIf you are interested into knowing more on the capabilities of R notebooks, you can use this resources.\n\n\n\n\n\n\nImportant\n\n\n\nThis material is for your own development but it is not required for the course. I add it here as additional information in case you need to use it in your own research\n\n\n\nR Notebooks guide.\nDatacamp R Notebook tutorial",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "assignment1_key.html",
    "href": "assignment1_key.html",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "Task#1: Creating a dataframe\nYou were asked to create a dataframe made up sample data according to the following prescription:\nThis can be done exactly as in the example we followed in Workgroup 1, with the steps replicated below:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task1-creating-a-dataframe",
    "href": "assignment1_key.html#task1-creating-a-dataframe",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "4 columns/variables: one column with a factor with two levels, another column with a factor with 3 levels and two columns with numeric values.\n6 rows or observations\n\n\n\n1.1. Create variables with the data.\nAs you were asked to create 6 observations, every variable contains six entries.\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\")\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\")\nNounLength&lt;-c(6,7,4,5,8,6)\nNounFrequency&lt;-c(638,799,390,569,567,665) \n\n\n\n1.2. Create factors\nThe variables created above with categorical entries are considered as character vectors. We should convert them into factors using the function factor() .\n\nNounAnimacy &lt;- factor(NounAnimacy)\nNounGender &lt;- factor(NounGender)\n\n\n\n\n\n\n\nTip\n\n\n\nThe following could be performed in a single step by nesting functions as in the code below:\nNounAnimacy &lt;- factor(c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\"))\nNounGender&lt;- factor(c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\"))\n\n\n\n\n1.3. Create a dataframe\nWe can define a dataframe based on the variables we created as:\n\ndf_example&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency)\ndf_example\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nIf you look at the Environment window, a new variable was created called example with the specified contents:\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that when creating the dataset using the data.frame() function it builds a copy of the data. Modifying the original variables will not change df_example . You can remove the variables to clean up the workspace using rm()\n\nrm(NounAnimacy,NounFrequency, NounGender, NounLength)\n\n\n\nOnly df_example is left in the environment now:\n\nand you can access the individual columns in the dataframe using the $ operator. For example to see the column NounGender\n\ndf_example$NounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task2-loading-required-libraries",
    "href": "assignment1_key.html#task2-loading-required-libraries",
    "title": "Assignment #1 - Answer key",
    "section": "Task#2: loading required libraries",
    "text": "Task#2: loading required libraries\nLooking at the content of the assignment tasks, we will use data from the languageR package and functions from the dyplr package, which is part of the tidyverse environment.\nTo use both of them we have to load the libraries first\n\n# add code to load the required libraries\nlibrary(languageR)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "href": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "title": "Assignment #1 - Answer key",
    "section": "Task#3: Working with and exploring dataframes",
    "text": "Task#3: Working with and exploring dataframes\nFrom the languageR library a sample dataframe called lexdec is used in this assignment. This dataset contains lexical decision latencies elicited from 21 subjects for 79 English concrete nouns.\n\n3.1. Load dataset in the environment\nTo use the lexdec dataset, we load it in the environment using the function data() . Although this step is not needed to access the data, it conveniently includes is as any other variable in the Environment window, so that it is possible to inspect it.\n\ndata(lexdec)\n\n\nYou can browse the information on the dataset in the documentation included with the package. You can do that with the help() function, using ? or searching in the help tab.\nhelp(lexdec)\n?lexdec\nThe help tab shows the description of each of the variables in the dataset\n\n\n\n3.2. Explore contents of the dataset\nYou were asked to show the first rows of the dataset.\nTyping the name of the dataset will show you the full contents, which is not too handy. Instead you can use View(lexdec) to load it in the RStudio Viewer, or, just to inspect he first rows, you can use the head() function as follows:\n\nhead(lexdec)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect\n1      A1 6.340359    23   F        English correct     word     correct\n2      A1 6.308098    27   F        English correct  nonword     correct\n3      A1 6.349139    29   F        English correct  nonword     correct\n4      A1 6.186209    30   F        English correct     word     correct\n5      A1 6.025866    32   F        English correct  nonword     correct\n6      A1 6.180017    33   F        English correct     word     correct\n        Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1        owl  4.859812  1.3862944   0.6931472      3 animal           54\n2       mole  4.605170  1.0986123   1.9459101      4 animal           69\n3     cherry  4.997212  0.6931472   1.6094379      6  plant           83\n4       pear  4.727388  0.0000000   1.0986123      4  plant           44\n5        dog  7.667626  3.1354942   2.0794415      3 animal         1233\n6 blackberry  4.060443  0.6931472   1.3862944     10  plant           26\n  FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1         74       0.7912 simplex -0.3101549 6.3582     3.12   3.4758\n2         30       0.6968 simplex  0.8145080 6.4150     2.40   2.9999\n3         49       0.4754 simplex  0.5187938 6.3426     3.88   1.6278\n4         68       0.0000 simplex -0.4274440 6.3353     4.52   1.9908\n5        828       1.2129 simplex  0.3977961 6.2956     6.04   4.6429\n6         31       0.3492 complex -0.1698990 6.3959     3.28   1.5831\n  meanWeight      BNCw      BNCc       BNCd BNCcRatio BNCdRatio\n1     3.1806 12.057065  0.000000   6.175602  0.000000  0.512198\n2     2.6112  5.738806  4.062251   2.850278  0.707856  0.496667\n3     1.2081  5.716520  3.249801  12.588727  0.568493  2.202166\n4     1.6114  2.050370  1.462410   7.363218  0.713242  3.591166\n5     4.5167 74.838494 50.859385 241.561040  0.679589  3.227765\n6     1.1365  1.270338  0.162490   1.187616  0.127911  0.934882\n\n\nAs can be seen from the output, by default it displays 6 rows. Looking at the documentation of the head() function, it describes that the function can take an argument n specifying how many rows to display.\n\nhead(lexdec, n = 4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency FamilySize SynsetCount Length  Class FreqSingular FreqPlural\n1  4.859812  1.3862944   0.6931472      3 animal           54         74\n2  4.605170  1.0986123   1.9459101      4 animal           69         30\n3  4.997212  0.6931472   1.6094379      6  plant           83         49\n4  4.727388  0.0000000   1.0986123      4  plant           44         68\n  DerivEntropy Complex      rInfl meanRT SubjFreq meanSize meanWeight      BNCw\n1       0.7912 simplex -0.3101549 6.3582     3.12   3.4758     3.1806 12.057065\n2       0.6968 simplex  0.8145080 6.4150     2.40   2.9999     2.6112  5.738806\n3       0.4754 simplex  0.5187938 6.3426     3.88   1.6278     1.2081  5.716520\n4       0.0000 simplex -0.4274440 6.3353     4.52   1.9908     1.6114  2.050370\n      BNCc      BNCd BNCcRatio BNCdRatio\n1 0.000000  6.175602  0.000000  0.512198\n2 4.062251  2.850278  0.707856  0.496667\n3 3.249801 12.588727  0.568493  2.202166\n4 1.462410  7.363218  0.713242  3.591166\n\n\n\n\n3.3 Extract the column names from the dataframe.\nAs per the workgroup notes, the names of the variables in the dataframe can be extracted using the colnames() function.\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n\n\n3.4. Sort dataframe\nYou were asked to sort the dataframe by the reaction time (RT variable). Sorting can be done in several ways, but we will use the tidyverse arrange() function as described in the workgroup.\n\nlexdec_ordered &lt;- arrange(lexdec,RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\nTo instead arrange in reducing order:\n\nlexdec_ordered &lt;- arrange(lexdec, desc(RT))\nhead(lexdec_ordered, n = 4)\n\n     Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n1194      T2 7.587311    44   F          Other incorrect  nonword     correct\n1619      M2 7.443664   105   F          Other   correct     word     correct\n1381      R1 7.425358   116   F        English   correct  nonword     correct\n1620      M2 7.403670   106   F          Other   correct     word     correct\n         Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1194  gherkin  2.079442          0    1.098612      7  plant            4\n1619     leek  3.332205          0    1.098612      4  plant            5\n1381 beetroot  3.555348          0    1.098612      8  plant           15\n1620 hedgehog  3.637586          0    1.098612      8 animal           21\n     FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1194          3            0 simplex  0.2231435 6.5161     3.16   2.2484\n1619         22            0 simplex -1.3437348 6.4200     3.68   1.8898\n1381         19            0 complex -0.2231435 6.4468     3.88   1.8705\n1620         16            0 simplex  0.2578291 6.5924     3.32   3.5920\n     meanWeight     BNCw    BNCc     BNCd BNCcRatio BNCdRatio\n1194     1.8185 0.111433 0.16249 0.237523  1.458184  2.131531\n1619     1.4590 0.846892 0.16249 2.375231  0.191866  2.804646\n1381     1.4442 0.635169 0.16249 0.475046  0.255822  0.747906\n1620     3.2545 2.718969 0.64996 3.325324  0.239047  1.223009\n\n\nAs an example, the previous can also be done using pipes. It does not make much difference in this task but it makes the code more readable when you perform several steps:\n\nlexdec_ordered &lt;- lexdec %&gt;% arrange(RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that on these examples, we saved the output in a new variable. If we were just calling the function as:\narrange(lexdec, RT)\nit would display on the console the output which is rather long and not easy to see.\n\n\nWe now have a new variable in the environment called lexdec_ordered containing a copy of the data from lexdec ordered by the value of RT .\n\n\n3.5. Select columns and rows\nYou were asked to create a new dataframe called lexdec_reduced with only the variables from Subject to Frequency and selecting only the entries from native English speakers (as coded in the NativeLanguage variable).\nAs described in the workgroup materials, two functions are used to select the data, one selecting the variables/columns of interest (select()) and another selecting the observations/rows based on a criteria (filter())\nLet‚Äôs see below how to do the same operation with and without using pipes.\n\n# Without pipes\n\nlexdec_reduced &lt;- select(lexdec,Subject:Frequency)\nhead(lexdec_reduced, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith this first command, we have selected a few columns from the original dataset. We can check that using for example colnames()\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n\ncolnames(lexdec_reduced)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"     \n\n\nAs you can see only the first columns (from Subject to Frequency) are retained in lexdec_reduced\nTo now select only the English native speakers, we use the function filter() :\n\nlexdec_filtered &lt;- filter(lexdec_reduced,NativeLanguage==\"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nTo check we can use the function unique() that tells us how many unique values are in a variable. The output below shows that NativeLanguage is a factor with two levels and both are present in the dataframe lexdec_reduced.\n\nunique(lexdec_reduced$NativeLanguage)\n\n[1] English Other  \nLevels: English Other\n\n\nIf we looked at the lexdec_filtered dataframe, it specifies that the variable is still a factor with two levels, but only ‚ÄúEnglish‚Äù is present in the data.\n\nunique(lexdec_filtered$NativeLanguage)\n\n[1] English\nLevels: English Other\n\n\nThe steps above could be performed using pipes:\n\n#Using pipes\n\nlexdec_filtered &lt;- lexdec %&gt;% \n                  select(Subject:Frequency) %&gt;% \n                  filter(NativeLanguage == \"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith identical results and a more readable code.\n\n\n3.6. Save dataframe\nFinally you were asked to save the filtered dataframe into a file in the /data directorate.\nLet‚Äôs save the data for example in text format using the write.table() function:\n\nwrite.table(lexdec_reduced,file = \"./data/reduced_data.txt\")\n\nThis command creates a file in the /data directory called reduced_data.txt\n\nWe can also save a copy in R format using saveRDS() if using R for further analysis.\n\nsaveRDS(lexdec_reduced,\"./data/reduced_data.Rda\")\n\n\n\nAnd this concludes the first assignment of the course! You will be gaining familiarity with R and RStudio along the workgroups.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "Workgroup2.html",
    "href": "Workgroup2.html",
    "title": "Workgroup 2: Data Exploration with R",
    "section": "",
    "text": "In this session we will continue to introduce basic concepts of data manipulation and plotting using the tidyverse package and start performing data exploration and descriptive statistics with R.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nPerform basic data transformation and reorganization.\nUnderstand concept of grammar of graphics and create basic plots.\nPerform basic descriptive statistics in R.\nCreate histograms and boxplot.\nCheck datasets for normality.\nWrite dataset summaries.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R"
    ]
  },
  {
    "objectID": "data_organization.html",
    "href": "data_organization.html",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Adding columns: mutate()\nAs mentioned in the previous workgroup, it is not in the scope of the course to cover all the functions available in the tidyverse framework. We introduce a subset of them useful to illustrate the concepts in the course.\nThe mutate() function, part of the dplyr package, allows to create new columns or variables in a dataset. It is used normally for values based in an existing column.\nAs an example, let‚Äôs use again the data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1. Dataset is in the file /ELP_full_length_frequency.csv in the /data directory.\nWe load the data as indicated in workgroup1\nlibrary(readr)\n\ndfELP &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(dfELP)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\nLet‚Äôs say we want to add two new calculated values:\nWe can add the two columns using mutate() as follows\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî purrr     1.1.0\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndfELP_extended &lt;- dfELP %&gt;% mutate(RT_per_char = RT/length, logRT = log10(RT))\nhead(dfELP_extended)\n\n# A tibble: 6 √ó 6\n  Word   Log10Freq length    RT RT_per_char logRT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.        151.  2.88\n2 zephyr      1.70      4  875.        219.  2.94\n3 zeroed      1.30      5  929.        186.  2.97\n4 zeros       1.70      5  625.        125.  2.80\n5 zest        1.54      4  659.        165.  2.82\n6 zigzag      1.36      6  785.        131.  2.90",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#adding-columns-mutate",
    "href": "data_organization.html#adding-columns-mutate",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Reading time per character: calculated dividing RT by length\nlogarithmic Reading Time : calculated using the log10() function\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTo avoid filling up the memory in the system, we will remove the variables after completing the examples when not using them again for a while\n\nrm(dfELP, dfELP_extended)",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#data-organization-long-and-wide-format",
    "href": "data_organization.html#data-organization-long-and-wide-format",
    "title": "Data organization and transformation",
    "section": "Data organization: long and wide format",
    "text": "Data organization: long and wide format\nA common task when preparing the analysis steps is to reshape the data in a way that can be used for the analysis.\nWe can widely characterize the data into two groups wide and long. (see section 3.9.4 of Field (2026) for details)\n\n\n\nFrom A. Field\n\n\nThe tidyr package contains two useful functions to allow to transform the data from one format to the other: pivot_wider() and pivot_longer() .\nAgain let‚Äôs look at one example based on data from an Event Related Potential (ERP) experiment. In ERP analysis one common approach is to compare the average amplitude of the signal measured in several electrodes on a time window of interest for the ERP component expected. The file EEG_DataSet_Wide.Rda in the /data directory contains an example data set with the average voltage measured in a set of electrodes in a time window of 250-500ms to investigate the so-called N400 ERP component.\nLet‚Äôs load and look at the data:\n\ndfEEG_wide &lt;- readRDS(\"./data/EEG_DataSet_Wide.Rda\")\nhead(dfEEG_wide, n=8)\n\n# A tibble: 8 √ó 64\n# Groups:   Subject [2]\n  Subject Condition    Fp1     Fpz    Fp2     AF3    AF4     F7     F5     F3\n  &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1       CondA     -3.05  -1.04   -2.88  -3.06   -3.31  -2.05  -2.06  -2.89 \n2 1       CondB     -1.03  -1.57   -0.842 -1.84   -0.361 -1.08  -1.06  -0.158\n3 1       CondC      0.575  1.52   -0.198  0.818   0.235  0.200  0.226  1.04 \n4 1       CondD     -2.20  -1.02   -1.63  -2.75   -1.90  -1.63  -1.62  -1.79 \n5 2       CondA      0.655  0.251   0.133  0.606   0.190  0.503  0.741  0.762\n6 2       CondB     -1.80  -2.13   -2.31  -2.33   -2.45  -1.23  -2.12  -2.12 \n7 2       CondC      0.145  0.0108  0.122  0.0548 -0.172  0.216  0.226  0.180\n8 2       CondD     -1.38  -1.66   -1.74  -0.980  -1.43  -0.797 -0.963 -0.944\n# ‚Ñπ 54 more variables: F1 &lt;dbl&gt;, Fz &lt;dbl&gt;, F2 &lt;dbl&gt;, F4 &lt;dbl&gt;, F6 &lt;dbl&gt;,\n#   F8 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FC5 &lt;dbl&gt;, FC3 &lt;dbl&gt;, FC1 &lt;dbl&gt;, FCz &lt;dbl&gt;, FC2 &lt;dbl&gt;,\n#   FC4 &lt;dbl&gt;, FC6 &lt;dbl&gt;, FT8 &lt;dbl&gt;, T7 &lt;dbl&gt;, C5 &lt;dbl&gt;, C3 &lt;dbl&gt;, C1 &lt;dbl&gt;,\n#   Cz &lt;dbl&gt;, C2 &lt;dbl&gt;, C4 &lt;dbl&gt;, C6 &lt;dbl&gt;, T8 &lt;dbl&gt;, TP7 &lt;dbl&gt;, CP5 &lt;dbl&gt;,\n#   CP3 &lt;dbl&gt;, CP1 &lt;dbl&gt;, CPz &lt;dbl&gt;, CP2 &lt;dbl&gt;, CP4 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   TP8 &lt;dbl&gt;, P7 &lt;dbl&gt;, P5 &lt;dbl&gt;, P3 &lt;dbl&gt;, P1 &lt;dbl&gt;, Pz &lt;dbl&gt;, P2 &lt;dbl&gt;,\n#   P4 &lt;dbl&gt;, P6 &lt;dbl&gt;, P8 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO5 &lt;dbl&gt;, PO3 &lt;dbl&gt;, POz &lt;dbl&gt;, ‚Ä¶\n\n\nThe table contains 64 columns, with one row representing the measurements on a particular condition per subject.\n\nColumn 1 - ‚ÄúSubject‚Äù - ID of the participant\nColumn 2 - ‚ÄúCondition‚Äù - factor representing the condition with four levels (CondA, CondB, CondC, CondD).\nColumns 3-64: Average Voltages in 250-500 ms time window after stimuli onset at 61 electrodes.\n\nFor the data analysis, we would like to actually see the effect on the Voltage measurement at different electrode sites, so we would like to actually have the data organized as:\n\n\n\nSubject\nCondition\nElectrode\nAmplitude\n\n\n\n\n1\nCondA\nFp1\n-3.047\n\n\n1\nCondA\nFpz\n-1.037\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nWe can do this with the pivot_longer() function, that has the following syntax:\npivot_longer(data, cols, names_to = \"name\", values_to = \"value\")\n\ndata : dataframe to be transformed\ncols: columns to be converted into longer format\nnames_to: name of the variable that will contain as values the names of the cols argument\nvalues_to: name of the variable that will contain values\n\nIn our example, we want to pivot all columns with electrode name. Normally you could provide a list, but since here we have several columns we can specify a range: ‚Äúfrom column Fp1 to CB2‚Äù\n\ndfEEG_long &lt;- pivot_longer(dfEEG_wide,cols = Fp1:CB2, names_to = \"Electrode\",values_to= \"AvgVoltage\") \nhead(dfEEG_long,n=8)\n\n# A tibble: 8 √ó 4\n# Groups:   Subject [1]\n  Subject Condition Electrode AvgVoltage\n  &lt;fct&gt;   &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 1       CondA     Fp1            -3.05\n2 1       CondA     Fpz            -1.04\n3 1       CondA     Fp2            -2.88\n4 1       CondA     AF3            -3.06\n5 1       CondA     AF4            -3.31\n6 1       CondA     F7             -2.05\n7 1       CondA     F5             -2.06\n8 1       CondA     F3             -2.89\n\n\nAs can be seen from the output, we have the data in the format expected\nTransforming into wide format is done similarly with the pivot_wider() function, that takes the following arguments:\n\ndata : dataframe to be transformed\nnames_from : column (or columns) to get the name of the output column\nvalues_from : column (or columns) to get the cell values from\n\nLet‚Äôs imagine that we would like to transform our last dataframe into a table of the form:\n\n\n\nSubject\nElectrode\nCondA\nCondB\nCondC\nCondD\n\n\n\n\n1\nFp1\n\n\n\n\n\n\n1\nFpz\n\n\n\n\n\n\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\nWe can do the following:\n\ndfEEG_wide_2 &lt;- pivot_wider(dfEEG_long,names_from = \"Condition\",values_from= \"AvgVoltage\") \nhead(dfEEG_wide_2,n=8)\n\n# A tibble: 8 √ó 6\n# Groups:   Subject [1]\n  Subject Electrode CondA  CondB  CondC CondD\n  &lt;fct&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1       Fp1       -3.05 -1.03   0.575 -2.20\n2 1       Fpz       -1.04 -1.57   1.52  -1.02\n3 1       Fp2       -2.88 -0.842 -0.198 -1.63\n4 1       AF3       -3.06 -1.84   0.818 -2.75\n5 1       AF4       -3.31 -0.361  0.235 -1.90\n6 1       F7        -2.05 -1.08   0.200 -1.63\n7 1       F5        -2.06 -1.06   0.226 -1.62\n8 1       F3        -2.89 -0.158  1.04  -1.79\n\n\n\n\n\n\n\n\nNote\n\n\n\nPivoting tables can be confusing at first. You will master it with practice on your own data. This section is included to illustrate a typical data manipulation and provide tips for your own analysis but will not be used in exercises or exams.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#further-reading",
    "href": "data_organization.html#further-reading",
    "title": "Data organization and transformation",
    "section": "Further reading",
    "text": "Further reading\nIf you want to go more into details here are links to good resources:\n\nR for data Science ( Wickham, √áetinkaya-Rundel, and Grolemund (2024)) Chapter 5 - Data tyding\nTidyverse article on Pivoting\n\n\n\n\n\nField, Andy P. 2026. Discovering Statistics Using R and RStudio. London: SAGE Publications.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "üìäData visualization in R",
    "section": "",
    "text": "Introduction to ggplot2\nR provides several ways to plot data, from simple plots using basic functions like plot() to complex visualization with specialized packages.\nFor this course we have chosen to introduce plotting based on the ggplot2 package, part of the tidyverse environment because of its versatility to generate almost any required visualization in the linguistics field. A full treatment of the capabilities of ggplot is beyond the scope of this course and you are not expected to know how to use it for the exams, but will need it for the assignments.\nFor a full description refer to Wickham (2016), available online.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#grammar-of-graphics",
    "href": "data_visualization.html#grammar-of-graphics",
    "title": "üìäData visualization in R",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\n\nThe name, ggplot, it is due to the underlying concept based on the notion of Grammar of Graphics ( Wilkinson (2011))\nThe concept behind ggplot2 divides a plot into different fundamental composable parts:\n\n\n\nImage extracted from ggplot introduction\n\n\nThe main components are\nPlot = Data + Mapping + Layers (or Geometry)\n\nData is a data frame with the source of the data to be represented.\nMapping is used to indicate with of the data elements are mapped to the aesthetics of the graph (i.e.¬†the x and y axis. It can also be used to control the color/ size / shape of points, the height of bars, etc.\nGeometry defines the type of graphics (i.e., histogram, box plot, line plot, density plot, dot plot, etc.).\n\nTo explain these concepts, let‚Äôs walk through an example of the steps to build a plot. We will make use again of the lexdec dataset in the languageR package.\n\nlibrary(languageR)\n\ndata(\"lexdec\")\n\nTo build a plot, first we have to indicate the data that will be used to ggplot\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec)\n\n\n\n\n\n\n\n\nThis command has created an empty canvas, since we have not mapped the data to the plot. We have to provide a mapping of which variables are to be displayed. We do this mapping with the aes() function. Let‚Äôs plot the Reading Time as a function of the Frequency of the word.\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT))\n\n\n\n\n\n\n\n\nNow we get a plot, with axis indicating the two elements we have mapped, but nothing is displayed. This is because we have not specified the geometry or representation.\nIf we want to make a scatter plot, we use the geom_point() function:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\n\nThe basic geometries in ggplot are:\n\ngeom_point() : scatter plot\ngeom_line() : line plot\ngeom_bar() : bar chart\ngeom_histogram() : histogram\ngeom_boxplot() : boxplot\n\nAdditional variables beyond x and y, can be considered using the colour or fill arguments. For instance, in the example above, we could color the points differently according to the NativeLanguage of the subject in the observation:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT, color=NativeLanguage)) + geom_point()\n\n\n\n\n\n\n\n\nIn the following we look at a couple of plots that we addressed in the first lectures",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#histograms",
    "href": "data_visualization.html#histograms",
    "title": "üìäData visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a representation of the frequency distribution of the data. It is created using the geom_histogram() function. Note that for this plot, only one variable is needed to be specified for the x-axis, since the other coordinate is the count of cases.\nFor example, to look at the distribution of the reaction time data:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can see that the data is only slightly right skewed, since it corresponds to a log transformed Reaction Time.\nIn a histogram, we can adjust the arguments bins and binwidth to determine the resolution of our grouping. The default is 30 bins distributed equally between the min and maximum value. Let‚Äôs see what happens if we change the value to 70:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nLet‚Äôs illustrate another capability of the layered concept to build graphs in ggplot: if we were interested to see if the distribution of reaction times is different between males and females, we could filter the data for each of the groups and plot two histograms (let‚Äôs do this using pipes this time)\n\n#histogram for females\n\nlexdec %&gt;% filter(Sex==\"F\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n#histogram for males\nlexdec %&gt;% filter(Sex==\"M\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nHowever we can also do it using facets in ggplot, which allows us to create multiple graphs based on a given variable. See the example below:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram() + facet_wrap(~Sex)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nNow both plots are places next to each other and, more importantly, with the same scale on the axis. By looking at the graphs, the distributions are similar, but the Female is higher, pointing to the fact that there were likely more female participants than males (remember the y-axis in a histogram is a count).\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying\n\n\n\n\n\n\nNote\n\n\n\nAdvanced:\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying in plot_histogram() a specific aesthetic aes(y =..density..)\n\nggplot(lexdec, aes(x=RT)) + geom_histogram( aes( y =..density..)) + facet_wrap(~Sex)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#boxplot",
    "href": "data_visualization.html#boxplot",
    "title": "üìäData visualization in R",
    "section": "Boxplot",
    "text": "Boxplot\n\nAnother representation of the distribution of the data that we discussed in the lecture is the boxplot. Boxplots can be plotted in ggplot using the geom_boxplot() function.\nIf we look again at the RT as a function of Sex can simply change the call as follows:\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot() \n\n\n\n\n\n\n\n\nOne observation is that the plot shows a number of points (circles) beyond the end of the ‚Äôwhisker‚Äô lines. If we look back at our definition in the lecture for the boxplot:\n\nThe box plot representation was based on the 5-point summary (min, max, median, Q1 and Q3) with the whiskers representing the min and max values.\nSoftware packages however, have a different implementation. By default geom_boxplot() places whisker edges at 1.5 times the Inter Quartile Range (IQR).\n\nYou can change the default behaviour by using the coef argument. For example, the call below extends the whiskers to 2*IQR.\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot( coef = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways check the manual of a function if you are not sure what is calculating/plotting.\nIn this case, looking at the help of page of geom_boxplot() you will find the following:\n‚ÄúThe upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‚Äúoutlying‚Äù points and are plotted individually‚Äù\n\n\n\n\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In Handbook of Computational Statistics: Concepts and Methods, 375‚Äì414. Springer.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Measures of central tendency\nWe described in the lecture three main measures of central tendency: the mean, the median and the mode.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-central-tendency",
    "href": "descriptive_stats.html#measures-of-central-tendency",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Mean\nThe mean is described in mathematically as:\n\\[\n\\overline{X}=\\frac{\\sum_{i=1}^{N}{X_i}}{N}\n\\]\nIf we have a list of numbers, we could calculate it using the function sum() as in\n\nx &lt;- c(1,3,5,6,8,2)\nsum(x) / 6\n\n[1] 4.166667\n\n\nThere is however a built in function mean() that does this calculation:\n\nmean(x)\n\n[1] 4.166667\n\n\n\n\nMedian\nSimilarly, the median can be calculated by ranking the values and selecting those in the middle, but there is a convenience function median()\n\nmedian(x)\n\n[1] 4\n\n\nIf we have a dataset with a large outlier, the median is a more robust measurement of central tendency compared to the mean.\n\ny &lt;- c(1,3,5,6,8,2,200)\nmean(y)\n\n[1] 32.14286\n\nmedian(y)\n\n[1] 5\n\n\n\n\nMode\nThere is no built in function to calculate the mode in R. It is simply the value repeated the most in a dataset. The function table() can be used to create a count of the number of elements in a dataframe as a function of variable (so-called contingency tables). We will see in the next sections how to do that with tidyverse.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#point-summaries",
    "href": "descriptive_stats.html#point-summaries",
    "title": "Descriptive Statistics",
    "section": "5-point summaries",
    "text": "5-point summaries\nThere are several ways to calculate the five point summaries discussed in the lecture for a dataset. Using individual functions:\n\nmin() : minimum of a set of numbers\nmax() : maximum of a set of numbers\nquantile() : calculates the quantile based on a threshold.\n\nquantile(x, prob=0.25) provides the first quartile Q1\nquantile(x, prob=0.75) provides the third quartile Q3\nquantile(x, prob=0.5) corresponds to the median\n\nIQR() : provides the Inter-Quartile Range, that is equivalent to Q3 - Q1\nrange() : difference between maximum and minimum values\nmean()\nmedian()\n\nTaking again the example of the lexdec dataset and the Reaction Time RT\n\nlibrary(languageR)\n\ndata(lexdec)\n\nmin(lexdec$RT)\n\n[1] 5.828946\n\nmax(lexdec$RT)\n\n[1] 7.587311\n\nrange(lexdec$RT)\n\n[1] 5.828946 7.587311\n\n\n\nquantile(lexdec$RT, prob = 0.25)\n\n     25% \n6.214608 \n\nquantile(lexdec$RT, prob = 0.75)\n\n    75% \n6.50204 \n\n#IQR calculated from quantiles\nquantile(lexdec$RT, prob = 0.75) - quantile(lexdec$RT, prob = 0.25)\n\n     75% \n0.287432 \n\nIQR(lexdec$RT)\n\n[1] 0.287432\n\n\nHowever, most of the values above can be calculated using the convenient summary() function\n\nsummary(lexdec$RT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.829   6.215   6.346   6.385   6.502   7.587",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-dispersion",
    "href": "descriptive_stats.html#measures-of-dispersion",
    "title": "Descriptive Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\nStandard deviation and variance\nAs described in the lecture the standard deviation of a dataset is calculated as follows:\n\nfor the full population: \\(\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\mu)^2}}{N}}\\)\nfor a sample of the population with the un-biased estimator: \\(s = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\overline{X})^2}}{N-1}}\\)\n\nIn R, the function sd() calculates the sample standard deviation. There is no built in function to calculate the full population \\(\\sigma\\) , but in general, for N &gt; 30 they are very close\n\nsd(lexdec$RT)\n\n[1] 0.2415091",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-summaries-with-tidyverse",
    "href": "descriptive_stats.html#data-summaries-with-tidyverse",
    "title": "Descriptive Statistics",
    "section": "Data summaries with tidyverse",
    "text": "Data summaries with tidyverse\nIn most cases, we have a most complex data structure where we want to calculate the summary statistics not only globally but in several cases of conditions.\nFor example, if we want to calculate the average and number of cases of the Reaction Time in the lexdec dataset for Male and Female and also between Native and non-native English speakers. We could perform it by individually filtering each group combination and calculating the mean and number of cases (length() provides the number of rows) as below:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Male and Native English Speaker\nlexdec_M_english &lt;- lexdec %&gt;% filter( (Sex==\"M\") & (NativeLanguage==\"English\"))\nmean(lexdec_M_english$RT)\n\n[1] 6.361977\n\nlength(lexdec_M_english$RT)\n\n[1] 395\n\n\nAnd this should be repeated for each of the four groups (Male-English, Male-Other, Female-English, Female-Other)\nR has several ways to simplify these calculations. We will look at the approach with tidyverse packages by making use of the functions group_by() and summarize().\ngroup_by() allows to specify grouping variables that would be applied to the next operation on the pipe. The function summarize() allows to create summary variables with the statistics of choice, including mean, sd, IQR, median, min, max, etc‚Ä¶ (look at the online help).\nThe two functions can be combined as in the example below, where we group by Sex and NativeLanguage and ask to create three columns with names numObs , avgRT and sdRT containing the number of observations, mean Reaction Time and Standard Deviation Time respectively.\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 5\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 F     English           553  6.29 0.195\n2 F     Other             553  6.47 0.274\n3 M     English           395  6.36 0.195\n4 M     Other             158  6.50 0.224\n\n\nWhen reporting summary descriptive statistics, it is common practice to provide the Standard Error (SE) that is the error on the sampling of the mean, which is calculated as\n\\[\ns_{\\overline{X}} = \\frac{s}{\\sqrt{N}}\n\\]\nWe can extend the function above to add a column with the SE calculation, which can be done on the basis of the new columns sdRT and numObs:\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT), seRT = sdRT / sqrt(numObs))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 6\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT    seRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 F     English           553  6.29 0.195 0.00830\n2 F     Other             553  6.47 0.274 0.0117 \n3 M     English           395  6.36 0.195 0.00981\n4 M     Other             158  6.50 0.224 0.0178",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "href": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "title": "Descriptive Statistics",
    "section": "Reporting Summary Descriptive Statistics",
    "text": "Reporting Summary Descriptive Statistics\nFor the reporting of each group of data, either a tabular form or descriptive paragraph can be used, providing the values as calculated. An example for the lexdec example we used could be as follows:\n\n\n\n\n\n\nNote\n\n\n\n‚ÄúThe logarithmically transformed reaction time of Female English Native Speakers (M = 6.29, SE = 0.01, n = 553) was faster than Male English Native Speakers (M=6.36, SE = 0.01, n=395), which was faster than both Female (M=6.47, SE=0.01, n=553) and Male (M=6.50, SE=0.02, n=158) non-native Speakers.‚Äù\n\n\nNote that the mean is denoted by a capital M. In this case we don‚Äôt have yet a statistical analysis to determine if the difference observed is significant. That should be reported as well and we will address it in the next lectures.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-normality-checks",
    "href": "descriptive_stats.html#data-normality-checks",
    "title": "Descriptive Statistics",
    "section": "Data Normality checks",
    "text": "Data Normality checks\nWe mentioned in the lectures the importance to be able to check the normality of data or, as we will see of the residuals of a model fit, to ensure that our conclusions based on the statistical assessment of the results of a linear model are valid. In this section we briefly present how to perform in R some of the measures to determine deviation from normality.\n\nSkewness and Kurtosis\nSkewness and Kurtosis of a distribution can be computed using functions in the moments package. Before using it, we have to install the package\n\n#install.packages(\"moments\")\nlibrary(moments)\n\nThe functions are easily called, kurtosis() and skewness() . Let‚Äôs see an example based on the lexdec dataset:\n\nskewness(lexdec$RT)\n\n[1] 0.9930124\n\n\nA positive number implies a right-tailed distribution. If we plot again the histogram for the RT data, this is clearly visible.\n\nlexdec %&gt;% ggplot(aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nIf now we calculate the kurtosis:\n\nkurtosis(lexdec$RT)\n\n[1] 4.405579\n\n\nA positive value indicates a distribution that is more concentrated in the center than a normal distribution.\nSo both measures point to a distribution deviating from normality on the data.\n\n\nQ-Q plots\nA Q-Q plot or ‚ÄúQuantile-Quantile‚Äù Plot, can be used to compare two distributions. It plots the quantiles from the measured data against what a theoretical or other distribution quantile would look like.\nA straight line implies that the two distributions compared are very similar. this is done in R using the qqplot() function. To test for normality, we want to compare the data against a thoretical normal distribution. This a particular case implemented with the function qqnorm() .\nIf we look in our running example\n\nqqnorm(lexdec$RT)\nqqline(lexdec$RT)\n\n\n\n\n\n\n\n\nqqline() adds a reference line for comparison. Clearly the data deviates from the line noting again the non-normality of the data.\nAn alternative to the above two commands is to use the qqPlot() function in the car package that produces charts with an error area more adequate for publication and reporting.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nqqPlot(lexdec$RT)\n\n\n\n\n\n\n\n\n[1] 1273  750\n\n\n\n\nShapiro-Wilk tests\nFinally, a number of tests can be used to provide a quantified assessment of the normality or not of the data.\nHere we present the Shapiro-Wilk test. This test considers a null hypothesis that the data is normally distributed. A significant outcome implies that the null hypothesis is not maintained and that the data is not normally distributed. It is implemented by the shapiro.test() function\n\nshapiro.test(lexdec$RT)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lexdec$RT\nW = 0.94738, p-value &lt; 2.2e-16\n\n\nA p-value lower than a defined threshold implies that the null hypothesis is rejected. In this case, p&lt;0.001 so we will assess the data as not being normally distributed.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Workgroup3.html",
    "href": "Workgroup3.html",
    "title": "Workgroup 3: Simple Linear Regression with R",
    "section": "",
    "text": "In this session we will start exploring the analysis of correlations between variables and applying the linear model concepts explained in the lecture with R.\nWe start looking in a bit more detail into the visual exploration of relationships using scatterplots.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate and format scatter plots with ggplot.\nVisualize linear model fits with data.\nUnderstand concept, calculation and significance checking of Pearson‚Äôs Correlation Coefficient in R.\nUnderstand basic linear models formula nomenclature.\nFit linear model with a continuous or interval predictor using lm()\nFamiliarize with the output of the lm() function.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R"
    ]
  },
  {
    "objectID": "plot_xy.html",
    "href": "plot_xy.html",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Scatterplot with two variables\nIn Workgroup #2, we introduced plotting in R with ggplot2 package and practiced generating a plot in assignment #2.\nIn this section, we will expand with a few additional concepts useful to explore visually the relationship between two variables.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#scatterplot-with-two-variables",
    "href": "plot_xy.html#scatterplot-with-two-variables",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Basic scatterplot\nA¬†scatterplot¬†or XY plot displays the values of two variables along two axes, showing the relationship between them and possible correlations.\nScatterplots can be generated in ggplot using the geom_point() function.\nLet‚Äôs look for an example at a file in the data directory at the lexdec in the languageR package as in Workgroup2 and explore the relationship between the lexical decision latency reaction time (RT) and the word Frequency .\n\nlibrary(languageR)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nOverplotting\nA few things can be noticed on the plot above:\n\nThere is not a continuous range of values of Frequency in the dataset, as it can be seen by the gaps between data. This is common in linguistics research, where we have limited examples of continuous predictors.\nSeveral points are ‚Äòoverplotted‚Äô on the same are, thus not providing a clear view of where there are more data points.\n\nThere are several ways to address this issue. We can change the shape of the data points from a solid circle to a hollow circle. The shape is controlled with the shape argument. The value for an empty circle is shape = 1 . Shapes values are in the reference of the function, although it is not immediate to find. I leave here an image for reference:\n\n\n\nFrom: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(shape = 1)\n\n\n\n\n\n\n\n\nNow you can see the areas with bigger overlap of points in the center of the group.\nIn case of a large dataset, it is bether to use the alpha (transparency) argument. This can be specified as a ratio. For example, a value alpha = 1/5 is to be interpreted as ‚Äú5 points to be overplotted to get a solid color‚Äù.\nCompare the plots below with two different values\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/5)\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/10)\n\n\n\n\n\n\n\n\n\n\nDiscrete variables\nA common case in linguistics research is with Catergorical values with a limited set of levels. The scatter plot will then look like follows\n\nggplot(lexdec,  aes(x = Class, y = RT)) + geom_point(alpha=1/10)\n\n\n\n\n\n\n\n\nIn those cases with very limited number of categories, a boxplot or violin plot will be a better option to display the relationship.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#mapping-additional-variables-to-features",
    "href": "plot_xy.html#mapping-additional-variables-to-features",
    "title": "Plotting relationship between two variables",
    "section": "Mapping additional variables to features",
    "text": "Mapping additional variables to features\n\nA scatterplot shows the relationship between two variables in X and Y, but it can be used to reflect the relationship with more variables by mapping them to specific asthetic features.\nFour commonly used:\n\nShape\nColor\nSize\nTransparency\n\nLet‚Äôs add in the plots above on the RT relationship with Frequency, a mapping to the word semantic category in the variable Class.\n\nggplot(lexdec,  aes(x = Frequency, y = RT,shape = Class)) + geom_point()\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class)) + geom_point()\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,size = Class)) + geom_point(alpha = 1/10)\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,alpha = Class)) + geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nWe can also add two mapping to the same variable. For example, if we want both different colors and different shape of the data points we can do:\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, shape=Class)) + geom_point()\n\n\n\n\n\n\n\n\nWe can also add another variable, for example NativeLanguage encoding the native on non-native nature of the speakers. Readability of the graph may be difficult though .\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, alpha=NativeLanguage)) + geom_point() \n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nAlways try different visualizations to find what is the best way to convey the message on your dataset.\nGood resource for inspiration and recommendations on data visualization types:\nR Graph Gallery: https://r-graph-gallery.com/\nFrom Data to Viz: https://www.data-to-viz.com/",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#plotting-linear-models",
    "href": "plot_xy.html#plotting-linear-models",
    "title": "Plotting relationship between two variables",
    "section": "Plotting linear models",
    "text": "Plotting linear models\nWe will look in next workgroups at different ways to represent the results of linear modeling, but we introduce here a useful feature of ggplot to plot fits to the data as another layer.\nThe function geom_smooth() adds a layer to a plot with aid the visualization of trends and relationships. It has two main arguments:\n\nmethod : function to use to calculate the ‚Äòsmoothed‚Äô version of the data. The value that is relevant to us is method = 'lm' , that would calculate a linear model fit and overplot it.\nformula: Formula of the linear fit model. The default value is formula = y ~ x . We will come back to this point in later lectures. For now we can use the default with no need to specify it.\n\n\nggplot(lexdec, aes(x = Frequency, y = RT)) + \n                geom_point(alpha = 1/5) +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat we can see now in the plot is a linear fit to the data, showing a negative correlation between Frequency and RT, with higher frequency words (more common) showing faster reaction times.\nAn interesting characteristic of this feature is that it applies the fit to the data as defined in ggplot aes(), so if we mapped another variable, like NativeLanguage to an aesthetic, we will get two fits, one per group:\n\nggplot(lexdec, aes(x = Frequency, y = RT, color = NativeLanguage))+\n                geom_point() +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot above shows :\n\nIn general there is a trend that words are comprehended/processed faster the most frequent they are.\nThe strength of this effect is more pronounced in Non-native English speakers.\n\n\nNow that we are able to observe the relationship between two variables let‚Äôs move to quantifying it and assessing their statistical significance.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Pearson‚Äôs correlation coefficient\nCorrelation is a measure of the dependence between two variables, and to which degree they are linearly related. It is not an indication of causality just of relationship.\nThe Pearson‚Äôs correlation coefficient, \\(r\\), is used to quantify the direction and magnitude of the association between two variables X, Y. It is calculated based in the sum of cross products of their values, normalized by their standard deviation:\n\\[\nr= \\frac{\\sum_{i=1}^N (x_i-\\bar{X})(y_i-\\bar{Y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\bar{X})^2}\\sqrt{\\sum_{i=1}^N (y_i-\\bar{Y})^2}}\n\\]\nThe coefficient takes values between -1 and 1:\nThe image below shows examples of correlation values and scatterplot of two variables in X and Y.\nIn R, we can calculate the Pearson correlation coefficient using the cor() function.\nContinuing with the previous example:\nlibrary(languageR)\ncor(lexdec$Frequency, lexdec$RT, method = \"pearson\")\n\n[1] -0.2263358\nThe result, \\(r=-0.23\\), is negative, reflecting an inverse relationship (higher Frequency results in lower RT) and the level of correlation is moderate.\nA question however is how do we know that the calculated value is significant? A NHST test is performed following the approach defined in the lecture:\nIn the case our example, the number of data points, n, is \\(n=1659\\)\nnrow(lexdec)\n\n[1] 1659\nThe t-statistics will be :\n\\[ t= \\frac{r\\times \\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{-0.23 \\times \\sqrt{1659-2}}{\\sqrt{1-(-0.23)^2}} = -9.46\n\\]\nIf we select a confidence level of 5%, \\(\\alpha = 0.05\\), we can extract the value of \\(t_{critical}\\) considering the \\(df = 1659-2=1657\\). This can be calculated in R using the function qt() that returns a value of the Student t distribution. Note that since we are running a two-tailed test, we use 0.025 for p (half of 0.05).\nqt(p = 0.025, df = 1657)\n\n[1] -1.961397\nThe t-value is smaller than the critical value, so we reject the null hypothesis: there is a correlation between our variables.\nThe complete process above, can be performed in R using the function cor.test()\ncor.test(lexdec$Frequency,lexdec$RT,method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  lexdec$Frequency and lexdec$RT\nt = -9.4587, df = 1657, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2715046 -0.1801720\nsample estimates:\n       cor \n-0.2263358\nIf we look at the results provided by the function, the calculated t-value, df and correlation coefficient are the same. In addition, the function returns a p-value (\\(p&lt;0.001\\)) and a confidence interval for the correlation coefficient (\\(95CI_r = [-0.27,-0.18]\\) )",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "correlation.html#pearsons-correlation-coefficient",
    "href": "correlation.html#pearsons-correlation-coefficient",
    "title": "Correlation",
    "section": "",
    "text": "r=‚àí1 a perfect negative relationship: when one variable increases the other one decreases.\nr = 1, a perfect positive relationship.\nr=0, indicates no relationship at all between the two variables\n\n\n\n\n\nPearson correlation coefficient\n\n\n\n\n\n\n\n\n\nStep 1: Define statistical hypotheses\nOur null hypothesis is that the two variables are uncorrelated, so \\(r=0\\)\n\\[\nH_0: r=0 \\\\\nH_a: r\\neq 0\n\\]\nStep 2: Define sampling distribution\nFor the Pearson coefficient, \\(r\\), the sampling distribution is normal if with a mean \\(\\mu_{r}=0\\) if \\(H_0\\) is true\nStep 3: Identify the test statistic\nThe test statistic is following a t-distribution with \\(df = n-2\\) and value\n\\[\nt=\\frac{r\\times\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\nNote: details beyond the scope of the course.\n\n\n\n\n\n\nStep 4: Determine critical value\n\n\n\n\nStep 5: reach statistical conclusion:\n\\[\nt_{value} = -9.62 \\lt t_{critical} = -1.96\n\\]",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "lm_intro.html",
    "href": "lm_intro.html",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "Formula notation in R\nAs explained in the lecture, a linear model of a dataset fits a line that minimizes the sum square errors between the predicted values and the data.\nThe generic function in R used to calculate a linear model fit based on data is lm() with the main arguments formula and data.\nA linear model with one predictor variable can be in general expressed as:\n\\[\nY_i = b_0 + b_1X_i+ \\epsilon_i\n\\]\nwhere \\(b_0\\) and \\(b_1\\) are referred to as the coefficients of the model and \\(\\epsilon_i\\) is the error term of the fit.\nIn our running example:\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n`geom_smooth()` using formula = 'y ~ x'\nWe would like to build a model as :\n\\[\nRT_i = b_0+b_1Frequency_i+\\epsilon_i\n\\]\nModels in R are specified using a notation in the form response~terms where response is the (numeric) dependent variable and terms are the linear predictors. A few notes:\nSo both formulas below are equivalent and can represent our model :\nRT ~ Frequency\nRT ~ 1 + Frequency\nThe formula can be extended to add more predictors (workgroup #4):\nRT ~ 1 + Frequency + NativeLanguage\nor to include interactions between predictors, using the operator : (workgroup #5):\nRT ~ 1 + Frequency:Length + NativeLanguage\nFor now, we will focus on a single predictor.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with `lm()`"
    ]
  },
  {
    "objectID": "lm_intro.html#formula-notation-in-r",
    "href": "lm_intro.html#formula-notation-in-r",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "\\(b_0\\): intercept - represents the value of the outcome variable Y, when X=0.\n\\(b_1\\): slope - represents the change on the value of Y, due to a change of 1 unit of the value of X.\n\n\n\n\n\n\n\nFormulas have an implicit intercept, so it is not necessary to include it.\nError is considered by the function and calculated and it is also not included in the definition",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with `lm()`"
    ]
  },
  {
    "objectID": "lm_intro.html#interpreting-lm-output",
    "href": "lm_intro.html#interpreting-lm-output",
    "title": "Linear Models in R with lm()",
    "section": "Interpreting lm() output",
    "text": "Interpreting lm() output\n\nThe function lm() has a simple nomenclature and usage. The data it returns is nevertheless extensive and we will focus on the interpretation of the output.\nLet‚Äôs look at out example:\n\nlm(RT~1+Frequency, data=lexdec)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nCoefficients:\n(Intercept)    Frequency  \n    6.58878     -0.04287  \n\n\nRunning the lm() function returns a simple set of data, with the formula reflecting again and the value of the coefficients. This result means that our model would be written as\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\nThe formula however calculates more elements. Instead, it is normally executed saving the output in a new variable.\n\nmodel1 &lt;- lm(RT~1+Frequency, data = lexdec)\n\nnames(model1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nAs we can see, the function returns a dataframe with many elements and variables. We will go through them at different points in the course.\nA simple way to display the results is to call the summary() function on the dataframe returned by lm() :\n\nsummary(model1)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55407 -0.16153 -0.03494  0.11699  1.08768 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.588778   0.022296 295.515   &lt;2e-16 ***\nFrequency   -0.042872   0.004533  -9.459   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2353 on 1657 degrees of freedom\nMultiple R-squared:  0.05123,   Adjusted R-squared:  0.05066 \nF-statistic: 89.47 on 1 and 1657 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs look at each of the output values individually:\n\nModel residuals\n\nThe first output of the function recalls the formula used for the fit and provides the 5-point summary of the model residuals.\n\n\n\n\n\nWe will see in the next lecture that one of the conditions for the validity of a linear model is the normality (normal distribution) of the model residuals, with a mean around 0. The 5-point summary provides an indication, nevertheless a check should be performed on the data. The residuals are included in the data structure as output of the model in a field called residuals . We can plot those residuals using for example qqplot()\n\nqqnorm(model1$residuals)\nqqline(model1$residuals)\n\n\n\n\n\n\n\n\n\n\nModel coefficients\n\nThe second part of the table, provides the coefficients :\n\n\n\n\n\nThe table includes the two coefficients (\\(\\beta_0\\) : intercept) and (\\(\\beta_1\\): slope), and for each of them the following columns with the values and the results of a statistic test to check their significance.\n\nEstimate: Coefficient value.\nStd. Error: standard error of the coefficient estimate (\\(\\sigma_{\\bar{b_i}}\\))\nt-value: t-statistic calculated based on the value, to test the significance of the coefficient\n\\[\nt=\\frac{\\hat{b_i}}{\\sigma_{\\hat{b_i}}}\n\\]\n\n\\[\nH_0:b_i=0\\\\H_a:b_i \\ne 0\n\\]\n\n\\(Pr(&gt;|t|)\\) or p-value: probability of the t-value in case \\(H_0\\) is true.\n\nFrom the above, in our example we can see that both coefficients are significantly different from 0, so the model is confirmed to be:\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nModel validity and Intercept interpretation:\nA model is always valid around the values used to generate them, and it is important to be careful to interpret the outcome.\nThe intercept value of 6.59 means that a word with a lemma log Frequency value of 0, is expected to have a logarithmic reaction time of 6.59 (in lexdec dataset both RT and Frequency are logarithmically transformed values) This might not make sense, as at words with extremely low frequency, the reaction time might be even longer.\nConsider the range of the data used to fit the model:\n\nmin(lexdec$Frequency)\n\n[1] 1.791759\n\nmax(lexdec$Frequency)\n\n[1] 7.77191\n\n\nWe should not use the model to predict values beyond this range.\n\n\n\n\n\nData fit quality\n\nThe statistical analysis shown before provides the significance assessment at individual coefficient level.\nThe last section of the output on the other hand, provides the statistical assessment of the model as a whole\n\n\n\n\n\nWe will describe these values in the next Lecture and Workgroup.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with `lm()`"
    ]
  },
  {
    "objectID": "Workgroup4.html",
    "href": "Workgroup4.html",
    "title": "Workgroup 4: Simple Linear Regression II",
    "section": "",
    "text": "Workgroup 4 material to be published in the coming weeks",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II"
    ]
  },
  {
    "objectID": "Workgroup5.html",
    "href": "Workgroup5.html",
    "title": "Workgroup 5: Multiple Regression with R",
    "section": "",
    "text": "Workgroup 5 material to be published in the coming weeks",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R"
    ]
  },
  {
    "objectID": "Workgroup6.html",
    "href": "Workgroup6.html",
    "title": "Workgroup 6: Multiple Regression with R II",
    "section": "",
    "text": "Workgroup 6 material to be published in the coming weeks",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baayen, R. H. 2008. Analyzing Linguistic Data: A Practical\nIntroduction to Statistics Using r. Cambridge University Press.\n\n\nField, Andy P. 2026. Discovering Statistics Using R and\nRStudio. London: SAGE Publications.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \"\nO‚ÄôReilly Media, Inc.\".\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005.\n‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù\nThe Journal of the Acoustical Society of America 118 (4):\n2561‚Äì69.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024.\nR for Data Science. O‚ÄôReilly.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In\nHandbook of Computational Statistics: Concepts and Methods,\n375‚Äì414. Springer.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using\nr. Routledge.",
    "crumbs": [
      "References"
    ]
  }
]