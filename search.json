[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Linear Models 2025-2026: Workbook",
    "section": "",
    "text": "Welcome\nThis is the Workbook for the Fundamentals of Linear Models Course\nThis Workbook was designed as a companion to the Workgroup lectures and aims to provide you with basics of statistical computing using the R language programming, an explanation of the exercises we will follow during the class as well as the assignments to be performed after the lecture.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Workbook organization",
    "section": "",
    "text": "The workbook is organized in six chapters to cover the Workgroup sessions.\nFor each workgroup, a number of descriptive sections are included explaining the main concepts and including code examples. The idea is that you read through the description and try the code examples and exercises yourself as you go through in the RStudio environment.\nAt the end of each workgroup section there is an introduction and description of an Assignment, to be started during the class and finished at home (if required).\nAnswer keys to the assignments will be included in the relevant Workgroup section after the due date of each of them.",
    "crumbs": [
      "Workbook organization"
    ]
  },
  {
    "objectID": "Workgroup1.html",
    "href": "Workgroup1.html",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "",
    "text": "üß† Learning Objectives\nIn this session we will familiarize with the basics on the R computing language and the RStudio environment.\nBy the end of this lesson, you will be able to:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-r",
    "href": "Workgroup1.html#what-is-r",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üõ†Ô∏è What is R?",
    "text": "üõ†Ô∏è What is R?\nR is a powerful, open-source programming language designed for statistical computing, data analysis and visualization. It is widely used among statisticians, data analysts, and researchers.\nIt was created by statisticians Ross Ihaka and Robert Gentleman in the early 1990s and has since become a standard tool in academia, research, and industry.\n\nKey Features of R\n\nStatistical Analysis: R allows to perform analysis using a wide range of statistical techniques including linear and nonlinear modeling, time-series analysis, classification, clustering, bayesian methods.\nData Visualization: R allows the creation of high-quality plots and graphics using specific packages (we will explain below what packages are) like ggplot2, lattice, and plotly.\nExtensibility: Thousands of community provided packages are available via CRAN (Comprehensive R Archive Network), covering several fields.\nData Handling: R includes robust tools for importing, cleaning, transforming, and manipulating data.\nCommunity Support: A large and active user community contributes to its development and provides extensive documentation and tutorials.\n\n\n\nInstalling R\nTo work with the programming language, the R interpreter needs to be installed. You can download it from the R homepage, which is:\n\nhttp://cran.r-project.org/\n\nThere are versions available for Windows, Mac and Linux (several distributions). Select the current version (version 4.5.1 at the start of this course) and install it in your personal computer if you want to use it at home.\nR installation includes a basic interface environment with a Console to enter commands and write scripts that can be launched using the R.exe or R.app.\n\nThis interface is nonetheless quite limited and it is not normally used for data analysis and script development. You can use this language through lots of different applications and environments (e.g.¬†VSCode, JupyterLabs, etc..) . For this course we will introduce the most commonly use development environment for R: RStudio.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-rstudio",
    "href": "Workgroup1.html#what-is-rstudio",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üíª What is RStudio?",
    "text": "üíª What is RStudio?\nRStudio is an Integrated Development Environment (IDE) for R, that makes coding in R and management of data analysis projects easier.\nRStudio is a product developed by a company called posit, but that provides a free, open source RStudio Desktop version that can be downloaded here:\n\nhttps://posit.co/download/rstudio-desktop/\n\nAs with R, there are versions available for Windows, Mac and several Linux distributions.\nOnce we launch RStudio, you can distinguish four different areas:\n\nConsole: Where R code is executed, you can type commands and see their output.\nSource: Where you write and save scripts, Notebooks.\nEnvironments: Includes tabs to inspect variables and inspect the command history as well as access tutorials.\nFiles/Plots/Packages/Help/Viewer: Includes tabs for file navigation, plotting, package management, and help.\n\n\n\n\nRStudio Environment (extracted from RStudio User Guide)\n\n\nWe will familiarize with the interface during the exercises in the Workgroup sessions, but you can find a full description and information the following resources provided in the Posit website:\n\nRStudio IDE User Guide",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "href": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üåêUsing R and RStudio on the web",
    "text": "üåêUsing R and RStudio on the web\nIf you don‚Äôt want to install in your own computer, you can use the Posit Cloud environment. that provides a Cloud Free option. This is the approach we will use in this course, so that you can access your work from anywhere.\nAll assignments will be performed in Posit Cloud where I can follow your progress and assist in case of issues.\n\n\nüìù Exercise1: Connect to Posit Cloud\nIn order to use Posit Cloud you need to register for this course following the link below.\nhttps://posit.cloud/spaces/681791/join?access_code=Da9RyPvqyx7Jymq_aHZpKYwBuJJ-45h5bjS1Y7tq\nYou will be directed to the following page. Select the sign-up option and create your account.\n\n\n\nPosit Cloud Sign-in page\n\n\nOnce you sign-in you should see the course project as in the image below:\n\n\n\n\n\nOpen it and explore the RStudio interface.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Commands and operators\nR is an interpreted language, which means that you can write commands to an interpreter in a console that will execute them and return the results. This is in comparison with compiled languages that require to compile the source code to translate it to a machine understandable code.\nIn the RStudio console, you can see the symbol below. This is the command prompt, indicating that the system us ready to execute an instruction or command.\nR syntax is relatively simple and, although daunting if you have never programmed before when you first encounter it, you will quickly get acquainted with it.\nHere‚Äôs a simple example of how R code looks. In the following sections we explain some basic concepts on the syntax and notation.\nBefore going forward, note that lines of code starting with the symbol # are not interpreted. This is use to introduce comments in your code for readability and documentation. We will come to that later when we talk about scripts and notebooks.\nEvery instruction to enter in the command prompt is called a command.\nA simple command is to perform an arithmetic operations like for example:\n1 + 2\n\n[1] 3\nThe command just calculated the addition of the two numbers. In this example, we used the operator + to do so.\nWe include a list below of the basic operators in R, grouped by category. Do not worry if not all are understandable yet:\n1. Arithmetic Operators\nUsed for basic mathematical operations:\n2. Relational (Comparison) Operators\nUsed to compare values:\n3. Logical Operators\nUsed for logical operations (return TRUE/FALSE):\n4. Assignment Operators\nUsed to assign values to variables:\n5. Miscellaneous Operators",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#commands-and-operators",
    "href": "intro_to_r.html#commands-and-operators",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Tip\n\n\n\nCopy the code as you read along in this workbook and try it yourself in RStudio to become familiar with using the tool and environment.\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n+\nAddition\n2 + 3\n\n\n-\nSubtraction\n5 - 2\n\n\n*\nMultiplication\n4 * 3\n\n\n/\nDivision\n10 / 2\n\n\n^ or **\nExponentiation\n2^3 or 2**3\n\n\n%%\nModulus (remainder)\n10 %% 3\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == y\n\n\n!=\nNot equal to\nx != y\n\n\n&gt;\nGreater than\nx &gt; y\n\n\n&lt;\nLess than\nx &lt; y\n\n\n&gt;=\nGreater than or equal to\nx &gt;= y\n\n\n&lt;=\nLess than or equal to\nx &lt;= y\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&\nElement-wise AND\nx &gt; 1 & x &lt; 5\n\n\n|\nElement-wise OR\nx &lt; 1 | x &gt; 5\n\n\n!\nNOT\n!TRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\nComment\n\n\n\n\n&lt;-\nPreferred assignment\nx &lt;- 5\nMost commonly used\n\n\n=\nAlternative assignment\nx = 5\nUsed in the assignment of values to function arguments (see function section below)\n\n\n-&gt;\nAssign right to left\n5 -&gt; x\nAlthough syntactically valid in R, not used often\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n%in%\nMembership test\n3 %in% c(1, 2, 3) will return TRUE\n\n\n:\nSequence\n1:5 returns 1 2 3 4 5",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#variables",
    "href": "intro_to_r.html#variables",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Variables",
    "text": "Variables\nA variable is a name that stores a value or data object. You can think of it as a labeled container that holds information you want to use or manipulate in your program.\nIn R, you assign values to variables mostly using the operator &lt;- .\n\nx &lt;- 5       # Assign 5 to variable x  \ny &lt;- 10      # Assign 10 to variable y  \nz &lt;- x + y   # Add x and y  \nprint(z)     # Print the result \n\n[1] 15\n\n\nVariables naming in R can be anything, but follow a few rules:\n\nMust start with a letter\nCan contain letters, numbers, underscores (_) or periods (.)\nAre case-sensitive (Name and name are different)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#data-types",
    "href": "intro_to_r.html#data-types",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Data types",
    "text": "Data types\n\nBasic types\nR supports several basic data types. Some of the most common are:\n\nNumeric: x &lt;- 3.14\nInteger: x &lt;- 5L (note the L)\nCharacter: name &lt;- \"Leticia\"\nLogical: flag &lt;- TRUE\n\n\n\nVectors and lists\nVectors are the most basic data structure in R. They are a groups of values built using the combine function,¬†c(). For example,¬†c(1, 2, 3, 4)¬†creates a four element series of positive integer values\n\nnumbers &lt;- c(1, 2, 3, 4)\nnumbers\n\n[1] 1 2 3 4\n\n\nYou can also perform operations on vectors.\n\nnumbers^2\n\n[1]  1  4  9 16\n\n\n\n\nDataframes\nstructures can be thought of as sets of data organized in a table format in rows and columns. They can be created using the dataframe() function.\n\ndf &lt;- data.frame(\n  SubjectID = c(\"S1\", \"S2\",\"S3\"),\n  age = c(25, 30, 28)\n)\ndf\n\n  SubjectID age\n1        S1  25\n2        S2  30\n3        S3  28\n\n\nYou can access the individual columns on a dataframe using the $ operator. Try to start typing the code below on the console. You will see that R provides suggested completions, displaying the available columns in the dataframe.\n\ndf$SubjectID\n\n[1] \"S1\" \"S2\" \"S3\"\n\n\nMany of the functions we will use in this course require a dataframe as an input or produce one as output, so it is the data structure you will use the most.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#par-functions",
    "href": "intro_to_r.html#par-functions",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Functions",
    "text": "Functions\nFunctions perform tasks in R. They take in inputs called arguments and return outputs. They are called using parentheses. For example, the function mean() in R calculates the mean of the elements we provide as input.\n\n# We can specify a list of numbers\nmean(c(1,2,3,4))\n\n[1] 2.5\n\n\nOr, more useful, we can provide a variable containing values:\n\n# we defined the variable numbers before\nmean(numbers)\n\n[1] 2.5\n\n\nThe parameters of a function and normally called arguments. You can either manually specify a function‚Äôs arguments or use the function‚Äôs default values. In the examples above mean() and sum() are simple functions with not many arguments, but this is not normally the case.\nTo know the arguments of a function you can use the R help. There are two ways to access the help pages for a function:\n\nUse the operator ? followed by the function name in the console, or use the help() function.\nFor example, type the instruction ?mean in the console. The help page for the mean() function will open in the RStudio Help panel as in the image below:\n\nYou can of course directly open the Help tab and search for the function of interest.\n\nAs you can see above, mean() actually has two other arguments, trim and na.rm . The arguments have a default value, so if we don‚Äôt explicitly include them in the function call, they will use that value. Let‚Äôs look at an example using na.rm :\nR has a special value called NA , which means ‚ÄúNot Available‚Äù and it is used to represent missing values on the data. In experimental work is often the case that some data point is lost or corrupted and we have incomplete datasets. Let‚Äôs assume you had performed an online experiment that computed the reaction time in miliseconds of 10 participants, and one value was not available as per the vector below:\n\nrt &lt;- c(234.2, 127.5, 256.2, NA, 287.1, 145.6, 358.9, 200.1, 398.3, 178.3)\n\nLet‚Äôs try to calculate the average reaction time of your data using mean():\n\nmean(rt)\n\n[1] NA\n\n\nThe function tries to calculate the average, but when one value is not available (NA) the result is also NA . In the help in the image below we see there is an argument na.rm that we can use to ignore the missing elements in the data:\n\nmean(rt, na.rm = TRUE)\n\n[1] 242.9111\n\n\nNow the function worked and summed all the numbers and divided them by 9, ignoring the missing data, in the calculation of the average.\nWe‚Äôll work with functions a lot throughout this book and you‚Äôll get lots of practice in understanding their behaviors, so don‚Äôt panic.\nFinally an advanced note to make you aware that you can define your own functions. The following code defines a new function called add() that, well, adds two numbers:\n\nadd &lt;- function(a, b) {\n  return(a + b)\n}\n\nOnce you have defined the function, you can use it as any other in R:\n\nadd(3.5, 2.5)\n\n[1] 6",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#factors",
    "href": "intro_to_r.html#factors",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Factors",
    "text": "Factors\nIn¬†R, a¬†factor¬†is a data structure used to represent¬†categorical data. Categorical data consists of variables that have a fixed number of unique values, known as¬†levels. These are typically used for any variable that classifies observations into groups. We will use factors extensively in the analysis of data\nWe convert a vector into a factor by using the factor() function. Let‚Äôs look at one example.\n\n\nüìù Exercise 2: Creating a dataframe with factors\nWe want to create a dataset of six words, collecting data of the animacy, gender, length and frequency of the word.\nFirst, we create individual variables with vectors including the values\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\") \nNounAnimacy \n\n[1] \"animate\"   \"inanimate\" \"inanimate\" \"animate\"   \"animate\"   \"animate\"  \n\n\n\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\") \nNounGender\n\n[1] \"feminine\"  \"masculine\" \"neuter\"    \"masculine\" \"feminine\"  \"feminine\" \n\n\n\nNounLength&lt;-c(6,7,4,5,8,6) \nNounLength \n\n[1] 6 7 4 5 8 6\n\nNounFrequency&lt;-c(638,799,390,569,567,665) \nNounFrequency\n\n[1] 638 799 390 569 567 665\n\n\nAs you can see from the output above, data in the variables NounAnimacy and NounGender are considered as words, or literal strings. The next step is to indicate they are factors.\n\nNounAnimacy&lt;- factor(NounAnimacy) \nNounAnimacy \n\n[1] animate   inanimate inanimate animate   animate   animate  \nLevels: animate inanimate\n\nNounGender&lt;- factor(NounGender) \nNounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter\n\n\nAs you can see by the output produced, now the variables are considered as factors, and Levels indicate the unique values that the each takes.\nWith the variables above, we can now create a dataframe.\n\nDataexample&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency) \nDataexample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nA few useful functions when working with dataframes:\n\nnrow() : returns the number of rows of a dataframe. Normally corresponding to observations in experimental data files.\nncol(): returns the number of columns of a dataframe.\nhead() : displays the first rows of a dataframe or vector. By default, it displays the first 6 items, but you can specify how many rows to show with the argument n . Useful when displaying on the screen large datasets.\nView(): Opens the dataset in a excel-like data viewer in RStudio. Same can be done clicking the name of the variable in the Environment window in RStudio.\nstr() : can be used to display the structure of the dataframe.\ncolnames(): returns the names of the columns/variables in a dataframe.\n\nLet‚Äôs look at a few examples with the dataframe we just created.\nFirst we can inspect what is the dataframe structure:\n\nstr(Dataexample)\n\n'data.frame':   6 obs. of  4 variables:\n $ NounAnimacy  : Factor w/ 2 levels \"animate\",\"inanimate\": 1 2 2 1 1 1\n $ NounGender   : Factor w/ 3 levels \"feminine\",\"masculine\",..: 1 2 3 2 1 1\n $ NounLength   : num  6 7 4 5 8 6\n $ NounFrequency: num  638 799 390 569 567 665\n\n\nAs you can see, the output tells us that the dataframe is composed of four variables (columns), two with categorical values and two with numerical elements and provides the names of those columns. It tells us also that it contains 6 observations (rows). You can obtain also the information above using:\n\nnrow(Dataexample)\n\n[1] 6\n\nncol(Dataexample)\n\n[1] 4\n\ncolnames(Dataexample)\n\n[1] \"NounAnimacy\"   \"NounGender\"    \"NounLength\"    \"NounFrequency\"\n\n\nIf we want to display the first 4 observations in the dataset, you could use:\n\nhead(Dataexample, n = 4)\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#additional-reading-material",
    "href": "intro_to_r.html#additional-reading-material",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Additional Reading material",
    "text": "Additional Reading material\nBasics on R programming:\n\nChapter 1, Introduction to R, from Bodo Winter‚Äôs book ((Winter 2019)).\nSections 2 to 8 on R basic Tutorial by UMC Utrecht\nA good starters reference for R is the book ‚ÄúR for Data Science‚Äù ((Wickham, √áetinkaya-Rundel, and Grolemund 2024)) ). The book is available online freely at R for Data Science\nExcellent eBook to learn R Basics ((Grolemund 2014)) Hands On Programming with R\n\nDataframes:\n\nYouTube Videos by DataCamp:\n\nhttps://www.youtube.com/watch?v=9f2g7RN5N0I\nhttps://youtu.be/Nh6tSD4i4qs?feature=shared\n\n\n\n\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "using_libraries.html",
    "href": "using_libraries.html",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Installing and using a Package\nOne of the big benefits of R in comparison with other statistics packages is its open nature. The functionality is easily extended by groups all around the world by developing libraries that can be easily installed and used.\nR packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet.\nA package is a collection of R functions, data, and compiled code bundled together. Packages are created by the R community and cover a wide range of topics like data manipulation, visualization, machine learning, etc.\nPackages have to be installed and loaded before using them.\nWe need to install packages only once in our environment. That can be done in two ways:\nFor example, in several exercises in this book we use a library (created by for the book (Baayen 2008)) that contains some utilities and a few sample datasets with linguistics examples.\nLet‚Äôs install the library using the following command:\nAfter installing the package, the contents will still not be available until you load the package, this has to be done in every new work session and it is done with the library() function.\nNow you can access the functions and data in the languageR package.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#installing-and-using-a-package",
    "href": "using_libraries.html#installing-and-using-a-package",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Using the install.packages() function or\nUsing the Packages tab in RStudio:\n\n\n\n\ninstall.packages(\"languageR\")\n\n\n\n\n\n\nWarning\n\n\n\nThe installation of a package can give an error if you try to install a package that is already installed and loaded. If you get an error saying that you have already the package, just cancel.\n\n\n\nlibrary(languageR)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#using-datasets-available-in-packages",
    "href": "using_libraries.html#using-datasets-available-in-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Using datasets available in packages",
    "text": "Using datasets available in packages\n\nAs mentioned, the main objective of a package is to distribute functions, but it is often convenient to include example datasets that can be used to illustrate the use of the functions. In other cases, there are packages that are use mainly to distribute data. The later is the case for example for the languageR package, provided as a companion to the book Baayen (2008) that we will use in some of the examples and assignments in this course.\nYou can explore data sets available from all loaded packages using the data() function. If you want the data from a specific package, specified with the argument package .\nTake a look at the packages datasets included in languageR using the following command.\ndata(package=\"languageR\")\nYou will see that a new tab opens in the Editor area with the content:\n\nIf you want more information more information of a particular dataset, you can use the help operator ? as with any function.\nFor example in today‚Äôs assignment we will use the lexdec dataset containing Lexical decision latencies collected from a group of speakers. Run the following command to read the composition of the dataframe.\n?lexdec",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#useful-packages",
    "href": "using_libraries.html#useful-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Useful packages",
    "text": "Useful packages\n\nThere are by now thousands of community contributed packages available for R and the list grows by the day (see complete list in CRAN website).\nIn practice you will use only a few packages on your data analysis tasks. I list below a number of commonly used packages for further reference (note we will only use a few of those in this course and I will always indicate it in the specific sessions or assignments).\n\nPackages for data input and output\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\nRead Excel files\nPart of tidyverse\n\n\nreadr\nRead information in tabular format from CSV and TAB separated files\nPart of tidyverse\n\n\n\n\n\nPackages for data analysis\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\n\n\n\n\n\n\n\n\n\n\n\n\nPackages for data visualization and manipulation\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nggplot2\nLibrary to create graphics and data visualizations\nPart of tidyverse\n\n\ndplyr\nData manipulation functions\nPart of tidyverse\n\n\ntidyr\nFunctions to transform data from wide to long format.\nPart of tidyverse\n\n\nforcats\nFunctions to modify factors levels and ordering\nPart of tidyverse\n\n\n\nWe will explore functions from the packages above in the Workgroups 1 & 2\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "read_data.html",
    "href": "read_data.html",
    "title": "Reading and Saving data in R",
    "section": "",
    "text": "Working directory and paths in R\nWe have seen how to access datasets contained in packages, but to analyze your own data you will need to load it in the RStudio environment.\nWe will look in what follows at a few examples of loading data in different common formats (text files in general, excel files, files in SPSS format, R files).\nBefore entering into the details on the read and write of files, it is important to understand where the files are located and how to provide paths in R.\nPaths for files in R are relative to the ‚ÄúWorking Directory‚Äù. To know which is the working directory, you can use the function getwd() or select the option ‚ÄúGo to working directory‚Äù in the files tab.\nThe paths of a file are relative to that directory. Considering this, in the examples and assignments in this course, we will load data in the /data folder as follows:\n'./data/FILE_NAME'\nThe ./ in the path above indicates the working directory.\nWhen running commands loading or saving files, take care that you indicate the path correctly if you get an error.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "read_data.html#sec-r-read",
    "href": "read_data.html#sec-r-read",
    "title": "Reading and Saving data in R",
    "section": "Reading data from files",
    "text": "Reading data from files\n\nWe cover below examples for importing data in R, covering the most common methods and packages.\n\nData from text files\nThe most generic function to read data from a text file is read.table() . The arguments allow to define if the data includes a header (i.e.¬†a first row with the names of the columns/variables) and the separator used.\nFor example, let‚Äôs load a file in the /data directory containing the sample dataset from the previous exercise using the function below:\n\ndata_sample &lt;- read.table(\"./data/sample_wg1_text.txt\")\ndata_sample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\n\n\nData from CSV files\nMany experimental data is saved in a text or CSV (Comma separated) file. The best method to read data from these files is to use the read_csv() function in the readr package.\nLet‚Äôs see an example of usage loading the file in the /data directorate ‚ÄúELP_full_length_frequency.csv‚Äù with data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1.\n\nlibrary(readr)\n\ndata_sample_csv &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(data_sample_csv)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\n\n\nAs you can see, the file was loaded with 33,057 observations!\nTo free up memory, let‚Äôs remove the variable from the workspace using the rm() function (remove):\n\nrm(data_sample_csv)\n\nIf your data is tab-separated, you can use the function read_tsv() . By default, the functions expect that the first row contains the names of the columns/variables to be read. If that is not the case, you should modify the argument col_names = FALSE .\n\n\nData from Excel files\nThe best way to read Excel files is using the readxl package. If your file has several worksheets, you can use the sheet argument to specify either an index or the name of the worksheet to read.\n\n# load library\nlibrary(readxl)\n\n# Using readxl\ndata_sample_excel &lt;- read_excel(\"./data/sample_wg1_excel.xlsx\", sheet = 1)\ndata_sample_excel\n\n# A tibble: 6 √ó 4\n  NounAnimacy NounGender NounLength NounFrequency\n  &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 animate     feminine            6           638\n2 inanimate   masculine           7           799\n3 inanimate   neuter              4           390\n4 animate     masculine           5           569\n5 animate     feminine            8           567\n6 animate     feminine            6           665\n\n\n\n\nData in R files\nData can also be saved and loaded in R format directly. This is useful if you are performing your analysis in R to save data intermediate steps as it is a compact and efficient format, even though to share with other researchers and in open access journals you should export it to a more generic format.\nTo illustrate again let‚Äôs load a file in the /data directory containing a sample real dataset from a Event Related Potential (ERP) study that we will use in some of the exercises in the course:\n\ndata_sample_r &lt;- readRDS(\"./data/eegSampleData.Rda\")\nhead(data_sample_r)\n\n       time Subject Condition       Fp1       Fpz       Fp2       AF3      AF4\n1 -0.200000       1     CondA 1.9596113 1.6237565 1.4964751 1.9707278 2.232514\n2 -0.198999       1     CondA 1.7536104 1.4013030 1.3669174 1.7740067 2.084792\n3 -0.197998       1     CondA 1.5272095 1.1631496 1.2091597 1.5532856 1.912671\n4 -0.196997       1     CondA 1.2834086 0.9148962 1.0254020 1.3115644 1.718250\n5 -0.195996       1     CondA 1.0260077 0.6621427 0.8183443 1.0523433 1.504029\n6 -0.194995       1     CondA 0.7589068 0.4100893 0.5914866 0.7800222 1.273008\n          F7          F5        F3        F1        Fz        F2        F4\n1  0.9002055  0.97041392 1.6839135 1.8026822 1.7678664 1.8301742 1.8249546\n2  0.7315209  0.80466543 1.5461834 1.6655317 1.6480326 1.7056625 1.6921552\n3  0.5449364  0.62021693 1.3802533 1.4985812 1.4956989 1.5476508 1.5259558\n4  0.3422518  0.41866843 1.1874231 1.3030307 1.3121651 1.3574391 1.3284565\n5  0.1255673  0.20221994 0.9697930 1.0812802 1.0997314 1.1373274 1.1027571\n6 -0.1025173 -0.02662856 0.7301629 0.8365297 0.8614976 0.8909157 0.8531577\n         F6        F8         FT7       FC5       FC3       FC1       FCz\n1 1.6807566 1.6442472  0.79175532 1.0834129 1.1766302 1.2950722 1.6606659\n2 1.5340938 1.4931450  0.62251420 0.9396381 1.0489977 1.1714287 1.5515010\n3 1.3574309 1.3124428  0.43667307 0.7727634 0.8951652 1.0198851 1.4089360\n4 1.1532681 1.1046406  0.23643195 0.5843886 0.7166327 0.8417415 1.2341711\n5 0.9250052 0.8733384  0.02489083 0.3771138 0.5154001 0.6391979 1.0294061\n6 0.6768424 0.6229362 -0.19425030 0.1538391 0.2946676 0.4151543 0.7976412\n        FC2      FC4       FC6       FT8         T7          C5        C3\n1 1.7052435 1.828386 1.6453833 1.5302436  0.6388098  0.82305823 1.1149519\n2 1.6106100 1.725298 1.5067077 1.3838023  0.4772181  0.66489982 1.0142226\n3 1.4794766 1.593309 1.3373322 1.2091609  0.2951264  0.48534141 0.8858932\n4 1.3130431 1.433921 1.1395566 1.0086196  0.0949347  0.28668300 0.7310638\n5 1.1138097 1.249232 0.9165811 0.7854783 -0.1200570  0.07182459 0.5512344\n6 0.8853762 1.042444 0.6724056 0.5438370 -0.3464487 -0.15573382 0.3492050\n         C1         Cz        C2        C4        C6        T8        TP7\n1 1.1306227 0.63096230 1.3182707 1.4574666 1.4729369 1.3580312 -0.1544125\n2 1.0314048 0.57397554 1.2505935 1.3564641 1.3762993 1.2646492 -0.2040742\n3 0.9031868 0.48648877 1.1480163 1.2251617 1.2511616 1.1442672 -0.2693359\n4 0.7472689 0.36920200 1.0112391 1.0652592 1.0988239 0.9982852 -0.3505976\n5 0.5654510 0.22341524 0.8417619 0.8787567 0.9212863 0.8285032 -0.4477593\n6 0.3606331 0.05132847 0.6424847 0.6691542 0.7215486 0.6379212 -0.5599209\n         CP5       CP3         CP1        CPz        CP2       CP4       CP6\n1 0.79635392 0.7767967  0.48835529 0.58856683 0.50392506 0.8973196 0.8844379\n2 0.69718724 0.6999467  0.42408573 0.53621461 0.46937197 0.8189115 0.8011065\n3 0.57452056 0.5954966  0.33161617 0.45326240 0.40481887 0.7127035 0.6903752\n4 0.42925389 0.4640466  0.21144661 0.33981018 0.31056577 0.5798955 0.5537438\n5 0.26278721 0.3069966  0.06507705 0.19715797 0.18771268 0.4227875 0.3934124\n6 0.07762053 0.1267466 -0.10529251 0.02740575 0.03845958 0.2442795 0.2123810\n        TP8          P7         P5          P3          P1          Pz\n1 0.9127783  0.44240355  0.5012748  0.50060479  0.43724552  0.44452279\n2 0.8270859  0.38499520  0.4463851  0.43989524  0.38284579  0.39195007\n3 0.7139935  0.30168685  0.3652955  0.35238568  0.30064606  0.30987735\n4 0.5753011  0.19277851  0.2583059  0.23837612  0.19084634  0.19880464\n5 0.4131088  0.05907016  0.1263162  0.09886657  0.05454661  0.05993192\n6 0.2306164 -0.09753819 -0.0289734 -0.06424299 -0.10655312 -0.10434080\n            P2        P4        P6        P8         PO7         PO5\n1  0.468759124 0.8508531 0.8558191 0.8671508  0.35547460  0.36233623\n2  0.432358313 0.7741135 0.7808388 0.7798563  0.29344668  0.29959166\n3  0.365657503 0.6698739 0.6783584 0.6649618  0.20641876  0.21124709\n4  0.268956693 0.5394343 0.5496781 0.5242673  0.09449085  0.09760252\n5  0.143555882 0.3848946 0.3967978 0.3598728 -0.04133707 -0.04054205\n6 -0.008344928 0.2089550 0.2223175 0.1750783 -0.19966498 -0.20118662\n          PO3         POz        PO4         PO6         PO8        CB1\n1  0.28232490 -0.03600213 -0.7460777  0.52166620  0.49329201 -0.2268531\n2  0.22316096 -0.06933023 -0.7465746  0.45118577  0.42236878 -0.2837141\n3  0.13819702 -0.13065833 -0.7646715  0.35510534  0.32644555 -0.3584751\n4  0.02763308 -0.21928642 -0.8008684  0.23462491  0.20662231 -0.4507361\n5 -0.10743085 -0.33401452 -0.8546652  0.09154448  0.06479908 -0.5599971\n6 -0.26529479 -0.47284262 -0.9249621 -0.07133595 -0.09642415 -0.6844582\n          O1         Oz         O2        CB2\n1 -0.2783048 -0.6701305 -0.8214062 -0.8811375\n2 -0.3294788 -0.6538060 -0.8241543 -0.8791028\n3 -0.4000527 -0.6546815 -0.8441024 -0.8940680\n4 -0.4900266 -0.6739570 -0.8817505 -0.9264333\n5 -0.5987006 -0.7125325 -0.9365985 -0.9760985\n6 -0.7245745 -0.7706080 -1.0072466 -1.0419638\n\n\n\n\nData from SPSS\nThe haven package can be used to read directly .sav files from SPSS.\nAs an example again, let‚Äôs open a file saved in SPSS from another ERP (Event Related Potential) study of code-switching in Dutch.\n\n# If not yet available, install the package using install.packages(\"haven\")\n\n# Load library\nlibrary(haven)\n\ndata_sample_spss &lt;- read_sav(\"./data/EEG_DataSet_LongFormat_pablos.sav\")\nhead(data_sample_spss)\n\n# A tibble: 6 √ó 6\n  Participant Language CS    Congruency Electrode Amplitude\n        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n1           7 Dutch    no_CS congruent  Fp1           0.712\n2           7 Dutch    no_CS congruent  AF3           0.313\n3           7 Dutch    no_CS congruent  F7            0.982\n4           7 Dutch    no_CS congruent  F3            0.587\n5           7 Dutch    no_CS congruent  FC1           0.711\n6           7 Dutch    no_CS congruent  FC5           2.83 \n\n\nAgain to preserve memory, let‚Äôs clean the workspace by removing the variables we created using :\n\nrm(data_sample_spss, data_sample_r)\n\n\n\nOther formats\nIn your own research, if you have data in other specific formats, you may have to develop your own function to read it, although it is highly likely there is already a solution available. Always search first in the internet‚Ä¶ For example, imagine you have data in a file saved in the software Matlab, typing ‚Äúread Matlab files in R‚Äù in any search engine points to the R package R.matlab that offers the functions readMat() and writeMat() to read and write respectively Matlab files.\nA few examples of useful packages for data reading in linguistics research are listed below:\n\n\n\nPackage\nDescription\n\n\n\n\neegUtils\nUtilities for Electroencephalographic (EEG) Analysis:\nIncludes import functions for EEG files from several EEG acquisition and analysis software suites: ‚ÄòBioSemi‚Äô (.BDF), ‚ÄòNeuroscan‚Äô (.CNT), ‚ÄòBrain Vision Analyzer‚Äô (.VHDR), ‚ÄòEEGLAB‚Äô (.set) and ‚ÄòFieldtrip‚Äô (.mat)\n\n\nrprime\nPackage for parsing¬†.txt¬†generated by E-Prime, a program for running psychological experiments.\nSupport functions to read and clean data created in E-Prime\n\n\nchildesr\nPackage to access data in the childes-db, an open database of child language datasets from the CHILDES (Child Language Data Exchange System) data bank.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "tidyverse_intro.html",
    "href": "tidyverse_intro.html",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "üåê What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. These packages share an underlying philosophy, grammar, and data structures, making it easier to learn and use them together. Tidyverse simplifies tasks like data manipulation, visualization, and modeling.\nA full description of the tidyverse packages and functions is available in https://dplyr.tidyverse.org/",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#what-is-the-tidyverse",
    "href": "tidyverse_intro.html#what-is-the-tidyverse",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "Why Use tidyverse?\nUsing the functions in tidyverse have a number of advantages versus the use of individual packages or the available base R functions:\n\nConsistent and readable syntax.\nPipe operator (%&gt;%) for chaining operations (see later)\nDesigned for tidy data principles as described in Wickham, √áetinkaya-Rundel, and Grolemund (2024).\nIt has a strong user community and documentation.\n\n\n\nüì¶ Core tidyverse Packages\nWhen you install and load the tidyverse package, you get access to the core packages listed in the table below without needing to load them individually\n\n\n\nPackage\nUsage\n\n\n\n\nggplot2\nData visualization\n\n\ndplyr\nData manipulation\n\n\ntidyr\nData tidying\n\n\nreadr\nReading rectangular data (CSV, etc.)\n\n\npurrr\nFunctional programming\n\n\ntibble\nModern data frames\n\n\nstringr\nString manipulation\n\n\nforcats\nWorking with categorical data (factors)\n\n\n\nWe have already seen readr when loading files. We will explore today the tidyr functions.\n\n\nWorkflow Using tidyverse\nA generic workflow using tidyverse can be represented as follows:\n\n\n\n\n\nflowchart LR\n  load[\"Load tidyverse\"]\n  read[\"Read data\"]\n  clean[\"Clean and transform data\"]\n  plot[\"Visualize\"]\n  load --&gt; read\n  read --&gt; clean\n  clean --&gt; plot\n\n\n\n\n\n\nThis flow result in a code as the following example:\n\n\n\n\n\n\nImportant\n\n\n\nYou are not expected to understand all the code below now. We will explain different elements during the course. It is just intended as an example of the workflow.\n\n\n\n# Load tidyverse\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read data\n\ndf &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\", show_col_types = FALSE)\n\n# filter, clean and transform data\n\ncleandata &lt;- df %&gt;%\n  filter(length&gt;3) %&gt;%\n  mutate(rt_per_character = RT / length)\n\n# Visualize\n\nggplot(cleandata, aes(x = Log10Freq, y = RT)) + geom_point(color='grey') + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "href": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "A grammar of data manipulation",
    "text": "A grammar of data manipulation\n\ndplyr is a package within the tidyverse set of functions that allow to manipulate data. You can think of the functions in the package as a sort of ‚Äúgrammar of data manipulation‚Äù, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\nmutate() adds new variables that are functions of existing variables -&gt; operates on columns.\nrelocate(): moves variables to a different position (change order of columns) -&gt; operates on columns.\nselect() picks variables based on their names. -&gt; operates on columns.\nrename(): change variable names -&gt; operates on columns\narrange() changes the ordering of the rows. -&gt; operates on rows.\nfilter() picks cases based on their values. -&gt; operates on rows.\nsummarise() reduces multiple values down to a single summary (we will look at this in next workgroup).\n\n\n\n\n\n\n\nNote\n\n\n\nYou will see that the functions in tidyverse libraries return a so called tibble . The details are beyond the scope of this course, but you can think of a tibble as a version of a data frame.\n\n\nLet‚Äôs explore how to use the functions above with another of the datasets contained in the languageR package by Baayen (2008). We will use the DurationsOnt dataset that contains durational measurement of the Dutch prefix -ont from a study on a Spoken Dutch Corpus ( Pluymaekers, Ernestus, and Baayen (2005)).\n\n#load library\nlibrary(languageR)\n\n#load dataset in the environment\ndata(\"durationsOnt\")\n\n#display the structure of the dataset\nstr(durationsOnt)\n\n'data.frame':   102 obs. of  12 variables:\n $ Word                 : Factor w/ 102 levels \"ontbeten\",\"ontbijt\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Frequency            : num  3.09 4.84 0 3.76 1.95 ...\n $ Speaker              : Factor w/ 63 levels \"N01001\",\"N01002\",..: 44 15 53 55 46 42 25 25 63 4 ...\n $ Sex                  : Factor w/ 2 levels \"female\",\"male\": 1 1 2 1 2 1 2 2 2 1 ...\n $ YearOfBirth          : num  72 80 52 60 74 70 76 76 76 66 ...\n $ DurationOfPrefix     : num  0.113 0.1 0.14 0.161 0.161 ...\n $ DurationPrefixVowel  : num  0.0695 0.0354 0.0474 0.0959 0.064 ...\n $ DurationPrefixNasal  : num  0.0439 0.0651 0.0925 0.0648 0.0973 ...\n $ DurationPrefixPlosive: num  0 0 0 0 0 0 0 0 0 0 ...\n $ NumberOfSegmentsOnset: int  1 1 1 1 1 1 1 1 2 2 ...\n $ PlosivePresent       : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SpeechRate           : num  3.92 6.88 3.53 5.39 6.73 ...\n\n#look at the description of the variables\n?durationsOnt\n\nstarting httpd help server ... done\n\n\n\nSelecting variables: select()\n\nThe function select() allows to choose a subset of variables (columns of interest). The function call includes as first parameter the dataset followed by the list of columns you would like to keep.\nFor this example we would like to look only at the duration of the Prefix in seconds and are not interested in the other variables.\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word, Frequency, Speaker, Sex, YearOfBirth, DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\nThe new dataframe durationsOnt_simplified contains now only the columns we specified.\n\n\n\n\n\n\nTip\n\n\n\nIf you have many columns, there is a simpler way to specify ‚Äúkeep from column X to column Y‚Äù without having to list each one of the individually by using : as in the example below that provides the same result. Of course this does not work if you want to select non-contiguous columns.\n\n\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\n\n\nChanging variable names: rename()\n\nYou can change the naming of the variable using rename as follows - You call the function with as first parameter the dataset and after with the list of variables to be renamed.\\\n\n\n\n\n\n\nWarning\n\n\n\nNote that you write first the new name and then the old one.\n\n\n\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\nhead(durationsOnt_simplified_renamed)\n\n             Word Frequency Speaker Gender YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\n\n\nMoving variable position: relocate()\n\nWith big datasets including many variables (columns), sometimes it is useful to change the order of the columns. This can be done with the relocate() function.\nYou call the function providing again as first parameter the dataset and then the column you want to move. By default the function will move the column specified to become the first. Let‚Äôs say that we want to have the Speaker Identifier as the first column, we will use the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker           Word Frequency Gender YearOfBirth DurationOfPrefix\n24  N01143       ontbeten 3.0910425 female          72         0.113372\n58  N01041        ontbijt 4.8441871 female          80         0.100478\n40  N01157  ontbijtbuffet 0.0000000   male          52         0.139806\n42  N01162      ontbijten 3.7612001 female          60         0.160739\n60  N01145      ontbijtje 1.9459101   male          74         0.161283\n21  N01134 ontbijtservies 0.6931472 female          70         0.176658\n\n\nThe Speaker variable is now the first. If we want to define a specific location, we can use the arguments .after or .before . For instance, if we want to move the Gender to be just after the Speaker column we will call the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_reordered, Gender, .after = Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker Gender           Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female       ontbeten 3.0910425          72         0.113372\n58  N01041 female        ontbijt 4.8441871          80         0.100478\n40  N01157   male  ontbijtbuffet 0.0000000          52         0.139806\n42  N01162 female      ontbijten 3.7612001          60         0.160739\n60  N01145   male      ontbijtje 1.9459101          74         0.161283\n21  N01134 female ontbijtservies 0.6931472          70         0.176658\n\n\nNow that we have selected the variables that we want and in the order that we want them, let‚Äôs filter the data to select some cases\n\n\nSelecting cases: filter()\n\nWe can select cases or observations based on a criteria using the filter() function. The filter function is called providing as a first parameter the dataset, followed by a condition or criteria to use for the selection.\nLet‚Äôs say that we want to make an analysis on the duration of people born after 1970\n\ndurationsOnt_reordered_filtered_age &lt;- filter(durationsOnt_reordered,YearOfBirth &gt; 70)\n\n#check on the minimum value of the Year of Birth with the function min()\nmin(durationsOnt_reordered$YearOfBirth)\n\n[1] 23\n\nmin(durationsOnt_reordered_filtered_age$YearOfBirth)\n\n[1] 71\n\n\nAs you can see on the example, the filter() selected the cases based on the condition specified.\nIf we wanted to select for example only the cases for Females, we will have used the filter as Gender == \"female\" . What about if we want to select the females born after 1970 in a single step? You can combine conditions using the operators & (meaning ‚Äúand‚Äù ) and | (meaning ‚Äúor‚Äù). For example:\n\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\nhead(durationsOnt_reordered_filtered_age_gender)\n\n   Speaker Gender      Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female  ontbeten  3.091042          72         0.113372\n58  N01041 female   ontbijt  4.844187          80         0.100478\n46  N01103 female   onthoud  3.433987          77         0.168078\n43  N01089 female  onthulde  2.079442          75         0.165369\n55  N01065 female onthullen  1.945910          78         0.185638\n78  N01096 female  ontkende  2.833213          78         0.186745\n\n\n\n\nReordering cases: arrange()\nFinally, the arrange() function orders the dataset by a selected value. For example to order the dataframe by Frequency:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, Frequency)\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n    Speaker Gender             Word Frequency YearOfBirth DurationOfPrefix\n102  N01037 female ontstekingsvocht 0.0000000          81         0.099240\n20   N01051 female       ontmaskerd 0.6931472          79         0.190070\n93   N01171 female    ontploffingen 1.3862944          76         0.114520\n32   N01051 female      ontmaskeren 1.6094379          79         0.143948\n88   N01110 female        ontvoeren 1.6094379          77         0.098185\n55   N01065 female        onthullen 1.9459101          78         0.185638\n\n\nWhat about if we want to use a reverse order? This can be done use the desc() modified on the variable to be used:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#pipe-operator-or",
    "href": "tidyverse_intro.html#pipe-operator-or",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "Pipe operator (%>% or |> )",
    "text": "Pipe operator (%&gt;% or |&gt; )\n\nAs you can see, in all the examples above we pass as first argument to the function the dataframe on which we want to make the operation.\nPutting all the calls before together :\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nIn this examples I created on every step a new variable with a different name to save the output of the functions, which created a lot of new variables. This might be useful only in cases when you want to save the intermediate steps. In reality this is rarely the case since anyway you can always execute again the complete flow if you want to change something, and it is a better practice.\nThat being the case, we could just use on variable for example called durationsOnt_processed that we overwrite in every step, resulting in something like this:\ndurationsOnt_processed &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_processed &lt;- rename(durationsOnt_processed, Gender = Sex)\ndurationsOnt_processed &lt;- relocate(durationsOnt_processed, Speaker)\ndurationsOnt_processed &lt;- filter(durationsOnt_processed,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_processed &lt;- arrange(durationsOnt_processed, desc(Frequency))\nIn this process though, still everytime the output of the one call is the input for the next step (in the end it is an analysis workflow).\nIf we were to write textually what we did to the data would be something like:\n\n‚ÄúTake the durationsOnt dataset then\nSelect columns ‚Ä¶ then\nrename column Sex to Gender then\nmove column Speaker to the first column then\nmove column Gender after Speaker then\nselect cases of Females born after 1970 then\narrange by frequency in descending order.‚Äù\n\nThis is where the concept of a pipe was introduced in R. The pipe operator allows you to¬†pass the result of one function directly into the next function¬†as its first argument, without the need to explicitly write it. It‚Äôs widely used in the¬†tidyverse, especially with¬†dplyr.\nThe operator can be used with two syntax %&gt;% or the new |&gt; .\nUsing this, the examples before could be executed as below.\n\ndurationsOnt_processed &lt;- durationsOnt %&gt;% select(Word:DurationOfPrefix) %&gt;%\n                          rename(Gender = Sex) %&gt;%\n                          relocate(Speaker) %&gt;%\n                          relocate(Gender, .after = Speaker) %&gt;%\n                          filter((YearOfBirth &gt; 70) & (Gender==\"female\")) %&gt;%\n                          arrange(desc(Frequency))\n\nhead(durationsOnt_processed)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997\n\n\nAs you can see, the result is the case, but the code and data processing flow is much more readable, and there is no need to introduce intermediate variables.\nYou will become more familiar along the course with the basic usage of the tidyverse data manipulation.s\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005. ‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù The Journal of the Acoustical Society of America 118 (4): 2561‚Äì69.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "using_notebooks.html",
    "href": "using_notebooks.html",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "",
    "text": "R Scripts\nUp to now we have described the usage of R and RStudio as an interactive data analysis environment where we introduce commands and get outputs on the Console.\nAn important part of research and data analysis is being able to reproduce the analysis and work you produce and report.\nIn order to that, reproducible research includes:\nScripts are used to collect the commands and steps used in a data analysis. In the R language, a script is a text file with commands saved with extension .R.\nTo create a script, select File -&gt; New File -&gt; R Script\nThis will open a file in the Editor where you can type a series of commands and comments. Comments are lines starting with # .\nYou can execute the code you want to run highlighting it and pressing¬†Ctrl + Enter¬†(Windows) or¬†Cmd + Enter¬†(Mac) or with the Green ‚ÄúRun‚Äù button on the top right of the Editor window.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#r-markdown-and-notebooks",
    "href": "using_notebooks.html#r-markdown-and-notebooks",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "R Markdown and Notebooks",
    "text": "R Markdown and Notebooks\nAn alternative to scripts are Notebooks, which are a powerful way of documenting work by producing documents that mix plain text and code.¬†\nNotebooks include chunks of code that can be executed and the output displayed and included in the document, together with textual input formatted using R Markdown language. In fact the current workbook is written using this approach.\nA full description of R Markdown is beyond the scope of this course, but also not required to follow the content and exercises. In the section below a basic introduction is given to create a Notebook, use the Visual editor and generate PDF output to hand-in your assignments.\n\nNotebook instructions\n\n\n\n\n\n\nCaution\n\n\n\nBeware that the environment of a Notebook is not the same as the R session environment!\n\n\nTo create a notebook in R Markdown, you can select File -&gt; New File -&gt; R Notebook\n\nTo show the capabilities of the Notebook, we will look at the assignments to be delivered:\n\nOpen the file Assignment1.Rmd by clicking on the file on the name in the file tab.\nIf not active, select ‚ÄúVisual‚Äù in the edit mode on the upper left.\nEnter your name and Student ID in the author field.\n\n\n\nEnter the code to answer each of the questions in the relevant code section, that appear in grey and with a {r} marking.\nAfter entering the code, run it pressing the green arrow on the top right corner of the code section\nRemember to save from time to time!\n\n\nWhen you are ready to hand in your assignment you can generate a PDF to upload in Brightspace. This is the procedure to do it:\n\nClick on the small arrow next to the Knit button (see figure below).\nSelect ‚ÄúKnit to PDF‚Äù\nSave the PDF file generated somewhere in your drive or computer.\nUpload it to Brightspace on the relevant assignment.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#additional-reading",
    "href": "using_notebooks.html#additional-reading",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nIf you are interested into knowing more on the capabilities of R notebooks, you can use this resources.\n\n\n\n\n\n\nImportant\n\n\n\nThis material is for your own development but it is not required for the course. I add it here as additional information in case you need to use it in your own research\n\n\n\nR Notebooks guide.\nDatacamp R Notebook tutorial",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "assignment1_key.html",
    "href": "assignment1_key.html",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "Task#1: Creating a dataframe\nYou were asked to create a dataframe made up sample data according to the following prescription:\nThis can be done exactly as in the example we followed in Workgroup 1, with the steps replicated below:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task1-creating-a-dataframe",
    "href": "assignment1_key.html#task1-creating-a-dataframe",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "4 columns/variables: one column with a factor with two levels, another column with a factor with 3 levels and two columns with numeric values.\n6 rows or observations\n\n\n\n1.1. Create variables with the data.\nAs you were asked to create 6 observations, every variable contains six entries.\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\")\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\")\nNounLength&lt;-c(6,7,4,5,8,6)\nNounFrequency&lt;-c(638,799,390,569,567,665) \n\n\n\n1.2. Create factors\nThe variables created above with categorical entries are considered as character vectors. We should convert them into factors using the function factor() .\n\nNounAnimacy &lt;- factor(NounAnimacy)\nNounGender &lt;- factor(NounGender)\n\n\n\n\n\n\n\nTip\n\n\n\nThe following could be performed in a single step by nesting functions as in the code below:\nNounAnimacy &lt;- factor(c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\"))\nNounGender&lt;- factor(c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\"))\n\n\n\n\n1.3. Create a dataframe\nWe can define a dataframe based on the variables we created as:\n\ndf_example&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency)\ndf_example\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nIf you look at the Environment window, a new variable was created called example with the specified contents:\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that when creating the dataset using the data.frame() function it builds a copy of the data. Modifying the original variables will not change df_example . You can remove the variables to clean up the workspace using rm()\n\nrm(NounAnimacy,NounFrequency, NounGender, NounLength)\n\n\n\nOnly df_example is left in the environment now:\n\nand you can access the individual columns in the dataframe using the $ operator. For example to see the column NounGender\n\ndf_example$NounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task2-loading-required-libraries",
    "href": "assignment1_key.html#task2-loading-required-libraries",
    "title": "Assignment #1 - Answer key",
    "section": "Task#2: loading required libraries",
    "text": "Task#2: loading required libraries\nLooking at the content of the assignment tasks, we will use data from the languageR package and functions from the dyplr package, which is part of the tidyverse environment.\nTo use both of them we have to load the libraries first\n\n# add code to load the required libraries\nlibrary(languageR)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "href": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "title": "Assignment #1 - Answer key",
    "section": "Task#3: Working with and exploring dataframes",
    "text": "Task#3: Working with and exploring dataframes\nFrom the languageR library a sample dataframe called lexdec is used in this assignment. This dataset contains lexical decision latencies elicited from 21 subjects for 79 English concrete nouns.\n\n3.1. Load dataset in the environment\nTo use the lexdec dataset, we load it in the environment using the function data() . Although this step is not needed to access the data, it conveniently includes is as any other variable in the Environment window, so that it is possible to inspect it.\n\ndata(lexdec)\n\n\nYou can browse the information on the dataset in the documentation included with the package. You can do that with the help() function, using ? or searching in the help tab.\nhelp(lexdec)\n?lexdec\nThe help tab shows the description of each of the variables in the dataset\n\n\n\n3.2. Explore contents of the dataset\nYou were asked to show the first rows of the dataset.\nTyping the name of the dataset will show you the full contents, which is not too handy. Instead you can use View(lexdec) to load it in the RStudio Viewer, or, just to inspect he first rows, you can use the head() function as follows:\n\nhead(lexdec)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect\n1      A1 6.340359    23   F        English correct     word     correct\n2      A1 6.308098    27   F        English correct  nonword     correct\n3      A1 6.349139    29   F        English correct  nonword     correct\n4      A1 6.186209    30   F        English correct     word     correct\n5      A1 6.025866    32   F        English correct  nonword     correct\n6      A1 6.180017    33   F        English correct     word     correct\n        Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1        owl  4.859812  1.3862944   0.6931472      3 animal           54\n2       mole  4.605170  1.0986123   1.9459101      4 animal           69\n3     cherry  4.997212  0.6931472   1.6094379      6  plant           83\n4       pear  4.727388  0.0000000   1.0986123      4  plant           44\n5        dog  7.667626  3.1354942   2.0794415      3 animal         1233\n6 blackberry  4.060443  0.6931472   1.3862944     10  plant           26\n  FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1         74       0.7912 simplex -0.3101549 6.3582     3.12   3.4758\n2         30       0.6968 simplex  0.8145080 6.4150     2.40   2.9999\n3         49       0.4754 simplex  0.5187938 6.3426     3.88   1.6278\n4         68       0.0000 simplex -0.4274440 6.3353     4.52   1.9908\n5        828       1.2129 simplex  0.3977961 6.2956     6.04   4.6429\n6         31       0.3492 complex -0.1698990 6.3959     3.28   1.5831\n  meanWeight      BNCw      BNCc       BNCd BNCcRatio BNCdRatio\n1     3.1806 12.057065  0.000000   6.175602  0.000000  0.512198\n2     2.6112  5.738806  4.062251   2.850278  0.707856  0.496667\n3     1.2081  5.716520  3.249801  12.588727  0.568493  2.202166\n4     1.6114  2.050370  1.462410   7.363218  0.713242  3.591166\n5     4.5167 74.838494 50.859385 241.561040  0.679589  3.227765\n6     1.1365  1.270338  0.162490   1.187616  0.127911  0.934882\n\n\nAs can be seen from the output, by default it displays 6 rows. Looking at the documentation of the head() function, it describes that the function can take an argument n specifying how many rows to display.\n\nhead(lexdec, n = 4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency FamilySize SynsetCount Length  Class FreqSingular FreqPlural\n1  4.859812  1.3862944   0.6931472      3 animal           54         74\n2  4.605170  1.0986123   1.9459101      4 animal           69         30\n3  4.997212  0.6931472   1.6094379      6  plant           83         49\n4  4.727388  0.0000000   1.0986123      4  plant           44         68\n  DerivEntropy Complex      rInfl meanRT SubjFreq meanSize meanWeight      BNCw\n1       0.7912 simplex -0.3101549 6.3582     3.12   3.4758     3.1806 12.057065\n2       0.6968 simplex  0.8145080 6.4150     2.40   2.9999     2.6112  5.738806\n3       0.4754 simplex  0.5187938 6.3426     3.88   1.6278     1.2081  5.716520\n4       0.0000 simplex -0.4274440 6.3353     4.52   1.9908     1.6114  2.050370\n      BNCc      BNCd BNCcRatio BNCdRatio\n1 0.000000  6.175602  0.000000  0.512198\n2 4.062251  2.850278  0.707856  0.496667\n3 3.249801 12.588727  0.568493  2.202166\n4 1.462410  7.363218  0.713242  3.591166\n\n\n\n\n3.3 Extract the column names from the dataframe.\nAs per the workgroup notes, the names of the variables in the dataframe can be extracted using the colnames() function.\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n\n\n3.4. Sort dataframe\nYou were asked to sort the dataframe by the reaction time (RT variable). Sorting can be done in several ways, but we will use the tidyverse arrange() function as described in the workgroup.\n\nlexdec_ordered &lt;- arrange(lexdec,RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\nTo instead arrange in reducing order:\n\nlexdec_ordered &lt;- arrange(lexdec, desc(RT))\nhead(lexdec_ordered, n = 4)\n\n     Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n1194      T2 7.587311    44   F          Other incorrect  nonword     correct\n1619      M2 7.443664   105   F          Other   correct     word     correct\n1381      R1 7.425358   116   F        English   correct  nonword     correct\n1620      M2 7.403670   106   F          Other   correct     word     correct\n         Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1194  gherkin  2.079442          0    1.098612      7  plant            4\n1619     leek  3.332205          0    1.098612      4  plant            5\n1381 beetroot  3.555348          0    1.098612      8  plant           15\n1620 hedgehog  3.637586          0    1.098612      8 animal           21\n     FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1194          3            0 simplex  0.2231435 6.5161     3.16   2.2484\n1619         22            0 simplex -1.3437348 6.4200     3.68   1.8898\n1381         19            0 complex -0.2231435 6.4468     3.88   1.8705\n1620         16            0 simplex  0.2578291 6.5924     3.32   3.5920\n     meanWeight     BNCw    BNCc     BNCd BNCcRatio BNCdRatio\n1194     1.8185 0.111433 0.16249 0.237523  1.458184  2.131531\n1619     1.4590 0.846892 0.16249 2.375231  0.191866  2.804646\n1381     1.4442 0.635169 0.16249 0.475046  0.255822  0.747906\n1620     3.2545 2.718969 0.64996 3.325324  0.239047  1.223009\n\n\nAs an example, the previous can also be done using pipes. It does not make much difference in this task but it makes the code more readable when you perform several steps:\n\nlexdec_ordered &lt;- lexdec %&gt;% arrange(RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that on these examples, we saved the output in a new variable. If we were just calling the function as:\narrange(lexdec, RT)\nit would display on the console the output which is rather long and not easy to see.\n\n\nWe now have a new variable in the environment called lexdec_ordered containing a copy of the data from lexdec ordered by the value of RT .\n\n\n3.5. Select columns and rows\nYou were asked to create a new dataframe called lexdec_reduced with only the variables from Subject to Frequency and selecting only the entries from native English speakers (as coded in the NativeLanguage variable).\nAs described in the workgroup materials, two functions are used to select the data, one selecting the variables/columns of interest (select()) and another selecting the observations/rows based on a criteria (filter())\nLet‚Äôs see below how to do the same operation with and without using pipes.\n\n# Without pipes\n\nlexdec_reduced &lt;- select(lexdec,Subject:Frequency)\nhead(lexdec_reduced, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith this first command, we have selected a few columns from the original dataset. We can check that using for example colnames()\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n\ncolnames(lexdec_reduced)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"     \n\n\nAs you can see only the first columns (from Subject to Frequency) are retained in lexdec_reduced\nTo now select only the English native speakers, we use the function filter() :\n\nlexdec_filtered &lt;- filter(lexdec_reduced,NativeLanguage==\"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nTo check we can use the function unique() that tells us how many unique values are in a variable. The output below shows that NativeLanguage is a factor with two levels and both are present in the dataframe lexdec_reduced.\n\nunique(lexdec_reduced$NativeLanguage)\n\n[1] English Other  \nLevels: English Other\n\n\nIf we looked at the lexdec_filtered dataframe, it specifies that the variable is still a factor with two levels, but only ‚ÄúEnglish‚Äù is present in the data.\n\nunique(lexdec_filtered$NativeLanguage)\n\n[1] English\nLevels: English Other\n\n\nThe steps above could be performed using pipes:\n\n#Using pipes\n\nlexdec_filtered &lt;- lexdec %&gt;% \n                  select(Subject:Frequency) %&gt;% \n                  filter(NativeLanguage == \"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith identical results and a more readable code.\n\n\n3.6. Save dataframe\nFinally you were asked to save the filtered dataframe into a file in the /data directorate.\nLet‚Äôs save the data for example in text format using the write.table() function:\n\nwrite.table(lexdec_reduced,file = \"./data/reduced_data.txt\")\n\nThis command creates a file in the /data directory called reduced_data.txt\n\nWe can also save a copy in R format using saveRDS() if using R for further analysis.\n\nsaveRDS(lexdec_reduced,\"./data/reduced_data.Rda\")\n\n\n\nAnd this concludes the first assignment of the course! You will be gaining familiarity with R and RStudio along the workgroups.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "Workgroup2.html",
    "href": "Workgroup2.html",
    "title": "Workgroup 2: Data Exploration with R",
    "section": "",
    "text": "In this session we will continue to introduce basic concepts of data manipulation and plotting using the tidyverse package and start performing data exploration and descriptive statistics with R.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nPerform basic data transformation and reorganization.\nUnderstand concept of grammar of graphics and create basic plots.\nPerform basic descriptive statistics in R.\nCreate histograms and boxplot.\nCheck datasets for normality.\nWrite dataset summaries.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R"
    ]
  },
  {
    "objectID": "data_organization.html",
    "href": "data_organization.html",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Adding columns: mutate()\nAs mentioned in the previous workgroup, it is not in the scope of the course to cover all the functions available in the tidyverse framework. We introduce a subset of them useful to illustrate the concepts in the course.\nThe mutate() function, part of the dplyr package, allows to create new columns or variables in a dataset. It is used normally for values based in an existing column.\nAs an example, let‚Äôs use again the data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1. Dataset is in the file /ELP_full_length_frequency.csv in the /data directory.\nWe load the data as indicated in workgroup1\nlibrary(readr)\n\ndfELP &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(dfELP)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\nLet‚Äôs say we want to add two new calculated values:\nWe can add the two columns using mutate() as follows\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî purrr     1.1.0\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndfELP_extended &lt;- dfELP %&gt;% mutate(RT_per_char = RT/length, logRT = log10(RT))\nhead(dfELP_extended)\n\n# A tibble: 6 √ó 6\n  Word   Log10Freq length    RT RT_per_char logRT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.        151.  2.88\n2 zephyr      1.70      4  875.        219.  2.94\n3 zeroed      1.30      5  929.        186.  2.97\n4 zeros       1.70      5  625.        125.  2.80\n5 zest        1.54      4  659.        165.  2.82\n6 zigzag      1.36      6  785.        131.  2.90",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#adding-columns-mutate",
    "href": "data_organization.html#adding-columns-mutate",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Reading time per character: calculated dividing RT by length\nlogarithmic Reading Time : calculated using the log10() function\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTo avoid filling up the memory in the system, we will remove the variables after completing the examples when not using them again for a while\n\nrm(dfELP, dfELP_extended)",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#data-organization-long-and-wide-format",
    "href": "data_organization.html#data-organization-long-and-wide-format",
    "title": "Data organization and transformation",
    "section": "Data organization: long and wide format",
    "text": "Data organization: long and wide format\nA common task when preparing the analysis steps is to reshape the data in a way that can be used for the analysis.\nWe can widely characterize the data into two groups wide and long. (see section 3.9.4 of Field (2026) for details)\n\n\n\nFrom A. Field\n\n\nThe tidyr package contains two useful functions to allow to transform the data from one format to the other: pivot_wider() and pivot_longer() .\nAgain let‚Äôs look at one example based on data from an Event Related Potential (ERP) experiment. In ERP analysis one common approach is to compare the average amplitude of the signal measured in several electrodes on a time window of interest for the ERP component expected. The file EEG_DataSet_Wide.Rda in the /data directory contains an example data set with the average voltage measured in a set of electrodes in a time window of 250-500ms to investigate the so-called N400 ERP component.\nLet‚Äôs load and look at the data:\n\ndfEEG_wide &lt;- readRDS(\"./data/EEG_DataSet_Wide.Rda\")\nhead(dfEEG_wide, n=8)\n\n# A tibble: 8 √ó 64\n# Groups:   Subject [2]\n  Subject Condition    Fp1     Fpz    Fp2     AF3    AF4     F7     F5     F3\n  &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1       CondA     -3.05  -1.04   -2.88  -3.06   -3.31  -2.05  -2.06  -2.89 \n2 1       CondB     -1.03  -1.57   -0.842 -1.84   -0.361 -1.08  -1.06  -0.158\n3 1       CondC      0.575  1.52   -0.198  0.818   0.235  0.200  0.226  1.04 \n4 1       CondD     -2.20  -1.02   -1.63  -2.75   -1.90  -1.63  -1.62  -1.79 \n5 2       CondA      0.655  0.251   0.133  0.606   0.190  0.503  0.741  0.762\n6 2       CondB     -1.80  -2.13   -2.31  -2.33   -2.45  -1.23  -2.12  -2.12 \n7 2       CondC      0.145  0.0108  0.122  0.0548 -0.172  0.216  0.226  0.180\n8 2       CondD     -1.38  -1.66   -1.74  -0.980  -1.43  -0.797 -0.963 -0.944\n# ‚Ñπ 54 more variables: F1 &lt;dbl&gt;, Fz &lt;dbl&gt;, F2 &lt;dbl&gt;, F4 &lt;dbl&gt;, F6 &lt;dbl&gt;,\n#   F8 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FC5 &lt;dbl&gt;, FC3 &lt;dbl&gt;, FC1 &lt;dbl&gt;, FCz &lt;dbl&gt;, FC2 &lt;dbl&gt;,\n#   FC4 &lt;dbl&gt;, FC6 &lt;dbl&gt;, FT8 &lt;dbl&gt;, T7 &lt;dbl&gt;, C5 &lt;dbl&gt;, C3 &lt;dbl&gt;, C1 &lt;dbl&gt;,\n#   Cz &lt;dbl&gt;, C2 &lt;dbl&gt;, C4 &lt;dbl&gt;, C6 &lt;dbl&gt;, T8 &lt;dbl&gt;, TP7 &lt;dbl&gt;, CP5 &lt;dbl&gt;,\n#   CP3 &lt;dbl&gt;, CP1 &lt;dbl&gt;, CPz &lt;dbl&gt;, CP2 &lt;dbl&gt;, CP4 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   TP8 &lt;dbl&gt;, P7 &lt;dbl&gt;, P5 &lt;dbl&gt;, P3 &lt;dbl&gt;, P1 &lt;dbl&gt;, Pz &lt;dbl&gt;, P2 &lt;dbl&gt;,\n#   P4 &lt;dbl&gt;, P6 &lt;dbl&gt;, P8 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO5 &lt;dbl&gt;, PO3 &lt;dbl&gt;, POz &lt;dbl&gt;, ‚Ä¶\n\n\nThe table contains 64 columns, with one row representing the measurements on a particular condition per subject.\n\nColumn 1 - ‚ÄúSubject‚Äù - ID of the participant\nColumn 2 - ‚ÄúCondition‚Äù - factor representing the condition with four levels (CondA, CondB, CondC, CondD).\nColumns 3-64: Average Voltages in 250-500 ms time window after stimuli onset at 61 electrodes.\n\nFor the data analysis, we would like to actually see the effect on the Voltage measurement at different electrode sites, so we would like to actually have the data organized as:\n\n\n\nSubject\nCondition\nElectrode\nAmplitude\n\n\n\n\n1\nCondA\nFp1\n-3.047\n\n\n1\nCondA\nFpz\n-1.037\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nWe can do this with the pivot_longer() function, that has the following syntax:\npivot_longer(data, cols, names_to = \"name\", values_to = \"value\")\n\ndata : dataframe to be transformed\ncols: columns to be converted into longer format\nnames_to: name of the variable that will contain as values the names of the cols argument\nvalues_to: name of the variable that will contain values\n\nIn our example, we want to pivot all columns with electrode name. Normally you could provide a list, but since here we have several columns we can specify a range: ‚Äúfrom column Fp1 to CB2‚Äù\n\ndfEEG_long &lt;- pivot_longer(dfEEG_wide,cols = Fp1:CB2, names_to = \"Electrode\",values_to= \"AvgVoltage\") \nhead(dfEEG_long,n=8)\n\n# A tibble: 8 √ó 4\n# Groups:   Subject [1]\n  Subject Condition Electrode AvgVoltage\n  &lt;fct&gt;   &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 1       CondA     Fp1            -3.05\n2 1       CondA     Fpz            -1.04\n3 1       CondA     Fp2            -2.88\n4 1       CondA     AF3            -3.06\n5 1       CondA     AF4            -3.31\n6 1       CondA     F7             -2.05\n7 1       CondA     F5             -2.06\n8 1       CondA     F3             -2.89\n\n\nAs can be seen from the output, we have the data in the format expected\nTransforming into wide format is done similarly with the pivot_wider() function, that takes the following arguments:\n\ndata : dataframe to be transformed\nnames_from : column (or columns) to get the name of the output column\nvalues_from : column (or columns) to get the cell values from\n\nLet‚Äôs imagine that we would like to transform our last dataframe into a table of the form:\n\n\n\nSubject\nElectrode\nCondA\nCondB\nCondC\nCondD\n\n\n\n\n1\nFp1\n\n\n\n\n\n\n1\nFpz\n\n\n\n\n\n\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\nWe can do the following:\n\ndfEEG_wide_2 &lt;- pivot_wider(dfEEG_long,names_from = \"Condition\",values_from= \"AvgVoltage\") \nhead(dfEEG_wide_2,n=8)\n\n# A tibble: 8 √ó 6\n# Groups:   Subject [1]\n  Subject Electrode CondA  CondB  CondC CondD\n  &lt;fct&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1       Fp1       -3.05 -1.03   0.575 -2.20\n2 1       Fpz       -1.04 -1.57   1.52  -1.02\n3 1       Fp2       -2.88 -0.842 -0.198 -1.63\n4 1       AF3       -3.06 -1.84   0.818 -2.75\n5 1       AF4       -3.31 -0.361  0.235 -1.90\n6 1       F7        -2.05 -1.08   0.200 -1.63\n7 1       F5        -2.06 -1.06   0.226 -1.62\n8 1       F3        -2.89 -0.158  1.04  -1.79\n\n\n\n\n\n\n\n\nNote\n\n\n\nPivoting tables can be confusing at first. You will master it with practice on your own data. This section is included to illustrate a typical data manipulation and provide tips for your own analysis but will not be used in exercises or exams.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#further-reading",
    "href": "data_organization.html#further-reading",
    "title": "Data organization and transformation",
    "section": "Further reading",
    "text": "Further reading\nIf you want to go more into details here are links to good resources:\n\nR for data Science ( Wickham, √áetinkaya-Rundel, and Grolemund (2024)) Chapter 5 - Data tyding\nTidyverse article on Pivoting\n\n\n\n\n\nField, Andy P. 2026. Discovering Statistics Using R and RStudio. London: SAGE Publications.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "üìäData visualization in R",
    "section": "",
    "text": "Introduction to ggplot2\nR provides several ways to plot data, from simple plots using basic functions like plot() to complex visualization with specialized packages.\nFor this course we have chosen to introduce plotting based on the ggplot2 package, part of the tidyverse environment because of its versatility to generate almost any required visualization in the linguistics field. A full treatment of the capabilities of ggplot is beyond the scope of this course and you are not expected to know how to use it for the exams, but will need it for the assignments.\nFor a full description refer to Wickham (2016), available online.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#grammar-of-graphics",
    "href": "data_visualization.html#grammar-of-graphics",
    "title": "üìäData visualization in R",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\n\nThe name, ggplot, it is due to the underlying concept based on the notion of Grammar of Graphics ( Wilkinson (2011))\nThe concept behind ggplot2 divides a plot into different fundamental composable parts:\n\n\n\nImage extracted from ggplot introduction\n\n\nThe main components are\nPlot = Data + Mapping + Layers (or Geometry)\n\nData is a data frame with the source of the data to be represented.\nMapping is used to indicate with of the data elements are mapped to the aesthetics of the graph (i.e.¬†the x and y axis. It can also be used to control the color/ size / shape of points, the height of bars, etc.\nGeometry defines the type of graphics (i.e., histogram, box plot, line plot, density plot, dot plot, etc.).\n\nTo explain these concepts, let‚Äôs walk through an example of the steps to build a plot. We will make use again of the lexdec dataset in the languageR package.\n\nlibrary(languageR)\n\ndata(\"lexdec\")\n\nTo build a plot, first we have to indicate the data that will be used to ggplot\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec)\n\n\n\n\n\n\n\n\nThis command has created an empty canvas, since we have not mapped the data to the plot. We have to provide a mapping of which variables are to be displayed. We do this mapping with the aes() function. Let‚Äôs plot the Reading Time as a function of the Frequency of the word.\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT))\n\n\n\n\n\n\n\n\nNow we get a plot, with axis indicating the two elements we have mapped, but nothing is displayed. This is because we have not specified the geometry or representation.\nIf we want to make a scatter plot, we use the geom_point() function:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\n\nThe basic geometries in ggplot are:\n\ngeom_point() : scatter plot\ngeom_line() : line plot\ngeom_bar() : bar chart\ngeom_histogram() : histogram\ngeom_boxplot() : boxplot\n\nAdditional variables beyond x and y, can be considered using the colour or fill arguments. For instance, in the example above, we could color the points differently according to the NativeLanguage of the subject in the observation:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT, color=NativeLanguage)) + geom_point()\n\n\n\n\n\n\n\n\nIn the following we look at a couple of plots that we addressed in the first lectures",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#histograms",
    "href": "data_visualization.html#histograms",
    "title": "üìäData visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a representation of the frequency distribution of the data. It is created using the geom_histogram() function. Note that for this plot, only one variable is needed to be specified for the x-axis, since the other coordinate is the count of cases.\nFor example, to look at the distribution of the reaction time data:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can see that the data is only slightly right skewed, since it corresponds to a log transformed Reaction Time.\nIn a histogram, we can adjust the arguments bins and binwidth to determine the resolution of our grouping. The default is 30 bins distributed equally between the min and maximum value. Let‚Äôs see what happens if we change the value to 70:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nLet‚Äôs illustrate another capability of the layered concept to build graphs in ggplot: if we were interested to see if the distribution of reaction times is different between males and females, we could filter the data for each of the groups and plot two histograms (let‚Äôs do this using pipes this time)\n\n#histogram for females\n\nlexdec %&gt;% filter(Sex==\"F\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n#histogram for males\nlexdec %&gt;% filter(Sex==\"M\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nHowever we can also do it using facets in ggplot, which allows us to create multiple graphs based on a given variable. See the example below:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram() + facet_wrap(~Sex)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nNow both plots are places next to each other and, more importantly, with the same scale on the axis. By looking at the graphs, the distributions are similar, but the Female is higher, pointing to the fact that there were likely more female participants than males (remember the y-axis in a histogram is a count).\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying\n\n\n\n\n\n\nNote\n\n\n\nAdvanced:\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying in plot_histogram() a specific aesthetic aes(y =..density..)\n\nggplot(lexdec, aes(x=RT)) + geom_histogram( aes( y =..density..)) + facet_wrap(~Sex)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#boxplot",
    "href": "data_visualization.html#boxplot",
    "title": "üìäData visualization in R",
    "section": "Boxplot",
    "text": "Boxplot\n\nAnother representation of the distribution of the data that we discussed in the lecture is the boxplot. Boxplots can be plotted in ggplot using the geom_boxplot() function.\nIf we look again at the RT as a function of Sex can simply change the call as follows:\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot() \n\n\n\n\n\n\n\n\nOne observation is that the plot shows a number of points (circles) beyond the end of the ‚Äôwhisker‚Äô lines. If we look back at our definition in the lecture for the boxplot:\n\nThe box plot representation was based on the 5-point summary (min, max, median, Q1 and Q3) with the whiskers representing the min and max values.\nSoftware packages however, have a different implementation. By default geom_boxplot() places whisker edges at 1.5 times the Inter Quartile Range (IQR).\n\nYou can change the default behaviour by using the coef argument. For example, the call below extends the whiskers to 2*IQR.\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot( coef = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways check the manual of a function if you are not sure what is calculating/plotting.\nIn this case, looking at the help of page of geom_boxplot() you will find the following:\n‚ÄúThe upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‚Äúoutlying‚Äù points and are plotted individually‚Äù\n\n\n\n\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In Handbook of Computational Statistics: Concepts and Methods, 375‚Äì414. Springer.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Measures of central tendency\nWe described in the lecture three main measures of central tendency: the mean, the median and the mode.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-central-tendency",
    "href": "descriptive_stats.html#measures-of-central-tendency",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Mean\nThe mean is described in mathematically as:\n\\[\n\\overline{X}=\\frac{\\sum_{i=1}^{N}{X_i}}{N}\n\\]\nIf we have a list of numbers, we could calculate it using the function sum() as in\n\nx &lt;- c(1,3,5,6,8,2)\nsum(x) / 6\n\n[1] 4.166667\n\n\nThere is however a built in function mean() that does this calculation:\n\nmean(x)\n\n[1] 4.166667\n\n\n\n\nMedian\nSimilarly, the median can be calculated by ranking the values and selecting those in the middle, but there is a convenience function median()\n\nmedian(x)\n\n[1] 4\n\n\nIf we have a dataset with a large outlier, the median is a more robust measurement of central tendency compared to the mean.\n\ny &lt;- c(1,3,5,6,8,2,200)\nmean(y)\n\n[1] 32.14286\n\nmedian(y)\n\n[1] 5\n\n\n\n\nMode\nThere is no built in function to calculate the mode in R. It is simply the value repeated the most in a dataset. The function table() can be used to create a count of the number of elements in a dataframe as a function of variable (so-called contingency tables). We will see in the next sections how to do that with tidyverse.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#point-summaries",
    "href": "descriptive_stats.html#point-summaries",
    "title": "Descriptive Statistics",
    "section": "5-point summaries",
    "text": "5-point summaries\nThere are several ways to calculate the five point summaries discussed in the lecture for a dataset. Using individual functions:\n\nmin() : minimum of a set of numbers\nmax() : maximum of a set of numbers\nquantile() : calculates the quantile based on a threshold.\n\nquantile(x, prob=0.25) provides the first quartile Q1\nquantile(x, prob=0.75) provides the third quartile Q3\nquantile(x, prob=0.5) corresponds to the median\n\nIQR() : provides the Inter-Quartile Range, that is equivalent to Q3 - Q1\nrange() : difference between maximum and minimum values\nmean()\nmedian()\n\nTaking again the example of the lexdec dataset and the Reaction Time RT\n\nlibrary(languageR)\n\ndata(lexdec)\n\nmin(lexdec$RT)\n\n[1] 5.828946\n\nmax(lexdec$RT)\n\n[1] 7.587311\n\nrange(lexdec$RT)\n\n[1] 5.828946 7.587311\n\n\n\nquantile(lexdec$RT, prob = 0.25)\n\n     25% \n6.214608 \n\nquantile(lexdec$RT, prob = 0.75)\n\n    75% \n6.50204 \n\n#IQR calculated from quantiles\nquantile(lexdec$RT, prob = 0.75) - quantile(lexdec$RT, prob = 0.25)\n\n     75% \n0.287432 \n\nIQR(lexdec$RT)\n\n[1] 0.287432\n\n\nHowever, most of the values above can be calculated using the convenient summary() function\n\nsummary(lexdec$RT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.829   6.215   6.346   6.385   6.502   7.587",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-dispersion",
    "href": "descriptive_stats.html#measures-of-dispersion",
    "title": "Descriptive Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\n\nStandard deviation and variance\nAs described in the lecture the standard deviation of a dataset is calculated as follows:\n\nfor the full population: \\(\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\mu)^2}}{N}}\\)\nfor a sample of the population with the un-biased estimator: \\(s = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\overline{X})^2}}{N-1}}\\)\n\nIn R, the function sd() calculates the sample standard deviation. There is no built in function to calculate the full population \\(\\sigma\\) , but in general, for N &gt; 30 they are very close\n\nsd(lexdec$RT)\n\n[1] 0.2415091",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-summaries-with-tidyverse",
    "href": "descriptive_stats.html#data-summaries-with-tidyverse",
    "title": "Descriptive Statistics",
    "section": "Data summaries with tidyverse",
    "text": "Data summaries with tidyverse\nIn most cases, we have a most complex data structure where we want to calculate the summary statistics not only globally but in several cases of conditions.\nFor example, if we want to calculate the average and number of cases of the Reaction Time in the lexdec dataset for Male and Female and also between Native and non-native English speakers. We could perform it by individually filtering each group combination and calculating the mean and number of cases (length() provides the number of rows) as below:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Male and Native English Speaker\nlexdec_M_english &lt;- lexdec %&gt;% filter( (Sex==\"M\") & (NativeLanguage==\"English\"))\nmean(lexdec_M_english$RT)\n\n[1] 6.361977\n\nlength(lexdec_M_english$RT)\n\n[1] 395\n\n\nAnd this should be repeated for each of the four groups (Male-English, Male-Other, Female-English, Female-Other)\nR has several ways to simplify these calculations. We will look at the approach with tidyverse packages by making use of the functions group_by() and summarize().\ngroup_by() allows to specify grouping variables that would be applied to the next operation on the pipe. The function summarize() allows to create summary variables with the statistics of choice, including mean, sd, IQR, median, min, max, etc‚Ä¶ (look at the online help).\nThe two functions can be combined as in the example below, where we group by Sex and NativeLanguage and ask to create three columns with names numObs , avgRT and sdRT containing the number of observations, mean Reaction Time and Standard Deviation Time respectively.\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 5\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 F     English           553  6.29 0.195\n2 F     Other             553  6.47 0.274\n3 M     English           395  6.36 0.195\n4 M     Other             158  6.50 0.224\n\n\nWhen reporting summary descriptive statistics, it is common practice to provide the Standard Error (SE) that is the error on the sampling of the mean, which is calculated as\n\\[\ns_{\\overline{X}} = \\frac{s}{\\sqrt{N}}\n\\]\nWe can extend the function above to add a column with the SE calculation, which can be done on the basis of the new columns sdRT and numObs:\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT), seRT = sdRT / sqrt(numObs))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 6\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT    seRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 F     English           553  6.29 0.195 0.00830\n2 F     Other             553  6.47 0.274 0.0117 \n3 M     English           395  6.36 0.195 0.00981\n4 M     Other             158  6.50 0.224 0.0178",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "href": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "title": "Descriptive Statistics",
    "section": "Reporting Summary Descriptive Statistics",
    "text": "Reporting Summary Descriptive Statistics\nFor the reporting of each group of data, either a tabular form or descriptive paragraph can be used, providing the values as calculated. An example for the lexdec example we used could be as follows:\n\n\n\n\n\n\nNote\n\n\n\n‚ÄúThe logarithmically transformed reaction time of Female English Native Speakers (M = 6.29, SE = 0.01, n = 553) was faster than Male English Native Speakers (M=6.36, SE = 0.01, n=395), which was faster than both Female (M=6.47, SE=0.01, n=553) and Male (M=6.50, SE=0.02, n=158) non-native Speakers.‚Äù\n\n\nNote that the mean is denoted by a capital M. In this case we don‚Äôt have yet a statistical analysis to determine if the difference observed is significant. That should be reported as well and we will address it in the next lectures.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-normality-checks",
    "href": "descriptive_stats.html#data-normality-checks",
    "title": "Descriptive Statistics",
    "section": "Data Normality checks",
    "text": "Data Normality checks\nWe mentioned in the lectures the importance to be able to check the normality of data or, as we will see of the residuals of a model fit, to ensure that our conclusions based on the statistical assessment of the results of a linear model are valid. In this section we briefly present how to perform in R some of the measures to determine deviation from normality.\n\nSkewness and Kurtosis\nSkewness and Kurtosis of a distribution can be computed using functions in the moments package. Before using it, we have to install the package\n\n#install.packages(\"moments\")\nlibrary(moments)\n\nThe functions are easily called, kurtosis() and skewness() . Let‚Äôs see an example based on the lexdec dataset:\n\nskewness(lexdec$RT)\n\n[1] 0.9930124\n\n\nA positive number implies a right-tailed distribution. If we plot again the histogram for the RT data, this is clearly visible.\n\nlexdec %&gt;% ggplot(aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\n\nIf now we calculate the kurtosis:\n\nkurtosis(lexdec$RT)\n\n[1] 4.405579\n\n\nA positive value indicates a distribution that is more concentrated in the center than a normal distribution.\nSo both measures point to a distribution deviating from normality on the data.\n\n\nQ-Q plots\nA Q-Q plot or ‚ÄúQuantile-Quantile‚Äù Plot, can be used to compare two distributions. It plots the quantiles from the measured data against what a theoretical or other distribution quantile would look like.\nA straight line implies that the two distributions compared are very similar. this is done in R using the qqplot() function. To test for normality, we want to compare the data against a thoretical normal distribution. This a particular case implemented with the function qqnorm() .\nIf we look in our running example\n\nqqnorm(lexdec$RT)\nqqline(lexdec$RT)\n\n\n\n\n\n\n\n\nqqline() adds a reference line for comparison. Clearly the data deviates from the line noting again the non-normality of the data.\nAn alternative to the above two commands is to use the qqPlot() function in the car package that produces charts with an error area more adequate for publication and reporting.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nqqPlot(lexdec$RT)\n\n\n\n\n\n\n\n\n[1] 1273  750\n\n\n\n\nShapiro-Wilk tests\nFinally, a number of tests can be used to provide a quantified assessment of the normality or not of the data.\nHere we present the Shapiro-Wilk test. This test considers a null hypothesis that the data is normally distributed. A significant outcome implies that the null hypothesis is not maintained and that the data is not normally distributed. It is implemented by the shapiro.test() function\n\nshapiro.test(lexdec$RT)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lexdec$RT\nW = 0.94738, p-value &lt; 2.2e-16\n\n\nA p-value lower than a defined threshold implies that the null hypothesis is rejected. In this case, p&lt;0.001 so we will assess the data as not being normally distributed.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "assignment2_key.html",
    "href": "assignment2_key.html",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "",
    "text": "Task#1: Introduction to the dataset\nExercises in this assignment used a dataset with real data subset from a real Self-Paced Reading study on Negative Polarity Items and complementizer agreement in Basque (see Pablos, L., & Saddy, D. (2009). Negative polarity items and complementizer agreement in Basque. in Alter, K., Horne, M., Lindgren, M., Roll, M., & von Koss Torkildsen, J.(Eds.), Papers from Brain Talk: Discourse with and in the Brain. The 1st Birgit Rausing Language Program Conference in Linguistics. Lund: Lund University, Media Tryck. ISBN: 978-91-633-5561-5.)\nThis data is a real set with the outputs generated by the software Linger created by the MIT Ted Lab to easily conduct Self Paced reading experiments",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task1-introduction-to-the-dataset",
    "href": "assignment2_key.html#task1-introduction-to-the-dataset",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "",
    "text": "1.1 Load required libraries for the assignment\nThe packages required for this assignment are tidyverse and moments for the kurtosis and skewness calculations.\n\n\n1.2 Load dataset\nThe data for the exercise is contained in the file BasqueNPI.Rda in the /data folder.\nThis is a R format file, containing a data table. The data was saved using saveRDS() function so it can be read using readRDS() as described in the first workgroup section describing how to load a R format file.\n\n#Read dataset\ndfBasqueNPI &lt;- readRDS('./data/BasqueNPI.Rda')\n\n\n\n\n\n\n\nTip\n\n\n\nThe file location can be indicated with absolute or relative paths. The relative paths refers to the current working directory, that is indicated with ./ , therefore ./data/BasqueNPI.Rda can be read as: ‚Äúfile BasqueNPI.Rda in the data folder under the current directory‚Äù).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA few of you got an error when trying to open the file, saying that the file was corrupted when using the load() function in R.\nsaveRDS() and readRDS() allow to save a single R object to a file and to restore the object, . This differs from save() and load(), which save and restore one or more variables into the environment.\n\n\nAfter this step you have a dataframe in the environment, with the following variables\n\nEXPT - Specifies the type of data of the item collected, it contains three different values:\n\npractice: items presented at the beginning of the experiment to practice the method\nfiller: items included in the experiment to ‚Äòdisguise‚Äô the real measured target sentences.\nbasquenpi: target items corresponding to the experimental manipulation.\n\nItem ‚Äì Factor identifying the sentence used (coded as a number from 1 to 72).\nSubject ‚Äì Factor identifying the participant on the experiment (coded as a number from 1 to 32)\nEmbeddedSubject ‚Äì Factor/predictor indicating the nature of the embedded subject with the following levels:\n\n[Empty] ‚Äì for practice and filler items\nNP ‚Äì for target sentences with a Noun Phrase as subject\nNPI ‚Äì for target sentences with a Negative Polarity Item as subject\n\nAgreementMorphology ‚Äì Factor/predictor indicating the nature of agreement with the following levels:\n\n[Empty] ‚Äì for practice and filler items\nDeclarative ‚Äì for target sentences that contained a complementizer with declarative morphology\nPartitive ‚Äì for target sentences that contained a complementizer with partitive morphology\n\nSequenceBin ‚Äì the position of the item in the sequence seen by the subject.\nWordNumber ‚Äì the word number in the sentence (starting with 1)\nWord ‚Äì Actual word presented\nRegionNumber ‚Äì Region number for the analysis of the reading time\nRWRT ‚Äì RaW (recorded) Reading Time of the word\nRWZS ‚Äì Z-Score of the raw reading time\nRSRT ‚Äì Calculated Residual Reading Time\nRSZS ‚Äì Z-Score of the residual reading time\nQPCT ‚Äì Correctness of the comprehension response (100 ‚Äì correct; 0 ‚Äì incorrect)",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task2-explore-data-calculate-summary-statistics",
    "href": "assignment2_key.html#task2-explore-data-calculate-summary-statistics",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "Task#2: Explore Data & Calculate Summary Statistics",
    "text": "Task#2: Explore Data & Calculate Summary Statistics\n\n2.1 Select a subset of the data\nSelect a subset of data with only the target items from the experiment\n\n#Your code\nbasquenpi_data &lt;- dfBasqueNPI %&gt;% filter(EXPT==\"basquenpi\")\nhead(basquenpi_data)\n\n       EXPT Item Subject SequenceBin WordNumber       Word RegionNumber RWRT\n1 basquenpi    5       1          11          1      Kepak            1  554\n2 basquenpi    5       1          11          2   ziurtatu            2  702\n3 basquenpi    5       1          11          3         du            3  412\n4 basquenpi    5       1          11          4 ikastolara            4  552\n5 basquenpi    5       1          11          5       inor            5  393\n6 basquenpi    5       1          11          6  autobusez            6  642\n    RWZS    RSRT   RSZS QPCT EmbeddedSubject AgreementMorphology\n1 -0.611   14.48 -0.049  100             NPI           Partitive\n2 -0.025   -0.71  0.224  100             NPI           Partitive\n3 -0.636   35.67 -0.259  100             NPI           Partitive\n4 -0.687 -259.50 -0.624  100             NPI           Partitive\n5 -0.704  -92.13 -0.321  100             NPI           Partitive\n6 -0.301 -115.11 -0.243  100             NPI           Partitive\n\n\n\n\n2.2 Plot histograms of the data.\n\nPlot histogram of Raw Reading Time (RWRT)\nTo plot a histogram of the Raw Reading Time (RWRT) we use ggplot()\n\n#Plot histogram\nbasquenpi_data %&gt;% ggplot(aes(x = RWRT)) + geom_histogram()\n\n\n\n\n\n\n\n\nWhat can we observe in this distribution?\n\nIt is a disribution skewed to the right with a lower limit of 0 -&gt; Since RWRT measures the Raw Reading Time, the minimum reading time is zero, obviously, as it can not contain negatives values.\nThe graph extends above 10000. Although not visible, that implies that there are\nMost values seem to be between 0 and 2000: the default plot created by geom.histogram() uses 30 subdivisions or bins. To have a more fine grouping, we can indicate the number of bins used with the argument bins\n\nbasquenpi_data %&gt;% ggplot(aes(x = RWRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\n\n\n\nNow it can be clearly seen that the bigger proportion of reading times is between 500 and 1000 ms.\nThe distribution is very right skewed because there are probably a few measurements with very long reading time (e.g.¬†possibly a participant loosing attention and not pressing a key).\n\nWe can check the histogram only for values of the RWRT less than a certain limit.\nWe will normally perform a filtering of outliers based on the knowledge of our experiement and the intial data exploration. For example, looking at the plot above, we could eliminate those trials with RWRT &gt; 3000 ms considering that they are too long to be valid. Let‚Äôs plot a new histogram with only those trials with RWRT &lt; 3000.\n\nbasquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% ggplot(aes(x = RWRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\n\nNow we can see that the distribution is more symmetric, after removing outlayer points with RWRT &gt; 3000. If we want to see how many trials we have removed with the filtering, we can calculate the percentage of trial removed\n\n\\(Percentage\\,removed\\,trials = \\dfrac{n_{total}-n_{filtered}}{n_{total}}*100\\).\nThere are many ways to know the size of observations in a dataframe (i.e.¬†number of rows). The example below uses the function nrow() . We also demonstrate how you can write the results with a message on the screen using cat()\n\n# calculation\nn_total &lt;- basquenpi_data %&gt;% nrow()\nn_filtered &lt;- basquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% nrow()\n\nn_removed &lt;- (n_total - n_filtered)/n_total * 100\n\ncat(\"Total number of observations: \", n_total, \"\\n\")\n\nTotal number of observations:  7328 \n\ncat(\"Number of observations with RWRT &gt; 3000: \", (n_total - n_filtered) , \"\\n\")\n\nNumber of observations with RWRT &gt; 3000:  44 \n\ncat(\"Percentage of data removed: \", n_removed, \"%\",\"\\n\")\n\nPercentage of data removed:  0.6004367 % \n\n\nRemoving data points with RWRT &gt; 3000, only results in 0.6% of the data being discarded.\n\n\nPlot a histogram of the Residual Reading Time (RSRT)\nThe Residual Reading Time (RSRT) is a way to correct for sentence length, word length, and individual differences between participants‚Äô reading speeds. It is referenced to the average reading time per participant, and can have negative values (indicating faster reading than the average). We will look at the way it is calculated in an assignment once we have gone over the principles of regression.\nFor the purpose of this exercise we can plot the histogram to compare with the RWRT.\n\nbasquenpi_data %&gt;% ggplot(aes(x = RSRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\nProvide your observations on the distribution:\n\nThe distribution is roughly centered around 0, and it is more symmetric than the RWRT.\nLet‚Äôs plot the distribution of RSRT filtering the data as above based on the RWRT (RWRT&lt;3000).\n\nbasquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% ggplot(aes(x = RSRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\n,We can see that the distribution is centered around 0 and still skewed to the right.\n\n\n\n\n2.3 Calculate Skewness and kurtosis for both RWRT and RSRT\nSkewness and kurtosis values using the functions with the same name in the library moments.\nCalculate Skewness for RWRT\n\nskewness(basquenpi_data$RWRT)  \n\n[1] 6.682073\n\n\nWhat does the output value indicate?\n\nA positive skewness value means that the distribution is right skewed. This is quite clear looking at the histogram above.\n\nCalculate Kurtosis for RWRT\n\nkurtosis(basquenpi_data$RWRT)\n\n[1] 96.25985\n\n\nWhat does the output value indicate?\n\nA positive kurtosis indicates a distribution more peaked than a normal distribution with the same mean and standard deviation.\n\nBelow we calculate the same values for the RSRT\n\nskewness(basquenpi_data$RSRT) \n\n[1] 7.348828\n\nkurtosis(basquenpi_data$RSRT)\n\n[1] 118.0003\n\n\n\nAs can be seen the RSRT is even more skewed and with higher kurtosis (more ‚Äòpointy‚Äô) than the RWRT distribution.\n\nFinally let‚Äôs look at the values on the dataset for values of RT &lt; 3000 ms\n\nbasquenpi_filtered&lt;-basquenpi_data %&gt;% filter(RWRT &lt; 3000)\n\nskewness(basquenpi_filtered$RWRT)\n\n[1] 2.377025\n\nkurtosis(basquenpi_filtered$RWRT)\n\n[1] 10.67131\n\nskewness(basquenpi_filtered$RSRT)\n\n[1] 2.226802\n\nkurtosis(basquenpi_filtered$RSRT)\n\n[1] 11.14173\n\n\nThe value show that the distribution is closer to a normal (not normal though) in both measures after the filtering as expected.\n\n\n2.4 Calculate mean and standard deviation\nCalculate the mean and standard deviation of the Raw Reading Time (RWRT) for the different experimental conditions in Region Number 5 to generate a summary table.\nThere are many ways to do this in R. Below we include one way using tidyverse summarize function:\n\nbasquenpi_data %&gt;% filter(RegionNumber==5) %&gt;% group_by(EmbeddedSubject,AgreementMorphology) %&gt;% summarize(m_rwrt = mean(RWRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 √ó 3\n# Groups:   EmbeddedSubject [2]\n  EmbeddedSubject AgreementMorphology m_rwrt\n  &lt;fct&gt;           &lt;fct&gt;                &lt;dbl&gt;\n1 NP              Declarative           751.\n2 NP              Partitive             751.\n3 NPI             Declarative           662.\n4 NPI             Partitive             587.\n\n\n\n\n\n\n\n\n\n\n\nMean\n\nEmbedded Subject\n\n\n\n\n\n\n\nNP\nNPI\n\n\nAgreement Morphology\nDeclarative\n750.70\n661.67\n\n\n\nPartitive\n751.33\n586.60\n\n\n\nA similar approach can be followed to calculate the Standard deviations table\n\nbasquenpi_data %&gt;% filter(RegionNumber==5) %&gt;% group_by(EmbeddedSubject,AgreementMorphology) %&gt;% summarize(sd_rwrt = sd(RWRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 √ó 3\n# Groups:   EmbeddedSubject [2]\n  EmbeddedSubject AgreementMorphology sd_rwrt\n  &lt;fct&gt;           &lt;fct&gt;                 &lt;dbl&gt;\n1 NP              Declarative            357.\n2 NP              Partitive              441.\n3 NPI             Declarative            808.\n4 NPI             Partitive              275.\n\n\n\n\n\n\n\n\n\n\n\nStd Dev\n\nEmbeddedSubject\n\n\n\n\n\n\n\nNP\nNPI\n\n\nAgreementMorphology\nDeclarative\n357.30\n807.98\n\n\n\nPartitive\n440.71\n274.91\n\n\n\n\n\n2.5 Calculate five-points summary\nCalculate and create a table with the five-point summary (i.e., min, max, median, 1st quartile(Q1) and 3rd quartile(Q3)) for the Residual Reading Time (RSRT) for the following subset of data:\n\nEmbeddedSubject = NP\nAgreementMorphology = Partitive\nRegion Number = 5\n\nWe can use the summary() function to calculate the 5-point summary.\n\nbasquenpi_data_subset &lt;- basquenpi_data %&gt;% filter( (RegionNumber == 5) & (AgreementMorphology==\"Partitive\") &(EmbeddedSubject == \"NP\")) \n\nsummary(basquenpi_data_subset$RSRT)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-495.320  -90.130    0.985   83.313  120.080 3089.090 \n\n\n\n\n\nMin\nMax\nmedian\n1st Quartile\n3rd Quartile\n\n\n\n\n-495.32\n3089.09\n0.985\n-90.13\n120.08\n\n\n\n\n\n2.6 Produce a boxplot for RWRT\n\n2.6.1. Create a boxplot for RSRT\nProduce a boxplot for the RWRT in different conditions reflected in the table of task 2.4: in Region 5, for the conditions Declarative NP, Declarative NPI, Partitive NP, Partitive NPI.\n\nbasquenpi_data %&gt;% filter(RegionNumber == 5) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RWRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\n\nWhat can you observe in the plot?:\n\nThere is a clear outlier in the NPI-Declarative condition.\nThe NP-Partitive condition seems to have more dispersion that the other ones.\n\nIf we plot again the same filtering the data to keep only the points with RWRT &lt; 2500 :\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RWRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\n\nWhat differences do you observe with the previous plot?\n\nNow it is clearer to appretiate what is the difference between the different conditions.\nThis plot allows a clear comparison between the different levels of AgreementMorphology, but what is we wanted to compare NPI vs NP within each level of AgreementMorphology? We can exchange the variables between x and the fill arguments.\n\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=AgreementMorphology, y= RWRT, fill = EmbeddedSubject)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n2.6.3. Create a boxplot for RSRT\nAs in the previous section, we create now the boxplot for RSRT as follows:\n\nbasquenpi_data %&gt;% filter(RegionNumber == 5) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RSRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\n\nAgain if we filter the data with a RWRT &lt; 2500\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RSRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n2.6.4. Comment on the differences between the RWRT and RSRT plot.\n\nIn the RWRT plots it can be seen that NPI conditions hae lower dispersion than NP conditions (boxes are smaller)\nthe RSRT is centered around 0 ms, as expected, however there are still a very significant number of outliers, with residuals over 1000 ms. This requires assessment during data analysis. We will look at this in future assignments.\n\n\n\n\n2.7 Normality checks\nTo check the distribution of the data, we saw in the lecture that a Quantile-Quantile Plot (qqplot) can be used. The function qqplot() is generic and compares some data with any type of distribution. To compare your data with a normal distribution and check normality, you can use qnorm().\nWe generate the Quartile-Quartile plot for RWRT using the command below\n\nqqnorm(basquenpi_data$RWRT)\nqqline(basquenpi_data$RWRT, col = 'red',lty='dashed')\n\n\n\n\n\n\n\n\nThe plot deviates clearly from a straight line, as expected by the skewness and kurtosis values calculated in task 2.2.\n\n\n\n\n\n\nNote\n\n\n\nIn the code we have used qqline() to plot a reference straight line for visual comparison of the separation from the expected line in a normal distribution.\n\n\nFor comparison, let‚Äôs look at the q-q plot for the filtered data\n\nbasquenpi_data_filtered &lt;- basquenpi_data %&gt;% filter(RWRT&lt;2500)\nqqnorm(basquenpi_data_filtered$RWRT)\nqqline(basquenpi_data$RWRT, col = 'red',lty='dashed')\n\n\n\n\n\n\n\n\nAlthough the plot is more linear, still deviates from the normal distribution.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task-3---calculate-data-summaries",
    "href": "assignment2_key.html#task-3---calculate-data-summaries",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "Task #3 - Calculate data summaries",
    "text": "Task #3 - Calculate data summaries\nThe assignment asked to generate a summary dataframe (basquenpi_data_summary ) using the filter(), group_by() and summarize()functions in dplyr with the data grouped by the factors RegionNumber, EmbeddedSubject and AgreementMorphology, and summarized via the addition of columns specifying the mean and standard deviations of the Raw Reading Time and Residual Reading Time.\n\n\n\n\n\n\nWarning\n\n\n\nSeveral of you pointed out that the filter() function was not required to be used in this task, which is correct starting from the data already filtered in task 2.1.\nIn the code below I assumed to perform a full processing chain from raw data to illustrate how using pipes can render readable code with all operations of the data together.\nOf course both approaches are correct.\n\n\nWe can generate the requested output with the following code line:\n\nbasquenpi_data_summary &lt;- dfBasqueNPI %&gt;% filter(EXPT == \"basquenpi\") %&gt;%   #filter the data\n              group_by(EmbeddedSubject,AgreementMorphology, RegionNumber) %&gt;% #apply a grouping\n              summarize(mean_rt = mean(RWRT), sd_rt=sd(RWRT), mean_rs = mean(RSRT), sd_rs= sd(RSRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject', 'AgreementMorphology'.\nYou can override using the `.groups` argument.\n\nhead(basquenpi_data_summary,20)\n\n# A tibble: 20 √ó 7\n# Groups:   EmbeddedSubject, AgreementMorphology [2]\n   EmbeddedSubject AgreementMorphology RegionNumber mean_rt sd_rt mean_rs sd_rs\n   &lt;fct&gt;           &lt;fct&gt;                      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 NP              Declarative                    1    789.  436.   41.4   388.\n 2 NP              Declarative                    2    727.  395.  -36.3   360.\n 3 NP              Declarative                    3    611.  414.  104.    369.\n 4 NP              Declarative                    4    893.  492.   16.1   406.\n 5 NP              Declarative                    5    751.  357.   84.3   330.\n 6 NP              Declarative                    6    759.  352.  -28.2   312.\n 7 NP              Declarative                    7    627.  292.  -68.2   268.\n 8 NP              Declarative                    8    515.  186.  -82.3   177.\n 9 NP              Declarative                    9    681.  406.  -71.6   352.\n10 NP              Declarative                   10    700.  554.  -79.8   557.\n11 NP              Partitive                      1    745.  371.   -3.58  338.\n12 NP              Partitive                      2    713.  273.  -49.1   277.\n13 NP              Partitive                      3    573.  288.   65.7   260.\n14 NP              Partitive                      4    971.  716.  102.    664.\n15 NP              Partitive                      5    751.  441.   83.3   413.\n16 NP              Partitive                      6    786.  405.   -1.84  362.\n17 NP              Partitive                      7    621.  299.  -74.4   288.\n18 NP              Partitive                      8    613.  321.  -29.2   297.\n19 NP              Partitive                      9    852.  950.  112.    911.\n20 NP              Partitive                     10    865.  560.   56.2   562.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#extra-credit---task4-plotting-with-ggplot2",
    "href": "assignment2_key.html#extra-credit---task4-plotting-with-ggplot2",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "EXTRA CREDIT - Task#4: Plotting with ggplot2",
    "text": "EXTRA CREDIT - Task#4: Plotting with ggplot2\nThe plot we want to create shows the mean RWRT for each region for the conditions where EmbeddedSubject==NP, for the two different levels of AgreementMorphology.\nWe present below the step-by-step code to build up the plot, and at the end we put it all together in a single code line.\nFrom the dataframe created in task 3.2 first we select the subset of cases where EmbeddedSubject==NP\n\nbasque_npi_summary_NP &lt;- basquenpi_data_summary %&gt;% filter(EmbeddedSubject==\"NP\")\n\nFirst step is to create a definition for a plot. With the command below, we create a plot based on the data in basque_npi_summary_NP and save the plot definition in a variable called p.¬†Typing the name of the variable shows the information from the plot as we build it.\n\np&lt;-ggplot(basque_npi_summary_NP) \np\n\n\n\n\n\n\n\n\nAt this point, nothing is shown, because we only specified that we will make a plot based on the data. The next step is to add the Aesthetics. This specifies what data the plot will use:\n\np&lt;-p+aes(x=RegionNumber, y=mean_rt) \np\n\n\n\n\n\n\n\n\nWith the command above, we informed that we will plot the mean_rt as a function of the RegionNumber. Still nothing is displayed, because we haven‚Äôt said yet how we plot it (lines, points, bars, etc). As you can see, we don‚Äôt need to provide in the commad the name of the variable containing the columns mean_rt and RegionNumber, because p knows which is the underlying data of the graph.\nWith the next command, we specify that we want to have a line using geom_line(), where each type of line corresponds one of the levels of AgreementMorphology\n\np+geom_line(aes(linetype=AgreementMorphology))\n\n\n\n\n\n\n\n\nNow, as you can see we get a graph!\nOn purpose, I did not saved the result back in the p variable, to illustrate a point. What if we wanted to have instead a barchart? We just call geom_bar() instead.\n\np+geom_bar(stat=\"identity\",aes(fill=AgreementMorphology),position=position_dodge() )\n\n\n\n\n\n\n\n\nComing back to our example, let‚Äôs add data points to the chart using geom_point(), where each point shape is also dependent on the level of Agreement Morphology.\n\np&lt;-p + geom_line(aes(linetype=AgreementMorphology))+\n       geom_point(aes(shape=AgreementMorphology)) \np\n\n\n\n\n\n\n\n\nWe have now all the information we wanted in the chart and stored in the variable p. For the axis, the RegionNumber only makes sense as a discrete value. The command below adds information on the x scale, defining it a continuous with values in a sequence from 1 to 10 in steps of 1.\n\np &lt;- p + scale_x_continuous(breaks = seq(1,10,1)) \np\n\n\n\n\n\n\n\n\nThe last step is to complete the decorations of the graph. We can do that using a number of functions from the ggplot2 library.\n\np + scale_color_grey() + theme_classic()\n\n\n\n\n\n\n\n\nThis is the plot requested to reproduce.\nIf we put the code together, we could do the following\n\np &lt;- basquenpi_data_summary %&gt;% filter(EmbeddedSubject==\"NP\") %&gt;%\n                                ggplot(aes(x = RegionNumber, y = mean_rt)) +\n                                geom_line(aes(linetype=AgreementMorphology)) + \n                                geom_point(aes(shape=AgreementMorphology)) +\n                                scale_x_continuous(breaks = seq(1,10,1)) +\n                                scale_color_grey() + \n                                theme_classic() \n\np\n\n\n\n\n\n\n\n\nSame plot as before.\n\n\n\n\n\n\nNote\n\n\n\nNote that in this exercise we maintain black and white (and grey) color with dashed and solid lines that is a format normally used for many publications.\n\n\n\nAs an illustration, let‚Äôs expand the example to include also error bars in the plot with the Standard Error of each point.\nAs a reminder, the standard error can be calculated as\n\\(SE = \\frac{\\sigma_{\\bar{X}}}{\\sqrt{n}}\\)\nFirst, let‚Äôs modify the summary variable to also calculate the SE in the variables se_rt and se_rs\n\nbasquenpi_data_summary_new &lt;- dfBasqueNPI %&gt;% filter(EXPT == \"basquenpi\") %&gt;%   #filter the data\n                            group_by(EmbeddedSubject,AgreementMorphology, RegionNumber) %&gt;%       \n                            summarize( mean_rt = mean(RWRT), \n                                       sd_rt=sd(RWRT), \n                                       se_rt = sd_rt/sqrt(n()), \n                                       mean_rs = mean(RSRT), \n                                       sd_rs= sd(RSRT), \n                                       se_rs = sd_rs/sqrt(n()))\n\n`summarise()` has grouped output by 'EmbeddedSubject', 'AgreementMorphology'.\nYou can override using the `.groups` argument.\n\n\nIn our plot, we want to display for every category an error bar. This can be done with the ggplot function geom_errorbar(). That function has two main arguments to specify the range of the error bar: ymin and ymax . The values of the error bars are \\(\\overline{X}\\pm SE\\)\n\np &lt;- basquenpi_data_summary_new %&gt;% filter(EmbeddedSubject==\"NP\") %&gt;%\n          ggplot(aes(x = RegionNumber, y = mean_rt)) +\n          geom_line(aes(linetype=AgreementMorphology)) + \n          geom_point(aes(shape=AgreementMorphology)) +\n          scale_x_continuous(breaks = seq(1,10,1)) +\n          scale_color_grey() + \n          theme_classic() +\n          geom_errorbar(aes(ymin = mean_rt-se_rt, ymax = mean_rt+se_rt) , width= 0.2,color = 'grey')\n\np\n\n\n\n\n\n\n\n\nThis is a more adequate plot to assess the data, as it shows that in some regions, the difference is within the error (e.g.¬†Region 6) and might not be significant. We will look in future workgroups at how to confirm this statistically with a linear model.\n\nEnd of the assignment #2",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup3.html",
    "href": "Workgroup3.html",
    "title": "Workgroup 3: Simple Linear Regression with R",
    "section": "",
    "text": "In this session we will start exploring the analysis of correlations between variables and applying the linear model concepts explained in the lecture with R.\nWe start by looking in a bit more detail into the visual exploration of relationships using scatter plots.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate and format scatter plots with ggplot.\nVisualize linear model fits with data.\nUnderstand concept, calculation and significance checking of Pearson‚Äôs Correlation Coefficient in R.\nUnderstand basic linear models formula nomenclature.\nFit linear model with a continuous or interval predictor using lm()\nFamiliarize with the output of the lm() function.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R"
    ]
  },
  {
    "objectID": "plot_xy.html",
    "href": "plot_xy.html",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Scatter plot with two variables\nIn Workgroup #2, we introduced plotting in R with ggplot2 package and practiced generating a plot in assignment #2.\nIn this section, we will expand with a few additional concepts useful to explore visually the relationship between two variables.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#scatter-plot-with-two-variables",
    "href": "plot_xy.html#scatter-plot-with-two-variables",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Basic scatter plot\nA¬†scatter plot¬†or XY plot displays the values of two variables along two axes, showing the relationship between them and possible correlations.\nScatter plots can be generated in ggplot using the geom_point() function.\nLet‚Äôs look again at the lexdec dataset in the languageR package as used Workgroup #2 and explore the relationship between the lexical decision latency reaction time (RT) and the word Frequency .\n\nlibrary(languageR)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\n\nA few things can be noticed on the plot above:\n\nThere is not a continuous range of values of Frequency in the dataset, as it can be seen by the gaps between data. This is common in our field of research, where we have limited examples of continuous predictors, and those available like the word Frequency have several data points measured on the same value.\nSeveral points are ‚Äòoverplotted‚Äô on the same area, thus not providing a clear view of around which values there are more data points.\n\n\n\nOverplotting\nThere are several ways to address overplotting and improve the visualization of the data.\n\nWe can change the shape of the data points from a solid circle to a hollow circle. The shape is controlled with the shape argument. The value for an empty circle is shape = 1 . Correspondence between values and shapes are in the reference manual of the function, although it is not immediate to find them. I leave here an image for reference:\n\n\n\n\nFrom: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(shape = 1)\n\n\n\n\n\n\n\n\nNow you can see there are areas with bigger overlap of points (more measurements) in the center band of the plot.\nIn case of a large dataset, it is better to use the alpha (transparency) argument. This can be specified as a ratio. For example, a value alpha = 1/5 is to be interpreted as ‚Äú5 points to be overplotted to get a full solid color‚Äù. That means that points are slightly transparent and overlapping them increases the shade.\nCompare the plots below with two different values\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/5)\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/10)\n\n\n\n\n\n\n\n\n\n\nDiscrete variables\nThe most common case in linguistics research is the use of categorical predictors with a limited set of levels. A scatter plot with categorical data would look like the plot below, where we display the same Reaction Time data as a function of the word class.\n\nggplot(lexdec,  aes(x = Class, y = RT)) + geom_point(alpha=1/50)\n\n\n\n\n\n\n\n\nIn those cases with very limited number of categories, a boxplot or violin plot will be a better option to display the relationship.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#mapping-additional-variables-to-features",
    "href": "plot_xy.html#mapping-additional-variables-to-features",
    "title": "Plotting relationship between two variables",
    "section": "Mapping additional variables to features",
    "text": "Mapping additional variables to features\n\nA scatter plot shows the relationship between two variables in X and Y, but it can be used to reflect the relationship with more variables by mapping them to specific aesthetic features.\nFour commonly used:\n\nShape\nColor\nSize\nTransparency\n\nLet‚Äôs add in the plots above on the RT relationship with Frequency, a mapping to the word semantic category in the variable Class.\n\nggplot(lexdec,  aes(x = Frequency, y = RT,shape = Class)) + geom_point()\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class)) + geom_point()\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,size = Class)) + geom_point(alpha = 1/10)\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,alpha = Class)) + geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nWe can also add two mapping to the same variable. For example, if we want both different colors and different shape of the data points we can do:\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, shape=Class)) + geom_point()\n\n\n\n\n\n\n\n\nWe can also add another variable, for example NativeLanguage encoding the native on non-native nature of the speakers. Readability of the graph may be difficult though .\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, alpha=NativeLanguage)) + geom_point() \n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\nAlways try different visualizations to find what is the best way to convey the message on your dataset.\nGood resources for inspiration and recommendations on data visualization types:\nR Graph Gallery: https://r-graph-gallery.com/\nFrom Data to Viz: https://www.data-to-viz.com/",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#plotting-linear-models",
    "href": "plot_xy.html#plotting-linear-models",
    "title": "Plotting relationship between two variables",
    "section": "Plotting linear models",
    "text": "Plotting linear models\nWe will look in next workgroups at different ways to represent the results of linear modeling, but we introduce here a useful feature of ggplot to plot fits to the data as another layer.\nThe function geom_smooth() adds a layer to a plot with aid the visualization of trends and relationships. It has two main arguments:\n\nmethod : function to use to calculate the ‚Äòsmoothed‚Äô version of the data. The value that is relevant to us is method = 'lm' , that would calculate a linear model fit and overplot it.\nformula: Formula of the linear fit model. The default value is formula = y ~ x . We will come back to this point in later lectures. For now we can use the default with no need to specify it.\n\n\nggplot(lexdec, aes(x = Frequency, y = RT)) + \n                geom_point(alpha = 1/5) +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat we can see now in the plot is a linear fit to the data, showing a negative correlation between Frequency and RT, with higher frequency words (more common) showing faster reaction times.\nAn interesting characteristic of this feature is that it applies the fit to the data as defined in ggplot aes(), so if we mapped another variable, like NativeLanguage to an aesthetic, we will get two fits, one per group:\n\nggplot(lexdec, aes(x = Frequency, y = RT, color = NativeLanguage))+\n                geom_point() +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot above shows :\n\nIn general there is a trend that words are comprehended/processed faster the most frequent they are.\nThe strength of this effect is more pronounced in Non-native English speakers.\n\n\nNow that we are able to observe the relationship between two variables let‚Äôs move to quantifying it and assessing their statistical significance.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Pearson‚Äôs correlation coefficient\nCorrelation is a measure of the dependence between two variables, and to which degree they are linearly related. It is not an indication of causality just of relationship.\nThe Pearson‚Äôs correlation coefficient, \\(r\\), is used to quantify the direction and magnitude of the association between two variables X, Y. It is calculated based in the sum of cross products of their values, normalized by their standard deviation:\n\\[\nr= \\frac{\\sum_{i=1}^N (x_i-\\bar{X})(y_i-\\bar{Y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\bar{X})^2}\\sqrt{\\sum_{i=1}^N (y_i-\\bar{Y})^2}}\n\\]\nThe coefficient takes values between -1 and 1:\nThe image below shows examples of correlation values and scatter plot of two variables in X and Y.\nIn R, we can calculate the Pearson correlation coefficient using the cor() function.\nContinuing with the previous example:\nlibrary(languageR)\ncor(lexdec$Frequency, lexdec$RT, method = \"pearson\")\n\n[1] -0.2263358\nThe result, \\(r=-0.23\\), is negative, reflecting an inverse relationship (higher Frequency results in lower RT) and the level of correlation is moderate.\nA question however is how do we know that the calculated value is significant? A NHST test is performed following the approach defined in the lecture:\nIn the case our example, the number of data points, n, is \\(n=1659\\)\nnrow(lexdec)\n\n[1] 1659\nThe t-statistics will be :\n\\[ t= \\frac{r\\times \\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{-0.23 \\times \\sqrt{1659-2}}{\\sqrt{1-(-0.23)^2}} = -9.46\n\\]\nIf we select a confidence level of 5%, \\(\\alpha = 0.05\\), we can extract the value of \\(t_{critical}\\) considering the \\(df = 1659-2=1657\\). This can be calculated in R using the function qt() that returns a value of the Student t distribution. Note that since we are running a two-tailed test, we use 0.025 for p (half of 0.05).\nqt(p = 0.025, df = 1657)\n\n[1] -1.961397\nThe t-value is smaller than the critical value, so we reject the null hypothesis: there is a correlation between our variables.\nThe complete process above, can be performed in R using the function cor.test()\ncor.test(lexdec$Frequency,lexdec$RT,method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  lexdec$Frequency and lexdec$RT\nt = -9.4587, df = 1657, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2715046 -0.1801720\nsample estimates:\n       cor \n-0.2263358\nIf we look at the results provided by the function, the calculated t-value, df and correlation coefficient are the same. In addition, the function returns a p-value (\\(p&lt;0.001\\)) and a confidence interval for the correlation coefficient (\\(95CI_r = [-0.27,-0.18]\\) )",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "correlation.html#pearsons-correlation-coefficient",
    "href": "correlation.html#pearsons-correlation-coefficient",
    "title": "Correlation",
    "section": "",
    "text": "r=‚àí1 a perfect negative relationship: when one variable increases the other one decreases.\nr = 1, a perfect positive relationship.\nr=0, indicates no relationship at all between the two variables\n\n\n\n\n\nPearson correlation coefficient\n\n\n\n\n\n\n\n\n\nStep 1: Define statistical hypotheses\nOur null hypothesis is that the two variables are uncorrelated, so \\(r=0\\)\n\\[\nH_0: r=0 \\\\\nH_a: r\\neq 0\n\\]\nStep 2: Define sampling distribution\nFor the Pearson coefficient, \\(r\\), the sampling distribution is normal with a mean \\(\\mu_{r}=0\\) if \\(H_0\\) is true\nStep 3: Identify the test statistic\nThe test statistic is following a t-distribution with \\(df = n-2\\) and value\n\\[\nt=\\frac{r\\times\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\nNote: details beyond the scope of the course.\n\n\n\n\n\n\nStep 4: Determine critical value\n\n\n\n\nStep 5: reach statistical conclusion:\n\\[\nt_{value} = -9.62 \\lt t_{critical} = -1.96\n\\]",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "lm_intro.html",
    "href": "lm_intro.html",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "Formula notation in R\nAs explained in the lecture, a linear model of a dataset fits a line that minimizes the sum square errors between the predicted values and the data.\nThe generic function in R used to calculate a linear model fit based on data is lm() with the main arguments formula and data.\nA linear model with one predictor variable can be in general expressed as:\n\\[\nY_i = b_0 + b_1X_i+ \\epsilon_i\n\\]\nwhere \\(b_0\\) and \\(b_1\\) are referred to as the coefficients of the model and \\(\\epsilon_i\\) is the error term of the fit.\nIn our running example:\nWe would like to build a model as :\n\\[\nRT_i = b_0+b_1Frequency_i+\\epsilon_i\n\\]\nModels in R are specified using a notation in the form response~terms where response is the (numeric) dependent variable and terms are the linear predictors. A few notes:\nSo both formulas below are equivalent and can represent our model :\nRT ~ Frequency\nRT ~ 1 + Frequency\nThe formula can be extended to add more predictors (workgroup #4):\nRT ~ 1 + Frequency + NativeLanguage\nor to include interactions between predictors, using the operator : (workgroup #5):\nRT ~ 1 + Frequency:Length + NativeLanguage\nFor now, we will focus on a single predictor.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "lm_intro.html#formula-notation-in-r",
    "href": "lm_intro.html#formula-notation-in-r",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "\\(b_0\\): intercept - represents the value of the outcome variable Y, when X=0.\n\\(b_1\\): slope - represents the change on the value of Y, due to a change of 1 unit of the value of X.\n\n\n\n\n\n\n\nNote\n\n\n\nNomenclature: Sometimes you will see a linear model with coefficients indicated with a ‚Äúhat‚Äù notation (\\(\\hat{}\\)). This is to reflect that the model we build is an estimation of the real model. So \\(b_0, b_1\\) would represent the population parameters while \\(\\hat{b}_0, \\hat{b}_1\\) would be the estimated parameters based on our sample.\nThis is equivalent to the difference between the population and sample means (\\(\\mu\\) and \\(\\bar{X}\\)) and standard deviation (\\(\\sigma\\) and \\(s\\)).\nIn the course for simplicity I will use the non-hat notation.\n\n\n\n\n\n\n\n\nFormulas have an implicit intercept, so it is not necessary to include it.\nError is considered by the function and calculated and it is also not included in the definition",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "lm_intro.html#interpreting-lm-output",
    "href": "lm_intro.html#interpreting-lm-output",
    "title": "Linear Models in R with lm()",
    "section": "Interpreting lm() output",
    "text": "Interpreting lm() output\n\nThe function lm() has a simple nomenclature and usage. The data it returns is nevertheless extensive and we will focus on the interpretation of the output.\nLet‚Äôs look at out example:\n\nlm(RT~1+Frequency, data=lexdec)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nCoefficients:\n(Intercept)    Frequency  \n    6.58878     -0.04287  \n\n\nRunning the lm() function returns a simple set of data, with the formula reflecting again and the value of the coefficients. This result means that our model would be written as\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\nThe formula however calculates more elements. Instead, it is normally executed saving the output in a new variable.\n\nmodel1 &lt;- lm(RT~1+Frequency, data = lexdec)\n\nnames(model1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nAs we can see, the function returns a dataframe with many elements and variables. We will go through them at different points in the course.\nA simple way to display the results is to call the summary() function on the dataframe returned by lm() :\n\nsummary(model1)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55407 -0.16153 -0.03494  0.11699  1.08768 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.588778   0.022296 295.515   &lt;2e-16 ***\nFrequency   -0.042872   0.004533  -9.459   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2353 on 1657 degrees of freedom\nMultiple R-squared:  0.05123,   Adjusted R-squared:  0.05066 \nF-statistic: 89.47 on 1 and 1657 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs look at each of the output values individually:\n\nModel residuals\n\nThe first output of the function recalls the formula used for the fit and provides the 5-point summary of the model residuals.\n\n\n\n\n\nWe will see in the next lecture that one of the conditions for the validity of a linear model is the normality (normal distribution) of the model residuals, with a mean around 0. The 5-point summary provides an indication, nevertheless a check should be performed on the data. The residuals are included in the data structure as output of the model in a field called residuals . We can assess the normality of the residuals by for example using a Q-Q Plot as we described in workgroup #2.\n\nqqnorm(model1$residuals)\nqqline(model1$residuals)\n\n\n\n\n\n\n\n\nThe model residuals are still significantly deviating from normality. In Workgroup #4 we will explore in details the model assessment.\n\n\nModel coefficients\n\nThe second part of the table, provides the coefficients :\n\n\n\n\n\nThe table provides the two coefficients (\\(b_0\\) : intercept) and (\\(b_1\\): slope), and for each of them the following columns with the values and the results of a statistic test to check their significance.\n\nEstimate: Coefficient value.\nStd. Error: standard error of the coefficient estimate (\\(\\sigma_{b_i}\\))\nt-value: t-statistic calculated based on the value, to test the significance of the coefficient\n\\[\nt=\\frac{b_i}{\\sigma_{b_i}}\n\\]\n\n\\[\nH_0:b_i=0\\\\\nH_a:b_i \\ne 0\n\\]\n\n\\(Pr(&gt;|t|)\\) or p-value: probability of the t-value in case \\(H_0\\) is true.\n\nFrom the above, in our example we can see that both coefficients are significantly different from 0, so the model is confirmed to be:\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nModel validity and Intercept interpretation:\nA model is always valid around the values used to generate them, and it is important to be careful to interpret the outcome.\nThe intercept value of 6.59 means that a word with a lemma log Frequency value of 0, is expected to have a logarithmic reaction time of 6.59 (in lexdec dataset both RT and Frequency are logarithmically transformed values) This might not make sense, as at words with extremely low frequency, the reaction time might be even longer.\nConsider the range of the data used to fit the model:\n\nmin(lexdec$Frequency)\n\n[1] 1.791759\n\nmax(lexdec$Frequency)\n\n[1] 7.77191\n\n\nWe should not use the model to predict values beyond this range.\n\n\n\n\nData fit quality\n\nThe statistical analysis shown before provides the significance assessment at individual coefficient level.\nThe last section of the output on the other hand, provides the statistical assessment of the model as a whole\n\n\n\n\n\nWe will describe these values in the next Lecture and Workgroup.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "assignment3_key.html",
    "href": "assignment3_key.html",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "",
    "text": "Task #1: Load required libraries\nFor this assignment we use tidyverse\n#load libraries\nlibrary(tidyverse)",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-2-open-dataset",
    "href": "assignment3_key.html#task-2-open-dataset",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #2: Open dataset",
    "text": "Task #2: Open dataset\nFor this exercise we used a simulated dataset from a hypothetical experiment measuring the duration of vowels during production, generated following an example from Winter (2019).\nThe dataset is in the file ./data/extended_vowel_duration.csv and contains data from 30 participants, each recorded pronouncing 20 different words.\n\n#Open file\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nThe data contains the following columns:\n\nSubjectID: ID of the participant from 1 to 30\nWord: ID of the word from 1 to 20\nlogFreqs: logarithmic word frequency\nDuration: vowel utterance duration measurement in milliseconds.\n\nThe SubjectID and Word columns are numerical, but should be treated as factors. It would not affect this exercise, but it is good practice to define categorical factors when reading the data to avoid changing them inadvertently.\nConvert SubjectID and Word to factors\n\ndfVowel$SubjectID &lt;- as.factor(dfVowel$SubjectID)\ndfVowel$Word &lt;- as.factor(dfVowel$Word)",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-3-plot-dataset",
    "href": "assignment3_key.html#task-3-plot-dataset",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #3: Plot dataset",
    "text": "Task #3: Plot dataset\nYou were asked to create a scatter plot of the measured vowel duration as a function of word frequency, including a linear fit. As per the example in the workgroup, we use ggplot\n\ndfVowel %&gt;% ggplot(aes(x=logFreqs, y=Duration)) + \n       geom_point(alpha = 1/4) +\n       geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nObservations on the plot:\n\nThere is an apparent linear relationship between the duration of vowels in produced words and the frequency of the word, expressed in logarithmic scale.\nMore frequent words, have a shorter duration, so we would expect a negative Pearson‚Äôs correlation coefficient value.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-4-calculate-correlation-coefficient",
    "href": "assignment3_key.html#task-4-calculate-correlation-coefficient",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #4: Calculate correlation coefficient",
    "text": "Task #4: Calculate correlation coefficient\nCalculate Pearson‚Äôs correlation coefficient between logFreqs and Duration\n\nr = cor(dfVowel$logFreqs, dfVowel$Duration)\nprint(r)\n\n[1] -0.6745614\n\n\n\nAs expected the negative Pearson‚Äôs coefficent value \\(r = -0.67\\) indicates a relatively high inverse correlation between the word frequency and the produced vowel duration.\n\n\nTask #4.1 (EXTRA CREDIT): Assess significance of the Correlation Coefficient\nTo confirm the significance of the calculated correlation coefficient \\(r\\), we use the NHST methodology following the procedure practiced in Workgroup 3 and recalled here:\n\n\nManual calculation\n\nStep 1: Define statistical hypotheses\nOur null hypothesis is that the two variables are uncorrelated, so \\(r=0\\)\n\\[\nH_0: r=0 \\\\\nH_a: r\\neq 0\n\\]\nStep 2: Define sampling distribution\nFor the Pearson coefficient, \\(r\\), the sampling distribution is normal with a mean \\(\\mu_{r}=0\\) if \\(H_0\\) is true\nStep 3: Identify the test statistic\nThe test statistic is following a t-distribution with \\(df = n-2\\) and value\n\\[\nt=\\frac{r\\times\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\n\nIn the exercise, the number of data points, n, is \\(n=600\\)\n\nn = nrow(dfVowel)\n\nThe t-statistics will be :\n\n t_stat = (r * sqrt(n-2)) / sqrt(1-r^2)\n t_stat\n\n[1] -22.34534\n\n\n\\[ t= \\frac{r\\times \\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{-0.67 \\times \\sqrt{600-2}}{\\sqrt{1-(-0.67)^2}} = -22.35\n\\]\n\nStep 4: Determine critical value\n\nIf we select a confidence level of 5%, \\(\\alpha = 0.05\\), we can extract the value of \\(t_{critical}\\) considering the \\(df = 600-2=598\\). This can be calculated in R using the function qt() that returns a value of the Student t distribution.\n\n\n\n\n\n\nImportant\n\n\n\nNote that we use a two-tailed test, so for a confidence level \\(\\alpha = 0.05\\) we use 0.025 for p (half of 0.05). The reason to use a two-tailed test is that our hypothesis is that \\(r \\ne 0\\), not whether it is larger or smaller than a certain value.\n\n\n\nqt(p = 0.025, df = 598)\n\n[1] -1.963939\n\n\n\nStep 5: reach statistical conclusion:\n\\[\nt_{value} = -22.35 \\lt t_{critical} = -1.96\n\\]\n\n\n\n\n\n\nNote\n\n\n\nPlease note how the \\(t_{critical}\\) value is virtually the same as for a normal distribution for an \\(\\alpha\\) level of 0.05, since the number of samples is high (&gt;&gt; 50).\n\n\n\nWe can conclude that the correlation coefficient is significantly different from 0. In other words, there is a statistically significant relationship between the variables.\n\n\nUsing cor.test()\nThe same test of significance can be done using the function available in R:\n\ncor.test(dfVowel$logFreqs,dfVowel$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  dfVowel$logFreqs and dfVowel$Duration\nt = -22.345, df = 598, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7159482 -0.6284500\nsample estimates:\n       cor \n-0.6745614 \n\n\nThe function provides the same results as in the manual calculation, with the addition of providing a 95% Confidence Interval for the coefficient.\n\n\n\n\n\n\nReporting correlation\n\n\n\nThe results could be reported following APA style as:\nThe produced vowel duration showed a negative relationship with the log transformed word frequency (\\(r(598) = -.67, p&lt;.01\\))",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-5-build-a-linear-model-based-for-vowel-duration",
    "href": "assignment3_key.html#task-5-build-a-linear-model-based-for-vowel-duration",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #5: Build a linear model based for vowel duration",
    "text": "Task #5: Build a linear model based for vowel duration\nTo generate a linear model for the vowel duration as a function of frequency we use the lm() function.\n\nmodel &lt;- lm(Duration~logFreqs, data = dfVowel)\nsummary(model)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the above produces the same output as if using the model with an explicit intercept:\n\nmodel &lt;- lm(Duration~ 1 + logFreqs, data = dfVowel)\nsummary(model)\n\n\nCall:\nlm(formula = Duration ~ 1 + logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nThe fitted model can be expressed as :\n\\[\nDuration_i = 307.51 -5.64 \\times logFreqs_i\n\\]\nwhere the intercept coefficient is \\(b_0 = 307.51\\) and the slope is \\(b_1 = -5.64\\).\nThe output includes the t statistic and p-value of each of the coefficients, that indicate that they are different from 0.\nIn the next assignment we will explore the fit qualify measures and the check of the model validity.\n\nEnd of assignment 3\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup4.html",
    "href": "Workgroup4.html",
    "title": "Workgroup 4: Simple Linear Regression II",
    "section": "",
    "text": "In this session we will continue to look at the interpretation of simple linear regression models outputs, including the quality of the data fit and the checking of model assumptions.\nWe also look at examples of models with a binary categorical predictor and the interpretation of its coefficients.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nUnderstand the quality of a model fit.\nCheck linear model assumptions.\nFit and interpret linear model with categorical predictors using lm()\nReport simple linear model results.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II"
    ]
  },
  {
    "objectID": "model_fit.html",
    "href": "model_fit.html",
    "title": "Linear Model output",
    "section": "",
    "text": "Model fit quality\nWe start from were we left in workgroup #3, looking at the last section of the output generated by the lm() function, that provides the statistical assessment of the model as a whole\nLet‚Äôs look at the different elements of the output\nWe know now how to create linear models fitting the data of interest, but how do we quantify how good is that fit, and if it adequately represents the data?",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_fit.html#sec-modelfit",
    "href": "model_fit.html#sec-modelfit",
    "title": "Linear Model output",
    "section": "",
    "text": "\\(R^2\\) coefficient\n\nAs seen in the lecture, the \\(R^2\\) value of a model represents the level of data variance explained by the model and can be expressed as:\n\\[R^2 = \\frac{SS_M}{SS_T}=\\frac{SS_T-SS_R}{SS_T} = 1-\\frac{SS_R}{SS_T}\\] where:\n\n\\(SS_T\\): Total sum of squares (deviation of data points from the data mean)\n\\(SS_M\\): Model sum of squares (deviation of regression line from the data mean)\n\\(SS_R\\): Residual sum of squares (deviation of data points from regression line)\n\nAs an exercise let‚Äôs calculate manually the \\(R^2\\) value of the linear model created in Assignment #3. We load the data again and fit the model for the vowel Duration as a function of logFreqs\n\nlibrary(tidyverse)\n\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nmod &lt;- lm(Duration~1+logFreqs, data=dfVowel)\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ 1 + logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nTo calculate \\(SS_T\\), we just add the squared difference between all of Duration points from their mean value:\n\nSS_t &lt;- sum( (dfVowel$Duration - mean(dfVowel$Duration))^2 )\nSS_t\n\n[1] 1994385\n\n\nFor \\(SS_R\\), the residuals from the model are available in the object returned by the lm() function:\n\nSS_r &lt;- sum (mod$residuals^2)\nSS_r\n\n[1] 1086874\n\n\nThe resulting value of R2 is then:\n\nR2 &lt;- (SS_t - SS_r)/SS_t\nR2\n\n[1] 0.455033\n\n\nThis is exactly the value returned in the summary(mod) output below and labelled Multiple R squared\nThe value indicates that the model explains 45.5% of the data variance.\n\n\n\n\n\n\nRelationship with Pearson‚Äôs correlation coefficient (r)\n\n\n\nNote that in the case of a single continuous variable, the \\(R^2\\) value corresponds to the square of the correlation coefficient r:\n\nr&lt;-cor(dfVowel$Duration,dfVowel$logFreqs)\nr^2\n\n[1] 0.455033\n\n\n\n\n\nAdjusted \\(R^2\\)\nIn the output of the model summary function there is another value labelled Adjusted R-squared\n\n\n\n\n\nThe \\(R^2_{Adjusted}\\) value is a correction applied to the case where more than one predictor variable is included in the model as we will see in the next lectures. It prevents the \\(R^2\\) value to increase with additional predictors while not improving the model fit. The adjustment is based on the number of data points (N) and the number of predictors (K):\n\\[\nR^2_{Adjusted} = 1-\\left( \\frac{SS_R}{SS_T}\\times \\frac{N-1}{N-K-1}\\right)\n\\]\nFor the cases we have seen so far with \\(K=1\\), the values are not too different.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_fit.html#model-f-statistic",
    "href": "model_fit.html#model-f-statistic",
    "title": "Linear Model output",
    "section": "Model F-statistic",
    "text": "Model F-statistic\nThe F-statistic displayed at the end of the output is from the so-called F-test for regression.\n\n\n\n\n\nIn essence, the test follows a NHST to identify if any regression slope coefficient (other than the intercept) is different from 0.\nFollowing our general NHST process:\n\n\n\n\n\n\nStep 1: Define statistical hypotheses\nThe null hypothesis here is that for a model, no slope coefficient is different from 0\n\\[\nH_0: b_1=b_2=b_3=...=0 \\\\\\\\\nH_a: b_k \\neq 0, \\text{for at least one value}\n\\]\nIn the case of single linear regression as we are looking at by now, this simplifies to\n\n\\[\nH_0: b_1=0 \\\\\\\\\nH_a: b_1\\neq 0\n\\]\n\nStep 2: Define sampling distribution: in an F-test, if \\(H_0\\) is true the ratio of explained and unexplained variance follow an F-distribution.\nStep 3: F-test statistic can be defined as the ratio of the mean squared errors\n\n\\[\nF = \\frac{MS_{mod}}{MS_{res}}\n\\]\nThe mean standard error are calculated from the Sum-squared errors and the degrees of freedom. The degrees of freedom of a model is the number of predictors (K), while for the residuals, the degrees of freedom depends on the number of data points (N-K-1).\n\\[\nMS_{mod} = \\frac{SS_{mod}}{df_{mod}} = \\frac{SS_{mod}}{K} \\\\\\\\ MS_{res} = \\frac{SS_{res}}{df_{res}}=\\frac{SS_{res}}{N-K-1}\n\\]\nIn our example, \\(N=600, K=1\\), and \\(SS_{mod}\\) we can calculate:\n\nSS_mod &lt;- SS_t-SS_r\nMS_mod &lt;- SS_mod / 1\nMS_res &lt;- SS_r / (600 - 1 -1)\n\nF_val &lt;- MS_mod / MS_res\nF_val\n\n[1] 499.3142\n\n\nAs you can see, the value is the same as in the output from model fit in the figure above.\n\nStep 4: determine the critical value. Here the F-distribution in R can be calculated with the function qf() (similar to the function qt() that we used to determine the critical t-value in the Pearson‚Äôs coefficient significant testing). Three arguments are required: the confidence level (0.05), and the two degrees of freedom (\n\nF_critical&lt;-qf(1-0.05,1,600-1-1)\nF_critical\n\n[1] 3.857056\n\n\nStep 5: reach statistical conclusion\n\nThe calculated F-value is much higher than the threshold above, therefore, the null hypothesis is rejected and the model is better than the model with only an intercept.\n\n\n\n\n\n\nImportant\n\n\n\nNote that the example above was intended for you to know where the numbers come from in the R output. You will not have to make the calculations step by step, as the lm() function already provides it in the output.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_comparison.html",
    "href": "model_comparison.html",
    "title": "Model selection",
    "section": "",
    "text": "Model comparison based on F-test\nUp to now we have created a model fitting data with one predictor, trying to assess whether it explains the data ‚Äúbetter‚Äù than a simpler model simply based on the mean of the data.\nWe can write the two models as:\nWe can compare which models best fits the data by comparing the amount of residual variance explained by one or the other, as measured by the \\(R^2\\) parameter. However, as explained in the previous section, the \\(R^2\\) value tends to improve as we add more parameters to the model, while their explanation value is not improved.\nWe will look in the following sections to three other methods: i) model comparison based on F-test, ii) based on Akaike‚Äôs Information Criterion (AIC) and iii) based on Bayesian Information Criterion (BIC).\nOne approach to check if adding elements to a model improve the model fit is to make a F-test of the difference in residual Sum of Squares.\nThis can be done using the anova() function.\nAs example, let‚Äôs again look at the data from Assignment 3 and create two models, one with only the intercept (null model), and one with our predictor (logFreqs)\nlibrary(tidyverse)\n\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nnull_model &lt;- lm(Duration~1,data=dfVowel)\nmod &lt;- lm(Duration~logFreqs, data = dfVowel)\nWe can compare if there is a significant difference between the two models residual sum of squares.\nanova(null_model, mod)\n\nAnalysis of Variance Table\n\nModel 1: Duration ~ 1\nModel 2: Duration ~ logFreqs\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    599 1994385                                  \n2    598 1086874  1    907511 499.31 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe output of the anova() function call, provides a F-test comparing the two models provided as input.\nWhat we can observed in this case is that the F test value is \\(F(1,598) = 499.31, p&lt;.01\\) indicating that there is a significant difference in the unexplained variance between both models. In other words, introducing the logFreqs predictor improved the model fit.\nAlthough this might is a trivial example with a single predictor, as we will see in the next lessons, model comparison will be extensively used when evaluating the explanatory value of more than one variable in Multiple Regression models.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#model-comparison-based-on-f-test",
    "href": "model_comparison.html#model-comparison-based-on-f-test",
    "title": "Model selection",
    "section": "",
    "text": "Note\n\n\n\nNote that in this case, with a single predictor, the value of this test is the same as the F-value provided by lm() when fitting a model to data and explained in the previous section:\n\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#akaike-information-criterion-aic",
    "href": "model_comparison.html#akaike-information-criterion-aic",
    "title": "Model selection",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\n\nThe model comparison based on F-test using anova() allows us to evaluate if adding a predictor term changes the model fit.\nA mathematical indicator that can be used to evaluate a model performance is the Akaike Information Criterion (AIC).\nDetails of the AIC mathematical basis are beyond the objectives of this course and are not required for its use in model selection, but the basic ideas are the following:\n\nAIC estimates the prediction error of a model\nA lower AIC indicates a model with less information loss, therefore a better fit to the data.\nAIC balances goodness of fit and simplicity of the model, avoiding overfitting.\n\nWe will come back to overfitting in the following lessons on Multiple Linear Regression.\nTo calculate the AIC value, we use the AIC() function (in capital letters). You can pass as arguments to the function more than one model, which will provide as output a table with the comparison of the values. In our running example:\n\nAIC(null_model,mod)\n\n           df      AIC\nnull_model  2 6572.076\nmod         3 6209.858\n\n\nAs you can see, the model with the logFreqs predictor has a lower AIC (6209) that the null model (6572) confirming it is better explaining the data.\nNote that the absolute value of AIC has no practical significance, but it is used always as a relative comparison between two or more models.\nCriteria: a change in AIC between models &gt;2 is considered significant (corresponds to ~35% probability that the lower AIC model improves the explanatory power of the model).",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#bayesian-information-criterion-bic",
    "href": "model_comparison.html#bayesian-information-criterion-bic",
    "title": "Model selection",
    "section": "Bayesian Information Criterion (BIC)",
    "text": "Bayesian Information Criterion (BIC)\n\nA similar indicator to AIC is the Bayesian Information Criterion (BIC). Again the details are beyond the scope of this course, but a few characteristics:\n\nBIC based on Bayesian framework\nLower BIC indicates a better model fit\nPenalized strongly complex models, so it is more adequate to select the simplest possible model explaining the data.\n\nIt is implemented in R by the BIC() function:\n\nBIC(null_model,mod)\n\n           df      BIC\nnull_model  2 6580.870\nmod         3 6223.049\n\n\nAgain in this case, the model with logFreqs predictor has a lower BIC than the null model.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_assumptions.html",
    "href": "model_assumptions.html",
    "title": "Model assumptions",
    "section": "",
    "text": "Linearity\nIn the last lecture, we discussed that linear regression has three main assumptions:\nLet‚Äôs look at how to check each of them.\nThe principle of linear models and correlation as explained in the course is based on the premises that there is a linear relationship between the predictor and outcome variables.\nTo a large extent this is checked visually. Let‚Äôs look at two examples:\nOn the running example of assignment 3\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\nIf we plot the linear model and the data:\ndfVowel %&gt;% ggplot(aes(x=logFreqs, y=Duration)) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nThe relationship appears to a large extent linear between the two variables.\nLet‚Äôs look in comparison to file ‚ÄúELP_full_length_frequency.csv‚Äù in the /data directory with data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1.\nelp_data &lt;- read.csv(\"./data/ELP_full_length_frequency.csv\")\nelp_data %&gt;% ggplot(aes(x = Log10Freq, y = RT)) + geom_point(alpha=1/20) + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nIn this dataset, particularly at larger values of Log10Freq the data appears to deviate from the best fit straight line, suggesting a deviation from linearity.\nWhen the data is not linear and fitted with a straight line as the one in green in the figure below, one observation that can be made is that the residuals will not be distributed equally around the fitted values, but in some areas will be larger than in others as in the figure in the right below.\nWe can plot the residuals against the fitted value, and if the resulting plot deviates from points centered around a flat line, the relationship between the variables is not linear.\nLet‚Äôs see an example based on the last dataset. We fit a model to the data and plot the residuals vs the fitted values. For this, we will use the function plot(). The function plot() when used on a model can generate 6 different diagnostic plots. We can select which one to plot using the which argument. which=1 plots the residuals against the fitted data.\nelp_model&lt;-lm(RT~Log10Freq, data=elp_data)\nplot(elp_model, which=1)\nAs you can see in the output, the data is curved and deviates from a flat line.\nIn case of the data not meeting the linearity assumption, data analysis requires the application of a data transformation or the use a general linear model. This is beyond the scope of this introductory course.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#normality-of-residuals",
    "href": "model_assumptions.html#normality-of-residuals",
    "title": "Model assumptions",
    "section": "Normality of residuals",
    "text": "Normality of residuals\n\nWe already looked at how to check normality of data sets in Workgroup 2. To check a model assumption, we apply the same approach for the model residuals.\nWe can use histograms, Q-Q plots and the Shapiro-Wilk test. Let‚Äôs look at our running example from assignment 3 with the mod model.\n\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nFirst we can plot a histogram of the residuals. I use first here the base R function hist() as ggplot() implementation requires a few tweaks 1 .\n\nhist(mod$residuals)\n\n\n\n\n\n\n\n\nAs can be seen, the residuals are apparently quite normally distributed.\nLet‚Äôs now use a Q-Q plot:\n\nqqnorm(mod$residuals)\nqqline(mod$residuals)\n\n\n\n\n\n\n\n\nThe model residuals are largely on the diagonal line indicating high degree of normality.\nYou can also use the function plot() on a model with the parameter which=2 to generate a QQ-plot. This version includes identification for the data points that are outliers with respect to the line:\n\nplot(mod,which=2)\n\n\n\n\n\n\n\n\nFinally, we can apply the Shapiro-Wilk test for normality that also confirms the observations.\n\nshapiro.test(mod$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mod$residuals\nW = 0.9986, p-value = 0.9221\n\n\nThe model normality of residuals assumptions are met in this example data.\nIn real world data, this is not always the case though. Looking instead to the ELP data, we see a clear deviation from the residuals‚Äô normality assumption.\n\nlibrary(languageR)\n\nmodel1&lt;-lm(RT~Frequency, data=lexdec)\n\nhist(elp_model$residuals)\n\n\n\n\n\n\n\nplot(elp_model, which = 2)\n\n\n\n\n\n\n\n\nIn this case the residuals distribution is skewed and non-normal as clearly shown by the Q-Q plot.\nIn those cases we will need to apply a data transformation or use a general linear model. This is beyond the scope of this introductory course.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#homogeneity-of-variance-of-residuals-homoscedasticity",
    "href": "model_assumptions.html#homogeneity-of-variance-of-residuals-homoscedasticity",
    "title": "Model assumptions",
    "section": "Homogeneity of variance of residuals (Homoscedasticity)",
    "text": "Homogeneity of variance of residuals (Homoscedasticity)\n\nFinal assumption to check is that the residuals have a homogeneous variance.\nThis can be done using the functions ncvTest() function in the car() package.\nncvTest() performs a Non-constant Variance Score Test. A significant test implies that the variance is not constant and the assumption is therefore not met. If we apply to the model for the data in Assignment 3, we see that the test is not significant (p = .42), implying the assumption is met.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nncvTest(mod)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.6488197, Df = 1, p = 0.42053\n\n\nIn comparison, the same test on the ELP data shows that the assumption is not met.\n\nncvTest(elp_model)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 2461.434, Df = 1, p = &lt; 2.22e-16\n\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#footnotes",
    "href": "model_assumptions.html#footnotes",
    "title": "Model assumptions",
    "section": "",
    "text": "ggplot() requires as data input a dataframe or a variable that can be converted to a dataframe. The model generated by lm() is not suitable to directly pass into ggplot. If you want to plot the histogram of residuals with ggplot() you can use the fortify() function that creates a dataframe based on the model. In the created dataframe, the residuals are in a variable called .resid .\nggplot(data = fortify(mod), aes(x=.resid)) + geom_histogram()\n‚Ü©Ô∏é",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_categorical.html",
    "href": "model_categorical.html",
    "title": "Linear Model with Categorical predictors",
    "section": "",
    "text": "Categorical predictor with two levels\nUp to now we have been looking at model fits with continuous predictors. In a large majority of linguistics research however, we use predictors that are not continuous but rather categorical in nature.\nWe already showed an example in Workgroup 2 of plotting the relationship between a categorical predictor and the outcome variable.\nWhen we fit a model, R applies dummy coding to represent the levels of a categorical factor.\nIn case of a categorical predictor with two levels, R uses a single dummy variable with two values. In the example above:\nIf we fit a linear model, it will have the following expression:\n\\[\nRT_i = b_0 + b_1 \\times NativeLanguage_i\n\\]\nWhen NativeLanguage = 0 (English), the model becomes \\(RT = b_0+b_1\\times0 = b_0\\). This implies that the intercept (\\(b_0\\)) will represent the value for English.\nWhen NativeLanguage =1 (Other), the model becomes \\(RT = b_0+b_1\\times1=b_0+b_1\\). That means that the slope (\\(b_1\\)) represents the change for the Other category with respect to English.\nLet‚Äôs look at how this appears if we fit a model in R:\nm1&lt;-lm(RT~1+NativeLanguage, data = lexdec)\nsummary(m1)\n\n\nCall:\nlm(formula = RT ~ 1 + NativeLanguage, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56877 -0.15289 -0.03231  0.11480  1.11318 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.318309   0.007435  849.78   &lt;2e-16 ***\nNativeLanguageOther 0.155821   0.011358   13.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2289 on 1657 degrees of freedom\nMultiple R-squared:  0.102, Adjusted R-squared:  0.1015 \nF-statistic: 188.2 on 1 and 1657 DF,  p-value: &lt; 2.2e-16\nWhat you see on the output above is that we have the (Intercept) value, as for continuous predictors, but the parameter for the predictor is now called NativeLanguageOther. That is to indicate that the dummy variable created has a value 1 for the ‚ÄúOther‚Äù level.\nSo our model would be:\n\\[RT = 6.32+0.16\\times NativeLanguageOther\\] The fitted values for each of the levels of the variable would be:\n\\[\nRT_{English} = 6.32 \\\\\\\\ RT_{Other}=6.32+0.16 = 6.48\n\\]",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_categorical.html#categorical-predictor-with-two-levels",
    "href": "model_categorical.html#categorical-predictor-with-two-levels",
    "title": "Linear Model with Categorical predictors",
    "section": "",
    "text": "NativeLanguage = 0, represents ‚ÄúEnglish‚Äù\nNativeLanguage = 1, represents ‚ÄúOther‚Äù",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_categorical.html#categorical-predictor-with-more-than-two-levels",
    "href": "model_categorical.html#categorical-predictor-with-more-than-two-levels",
    "title": "Linear Model with Categorical predictors",
    "section": "Categorical predictor with more than two levels",
    "text": "Categorical predictor with more than two levels\n\nWhen the categorical predictor has more than two levels, R creates a number of dummy variables to encode them. The number of variables used in the number of levels - 1.\nLet‚Äôs assume that NativeLanguage in the lexical decision experiment had three levels: English, Dutch, Other. R will use to Dummy Variables: NativeLanguageDutch and NativeLanguageOther .\n\n\n\n\nNativeLanguageDutch\nNativeLanguageOther\n\n\n\n\nEnglish\n0\n0\n\n\nDutch\n1\n0\n\n\nOther\n0\n1\n\n\n\nSo a model would look as follows:\n\\[\nRT = b_0 + b_1\\times NativeLanguageDutch + b_2\\times NativeLanguageOther\n\\]\nWe will treat these models with more than one predictor in the next lessons on Multiple Linear Regression.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_reporting1.html",
    "href": "model_reporting1.html",
    "title": "Simple Linear Model reporting",
    "section": "",
    "text": "A report of the results of linear models should include the following elements:\nLet‚Äôs look at an example based again on the example on assignment 3:\nThe result contains all the information required, with the exception of the Confidence Interval for the logFreqs coefficient.\nThis can be conveniently extracted using the function confint() that takes a model as a parameter.\nThe reporting of the analysis could be written as follows:",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Simple Linear Model reporting"
    ]
  },
  {
    "objectID": "model_reporting1.html#footnotes",
    "href": "model_reporting1.html#footnotes",
    "title": "Simple Linear Model reporting",
    "section": "",
    "text": "We will discussed standardized coefficient values in the next workgroups on multiple regression.‚Ü©Ô∏é",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Simple Linear Model reporting"
    ]
  },
  {
    "objectID": "Workgroup5.html",
    "href": "Workgroup5.html",
    "title": "Workgroup 5: Multiple Regression with R",
    "section": "",
    "text": "Workgroup 5 material to be published in the coming weeks",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R"
    ]
  },
  {
    "objectID": "Workgroup6.html",
    "href": "Workgroup6.html",
    "title": "Workgroup 6: Multiple Regression with R II",
    "section": "",
    "text": "Workgroup 6 material to be published in the coming weeks",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baayen, R. H. 2008. Analyzing Linguistic Data: A Practical\nIntroduction to Statistics Using r. Cambridge University Press.\n\n\nField, Andy P. 2026. Discovering Statistics Using R and\nRStudio. London: SAGE Publications.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \"\nO‚ÄôReilly Media, Inc.\".\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005.\n‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù\nThe Journal of the Acoustical Society of America 118 (4):\n2561‚Äì69.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024.\nR for Data Science. O‚ÄôReilly.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In\nHandbook of Computational Statistics: Concepts and Methods,\n375‚Äì414. Springer.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using\nr. Routledge.",
    "crumbs": [
      "References"
    ]
  }
]