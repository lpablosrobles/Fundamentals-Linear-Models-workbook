[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Linear Models 2025-2026: Workbook",
    "section": "",
    "text": "Welcome\nThis is the Workbook for the Fundamentals of Linear Models Course\nThis Workbook was designed as a companion to the Workgroup lectures and aims to provide you with basics of statistical computing using the R language programming, an explanation of the exercises we will follow during the class as well as the assignments to be performed after the lecture.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Workbook organization",
    "section": "",
    "text": "The workbook is organized in six chapters to cover the Workgroup sessions.\nFor each workgroup, a number of descriptive sections are included explaining the main concepts and including code examples. The idea is that you read through the description and try the code examples and exercises yourself as you go through in the RStudio environment.\nAt the end of each workgroup section there is an introduction and description of an Assignment, to be started during the class and finished at home (if required).\nAnswer keys to the assignments will be included in the relevant Workgroup section after the due date of each of them.",
    "crumbs": [
      "Workbook organization"
    ]
  },
  {
    "objectID": "Workgroup1.html",
    "href": "Workgroup1.html",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "",
    "text": "üß† Learning Objectives\nIn this session we will familiarize with the basics on the R computing language and the RStudio environment.\nBy the end of this lesson, you will be able to:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-r",
    "href": "Workgroup1.html#what-is-r",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üõ†Ô∏è What is R?",
    "text": "üõ†Ô∏è What is R?\nR is a powerful, open-source programming language designed for statistical computing, data analysis and visualization. It is widely used among statisticians, data analysts, and researchers.\nIt was created by statisticians Ross Ihaka and Robert Gentleman in the early 1990s and has since become a standard tool in academia, research, and industry.\n\nKey Features of R\n\nStatistical Analysis: R allows to perform analysis using a wide range of statistical techniques including linear and nonlinear modeling, time-series analysis, classification, clustering, bayesian methods.\nData Visualization: R allows the creation of high-quality plots and graphics using specific packages (we will explain below what packages are) like ggplot2, lattice, and plotly.\nExtensibility: Thousands of community provided packages are available via CRAN (Comprehensive R Archive Network), covering several fields.\nData Handling: R includes robust tools for importing, cleaning, transforming, and manipulating data.\nCommunity Support: A large and active user community contributes to its development and provides extensive documentation and tutorials.\n\n\n\nInstalling R\nTo work with the programming language, the R interpreter needs to be installed. You can download it from the R homepage, which is:\n\nhttp://cran.r-project.org/\n\nThere are versions available for Windows, Mac and Linux (several distributions). Select the current version (version 4.5.1 at the start of this course) and install it in your personal computer if you want to use it at home.\nR installation includes a basic interface environment with a Console to enter commands and write scripts that can be launched using the R.exe or R.app.\n\nThis interface is nonetheless quite limited and it is not normally used for data analysis and script development. You can use this language through lots of different applications and environments (e.g.¬†VSCode, JupyterLabs, etc..) . For this course we will introduce the most commonly use development environment for R: RStudio.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#what-is-rstudio",
    "href": "Workgroup1.html#what-is-rstudio",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üíª What is RStudio?",
    "text": "üíª What is RStudio?\nRStudio is an Integrated Development Environment (IDE) for R, that makes coding in R and management of data analysis projects easier.\nRStudio is a product developed by a company called posit, but that provides a free, open source RStudio Desktop version that can be downloaded here:\n\nhttps://posit.co/download/rstudio-desktop/\n\nAs with R, there are versions available for Windows, Mac and several Linux distributions.\nOnce we launch RStudio, you can distinguish four different areas:\n\nConsole: Where R code is executed, you can type commands and see their output.\nSource: Where you write and save scripts, Notebooks.\nEnvironments: Includes tabs to inspect variables and inspect the command history as well as access tutorials.\nFiles/Plots/Packages/Help/Viewer: Includes tabs for file navigation, plotting, package management, and help.\n\n\n\n\nRStudio Environment (extracted from RStudio User Guide)\n\n\nWe will familiarize with the interface during the exercises in the Workgroup sessions, but you can find a full description and information the following resources provided in the Posit website:\n\nRStudio IDE User Guide",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "href": "Workgroup1.html#using-r-and-rstudio-on-the-web",
    "title": "Workgroup 1: Introduction to R and RStudio",
    "section": "üåêUsing R and RStudio on the web",
    "text": "üåêUsing R and RStudio on the web\nIf you don‚Äôt want to install in your own computer, you can use the Posit Cloud environment. that provides a Cloud Free option. This is the approach we will use in this course, so that you can access your work from anywhere.\nAll assignments will be performed in Posit Cloud where I can follow your progress and assist in case of issues.\n\n\nüìù Exercise1: Connect to Posit Cloud\nIn order to use Posit Cloud you need to register for this course following the link below.\nhttps://posit.cloud/spaces/681791/join?access_code=Da9RyPvqyx7Jymq_aHZpKYwBuJJ-45h5bjS1Y7tq\nYou will be directed to the following page. Select the sign-up option and create your account.\n\n\n\nPosit Cloud Sign-in page\n\n\nOnce you sign-in you should see the course project as in the image below:\n\n\n\n\n\nOpen it and explore the RStudio interface.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Commands and operators\nR is an interpreted language, which means that you can write commands to an interpreter in a console that will execute them and return the results. This is in comparison with compiled languages that require to compile the source code to translate it to a machine understandable code.\nIn the RStudio console, you can see the symbol below. This is the command prompt, indicating that the system us ready to execute an instruction or command.\nR syntax is relatively simple and, although daunting if you have never programmed before when you first encounter it, you will quickly get acquainted with it.\nHere‚Äôs a simple example of how R code looks. In the following sections we explain some basic concepts on the syntax and notation.\nBefore going forward, note that lines of code starting with the symbol # are not interpreted. This is use to introduce comments in your code for readability and documentation. We will come to that later when we talk about scripts and notebooks.\nEvery instruction to enter in the command prompt is called a command.\nA simple command is to perform an arithmetic operations like for example:\n1 + 2\n\n[1] 3\nThe command just calculated the addition of the two numbers. In this example, we used the operator + to do so.\nWe include a list below of the basic operators in R, grouped by category. Do not worry if not all are understandable yet:\n1. Arithmetic Operators\nUsed for basic mathematical operations:\n2. Relational (Comparison) Operators\nUsed to compare values:\n3. Logical Operators\nUsed for logical operations (return TRUE/FALSE):\n4. Assignment Operators\nUsed to assign values to variables:\n5. Miscellaneous Operators",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#commands-and-operators",
    "href": "intro_to_r.html#commands-and-operators",
    "title": "‚úçÔ∏è R Syntax",
    "section": "",
    "text": "Tip\n\n\n\nCopy the code as you read along in this workbook and try it yourself in RStudio to become familiar with using the tool and environment.\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n+\nAddition\n2 + 3\n\n\n-\nSubtraction\n5 - 2\n\n\n*\nMultiplication\n4 * 3\n\n\n/\nDivision\n10 / 2\n\n\n\n^ or **\n\nExponentiation\n\n2^3 or 2**3\n\n\n\n%%\nModulus (remainder)\n10 %% 3\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n==\nEqual to\nx == y\n\n\n!=\nNot equal to\nx != y\n\n\n&gt;\nGreater than\nx &gt; y\n\n\n&lt;\nLess than\nx &lt; y\n\n\n&gt;=\nGreater than or equal to\nx &gt;= y\n\n\n&lt;=\nLess than or equal to\nx &lt;= y\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n&\nElement-wise AND\nx &gt; 1 & x &lt; 5\n\n\n|\nElement-wise OR\nx &lt; 1 | x &gt; 5\n\n\n!\nNOT\n!TRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\nComment\n\n\n\n&lt;-\nPreferred assignment\nx &lt;- 5\nMost commonly used\n\n\n=\nAlternative assignment\nx = 5\nUsed in the assignment of values to function arguments (see function section below)\n\n\n-&gt;\nAssign right to left\n5 -&gt; x\nAlthough syntactically valid in R, not used often\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n%in%\nMembership test\n\n3 %in% c(1, 2, 3) will return TRUE\n\n\n:\nSequence\n\n1:5 returns 1 2 3 4 5",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#variables",
    "href": "intro_to_r.html#variables",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Variables",
    "text": "Variables\nA variable is a name that stores a value or data object. You can think of it as a labeled container that holds information you want to use or manipulate in your program.\nIn R, you assign values to variables mostly using the operator &lt;- .\n\nx &lt;- 5       # Assign 5 to variable x  \ny &lt;- 10      # Assign 10 to variable y  \nz &lt;- x + y   # Add x and y  \nprint(z)     # Print the result \n\n[1] 15\n\n\nVariables naming in R can be anything, but follow a few rules:\n\nMust start with a letter\nCan contain letters, numbers, underscores (_) or periods (.)\nAre case-sensitive (Name and name are different)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#data-types",
    "href": "intro_to_r.html#data-types",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Data types",
    "text": "Data types\nBasic types\nR supports several basic data types. Some of the most common are:\n\nNumeric: x &lt;- 3.14\nInteger: x &lt;- 5L (note the L)\nCharacter: name &lt;- \"Leticia\"\nLogical: flag &lt;- TRUE\nVectors and lists\nVectors are the most basic data structure in R. They are a groups of values built using the combine function,¬†c(). For example,¬†c(1, 2, 3, 4)¬†creates a four element series of positive integer values\n\nnumbers &lt;- c(1, 2, 3, 4)\nnumbers\n\n[1] 1 2 3 4\n\n\nYou can also perform operations on vectors.\n\nnumbers^2\n\n[1]  1  4  9 16\n\n\nDataframes\nstructures can be thought of as sets of data organized in a table format in rows and columns. They can be created using the dataframe() function.\n\ndf &lt;- data.frame(\n  SubjectID = c(\"S1\", \"S2\",\"S3\"),\n  age = c(25, 30, 28)\n)\ndf\n\n  SubjectID age\n1        S1  25\n2        S2  30\n3        S3  28\n\n\nYou can access the individual columns on a dataframe using the $ operator. Try to start typing the code below on the console. You will see that R provides suggested completions, displaying the available columns in the dataframe.\n\ndf$SubjectID\n\n[1] \"S1\" \"S2\" \"S3\"\n\n\nMany of the functions we will use in this course require a dataframe as an input or produce one as output, so it is the data structure you will use the most.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#par-functions",
    "href": "intro_to_r.html#par-functions",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Functions",
    "text": "Functions\nFunctions perform tasks in R. They take in inputs called arguments and return outputs. They are called using parentheses. For example, the function mean() in R calculates the mean of the elements we provide as input.\n\n# We can specify a list of numbers\nmean(c(1,2,3,4))\n\n[1] 2.5\n\n\nOr, more useful, we can provide a variable containing values:\n\n# we defined the variable numbers before\nmean(numbers)\n\n[1] 2.5\n\n\nThe parameters of a function and normally called arguments. You can either manually specify a function‚Äôs arguments or use the function‚Äôs default values. In the examples above mean() and sum() are simple functions with not many arguments, but this is not normally the case.\nTo know the arguments of a function you can use the R help. There are two ways to access the help pages for a function:\n\n\nUse the operator ? followed by the function name in the console, or use the help() function.\nFor example, type the instruction ?mean in the console. The help page for the mean() function will open in the RStudio Help panel as in the image below:\n\n\nYou can of course directly open the Help tab and search for the function of interest.\n\nAs you can see above, mean() actually has two other arguments, trim and na.rm . The arguments have a default value, so if we don‚Äôt explicitly include them in the function call, they will use that value. Let‚Äôs look at an example using na.rm :\nR has a special value called NA , which means ‚ÄúNot Available‚Äù and it is used to represent missing values on the data. In experimental work is often the case that some data point is lost or corrupted and we have incomplete datasets. Let‚Äôs assume you had performed an online experiment that computed the reaction time in miliseconds of 10 participants, and one value was not available as per the vector below:\n\nrt &lt;- c(234.2, 127.5, 256.2, NA, 287.1, 145.6, 358.9, 200.1, 398.3, 178.3)\n\nLet‚Äôs try to calculate the average reaction time of your data using mean():\n\nmean(rt)\n\n[1] NA\n\n\nThe function tries to calculate the average, but when one value is not available (NA) the result is also NA . In the help in the image below we see there is an argument na.rm that we can use to ignore the missing elements in the data:\n\nmean(rt, na.rm = TRUE)\n\n[1] 242.9111\n\n\nNow the function worked and summed all the numbers and divided them by 9, ignoring the missing data, in the calculation of the average.\nWe‚Äôll work with functions a lot throughout this book and you‚Äôll get lots of practice in understanding their behaviors, so don‚Äôt panic.\nFinally an advanced note to make you aware that you can define your own functions. The following code defines a new function called add() that, well, adds two numbers:\n\nadd &lt;- function(a, b) {\n  return(a + b)\n}\n\nOnce you have defined the function, you can use it as any other in R:\n\nadd(3.5, 2.5)\n\n[1] 6",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#factors",
    "href": "intro_to_r.html#factors",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Factors",
    "text": "Factors\nIn¬†R, a¬†factor¬†is a data structure used to represent¬†categorical data. Categorical data consists of variables that have a fixed number of unique values, known as¬†levels. These are typically used for any variable that classifies observations into groups. We will use factors extensively in the analysis of data\nWe convert a vector into a factor by using the factor() function. Let‚Äôs look at one example.\n\nüìù Exercise 2: Creating a dataframe with factors\nWe want to create a dataset of six words, collecting data of the animacy, gender, length and frequency of the word.\nFirst, we create individual variables with vectors including the values\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\") \nNounAnimacy \n\n[1] \"animate\"   \"inanimate\" \"inanimate\" \"animate\"   \"animate\"   \"animate\"  \n\n\n\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\") \nNounGender\n\n[1] \"feminine\"  \"masculine\" \"neuter\"    \"masculine\" \"feminine\"  \"feminine\" \n\n\n\nNounLength&lt;-c(6,7,4,5,8,6) \nNounLength \n\n[1] 6 7 4 5 8 6\n\nNounFrequency&lt;-c(638,799,390,569,567,665) \nNounFrequency\n\n[1] 638 799 390 569 567 665\n\n\nAs you can see from the output above, data in the variables NounAnimacy and NounGender are considered as words, or literal strings. The next step is to indicate they are factors.\n\nNounAnimacy&lt;- factor(NounAnimacy) \nNounAnimacy \n\n[1] animate   inanimate inanimate animate   animate   animate  \nLevels: animate inanimate\n\nNounGender&lt;- factor(NounGender) \nNounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter\n\n\nAs you can see by the output produced, now the variables are considered as factors, and Levels indicate the unique values that the each takes.\nWith the variables above, we can now create a dataframe.\n\nDataexample&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency) \nDataexample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nA few useful functions when working with dataframes:\n\nnrow() : returns the number of rows of a dataframe. Normally corresponding to observations in experimental data files.\nncol(): returns the number of columns of a dataframe.\nhead() : displays the first rows of a dataframe or vector. By default, it displays the first 6 items, but you can specify how many rows to show with the argument n . Useful when displaying on the screen large datasets.\nView(): Opens the dataset in a excel-like data viewer in RStudio. Same can be done clicking the name of the variable in the Environment window in RStudio.\nstr() : can be used to display the structure of the dataframe.\ncolnames(): returns the names of the columns/variables in a dataframe.\n\nLet‚Äôs look at a few examples with the dataframe we just created.\nFirst we can inspect what is the dataframe structure:\n\nstr(Dataexample)\n\n'data.frame':   6 obs. of  4 variables:\n $ NounAnimacy  : Factor w/ 2 levels \"animate\",\"inanimate\": 1 2 2 1 1 1\n $ NounGender   : Factor w/ 3 levels \"feminine\",\"masculine\",..: 1 2 3 2 1 1\n $ NounLength   : num  6 7 4 5 8 6\n $ NounFrequency: num  638 799 390 569 567 665\n\n\nAs you can see, the output tells us that the dataframe is composed of four variables (columns), two with categorical values and two with numerical elements and provides the names of those columns. It tells us also that it contains 6 observations (rows). You can obtain also the information above using:\n\nnrow(Dataexample)\n\n[1] 6\n\nncol(Dataexample)\n\n[1] 4\n\ncolnames(Dataexample)\n\n[1] \"NounAnimacy\"   \"NounGender\"    \"NounLength\"    \"NounFrequency\"\n\n\nIf we want to display the first 4 observations in the dataset, you could use:\n\nhead(Dataexample, n = 4)\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "intro_to_r.html#additional-reading-material",
    "href": "intro_to_r.html#additional-reading-material",
    "title": "‚úçÔ∏è R Syntax",
    "section": "Additional Reading material",
    "text": "Additional Reading material\nBasics on R programming:\n\nChapter 1, Introduction to R, from Bodo Winter‚Äôs book ((Winter 2019)).\nSections 2 to 8 on R basic Tutorial by UMC Utrecht\nA good starters reference for R is the book ‚ÄúR for Data Science‚Äù ((Wickham, √áetinkaya-Rundel, and Grolemund 2024)) ). The book is available online freely at R for Data Science\nExcellent eBook to learn R Basics ((Grolemund 2014)) Hands On Programming with R\n\nDataframes:\n\n\nYouTube Videos by DataCamp:\n\nhttps://www.youtube.com/watch?v=9f2g7RN5N0I\nhttps://youtu.be/Nh6tSD4i4qs?feature=shared\n\n\n\n\n\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "‚úçÔ∏è R Syntax"
    ]
  },
  {
    "objectID": "using_libraries.html",
    "href": "using_libraries.html",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Installing and using a Package\nOne of the big benefits of R in comparison with other statistics packages is its open nature. The functionality is easily extended by groups all around the world by developing libraries that can be easily installed and used.\nR packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet.\nA package is a collection of R functions, data, and compiled code bundled together. Packages are created by the R community and cover a wide range of topics like data manipulation, visualization, machine learning, etc.\nPackages have to be installed and loaded before using them.\nWe need to install packages only once in our environment. That can be done in two ways:\nFor example, in several exercises in this book we use a library (created by for the book (Baayen 2008)) that contains some utilities and a few sample datasets with linguistics examples.\nLet‚Äôs install the library using the following command:\nAfter installing the package, the contents will still not be available until you load the package, this has to be done in every new work session and it is done with the library() function.\nNow you can access the functions and data in the languageR package.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#installing-and-using-a-package",
    "href": "using_libraries.html#installing-and-using-a-package",
    "title": "üì¶ Libraries and Packages",
    "section": "",
    "text": "Using the install.packages() function or\nUsing the Packages tab in RStudio:\n\n\n\n\ninstall.packages(\"languageR\")\n\n\n\n\n\n\nWarning\n\n\n\nThe installation of a package can give an error if you try to install a package that is already installed and loaded. If you get an error saying that you have already the package, just cancel.\n\n\n\nlibrary(languageR)",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#using-datasets-available-in-packages",
    "href": "using_libraries.html#using-datasets-available-in-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Using datasets available in packages",
    "text": "Using datasets available in packages\n\nAs mentioned, the main objective of a package is to distribute functions, but it is often convenient to include example datasets that can be used to illustrate the use of the functions. In other cases, there are packages that are use mainly to distribute data. The later is the case for example for the languageR package, provided as a companion to the book Baayen (2008) that we will use in some of the examples and assignments in this course.\nYou can explore data sets available from all loaded packages using the data() function. If you want the data from a specific package, specified with the argument package .\nTake a look at the packages datasets included in languageR using the following command.\ndata(package=\"languageR\")\nYou will see that a new tab opens in the Editor area with the content:\n\nIf you want more information more information of a particular dataset, you can use the help operator ? as with any function.\nFor example in today‚Äôs assignment we will use the lexdec dataset containing Lexical decision latencies collected from a group of speakers. Run the following command to read the composition of the dataframe.\n?lexdec",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "using_libraries.html#useful-packages",
    "href": "using_libraries.html#useful-packages",
    "title": "üì¶ Libraries and Packages",
    "section": "Useful packages",
    "text": "Useful packages\n\nThere are by now thousands of community contributed packages available for R and the list grows by the day (see complete list in CRAN website).\nIn practice you will use only a few packages on your data analysis tasks. I list below a number of commonly used packages for further reference (note we will only use a few of those in this course and I will always indicate it in the specific sessions or assignments).\n\nPackages for data input and output\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\nRead Excel files\nPart of tidyverse\n\n\nreadr\nRead information in tabular format from CSV and TAB separated files\nPart of tidyverse\n\n\n\n\n\nPackages for data analysis\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nforeign\nUsed to read data from other statistical analysis software as SPSS, Stata, etc\n\n\n\nreadxl\n\n\n\n\n\n\n\n\n\n\n\n\nPackages for data visualization and manipulation\n\n\n\nPackage\nUsage\nNotes\n\n\n\n\nggplot2\nLibrary to create graphics and data visualizations\nPart of tidyverse\n\n\ndplyr\nData manipulation functions\nPart of tidyverse\n\n\ntidyr\nFunctions to transform data from wide to long format.\nPart of tidyverse\n\n\nforcats\nFunctions to modify factors levels and ordering\nPart of tidyverse\n\n\n\nWe will explore functions from the packages above in the Workgroups 1 & 2\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "üì¶ Libraries and Packages"
    ]
  },
  {
    "objectID": "read_data.html",
    "href": "read_data.html",
    "title": "Reading and Saving data in R",
    "section": "",
    "text": "Working directory and paths in R\nWe have seen how to access datasets contained in packages, but to analyze your own data you will need to load it in the RStudio environment.\nWe will look in what follows at a few examples of loading data in different common formats (text files in general, excel files, files in SPSS format, R files).\nBefore entering into the details on the read and write of files, it is important to understand where the files are located and how to provide paths in R.\nPaths for files in R are relative to the ‚ÄúWorking Directory‚Äù. To know which is the working directory, you can use the function getwd() or select the option ‚ÄúGo to working directory‚Äù in the files tab.\nThe paths of a file are relative to that directory. Considering this, in the examples and assignments in this course, we will load data in the /data folder as follows:\n'./data/FILE_NAME'\nThe ./ in the path above indicates the working directory.\nWhen running commands loading or saving files, take care that you indicate the path correctly if you get an error.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "read_data.html#sec-r-read",
    "href": "read_data.html#sec-r-read",
    "title": "Reading and Saving data in R",
    "section": "Reading data from files",
    "text": "Reading data from files\n\nWe cover below examples for importing data in R, covering the most common methods and packages.\nData from text files\nThe most generic function to read data from a text file is read.table() . The arguments allow to define if the data includes a header (i.e.¬†a first row with the names of the columns/variables) and the separator used.\nFor example, let‚Äôs load a file in the /data directory containing the sample dataset from the previous exercise using the function below:\n\ndata_sample &lt;- read.table(\"./data/sample_wg1_text.txt\")\ndata_sample\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nData from CSV files\nMany experimental data is saved in a text or CSV (Comma separated) file. The best method to read data from these files is to use the read_csv() function in the readr package.\nLet‚Äôs see an example of usage loading the file in the /data directorate ‚ÄúELP_full_length_frequency.csv‚Äù with data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1.\n\nlibrary(readr)\n\ndata_sample_csv &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(data_sample_csv)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\n\n\nAs you can see, the file was loaded with 33,057 observations!\nTo free up memory, let‚Äôs remove the variable from the workspace using the rm() function (remove):\n\nrm(data_sample_csv)\n\nIf your data is tab-separated, you can use the function read_tsv() . By default, the functions expect that the first row contains the names of the columns/variables to be read. If that is not the case, you should modify the argument col_names = FALSE .\nData from Excel files\nThe best way to read Excel files is using the readxl package. If your file has several worksheets, you can use the sheet argument to specify either an index or the name of the worksheet to read.\n\n# load library\nlibrary(readxl)\n\n# Using readxl\ndata_sample_excel &lt;- read_excel(\"./data/sample_wg1_excel.xlsx\", sheet = 1)\ndata_sample_excel\n\n# A tibble: 6 √ó 4\n  NounAnimacy NounGender NounLength NounFrequency\n  &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 animate     feminine            6           638\n2 inanimate   masculine           7           799\n3 inanimate   neuter              4           390\n4 animate     masculine           5           569\n5 animate     feminine            8           567\n6 animate     feminine            6           665\n\n\nData in R files\nData can also be saved and loaded in R format directly. This is useful if you are performing your analysis in R to save data intermediate steps as it is a compact and efficient format, even though to share with other researchers and in open access journals you should export it to a more generic format.\nTo illustrate again let‚Äôs load a file in the /data directory containing a sample real dataset from a Event Related Potential (ERP) study that we will use in some of the exercises in the course:\n\ndata_sample_r &lt;- readRDS(\"./data/eegSampleData.Rda\")\nhead(data_sample_r)\n\n       time Subject Condition       Fp1       Fpz       Fp2       AF3      AF4\n1 -0.200000       1        12 1.9596113 1.6237565 1.4964751 1.9707278 2.232514\n2 -0.198999       1        12 1.7536104 1.4013030 1.3669174 1.7740067 2.084792\n3 -0.197998       1        12 1.5272095 1.1631496 1.2091597 1.5532856 1.912671\n4 -0.196997       1        12 1.2834086 0.9148962 1.0254020 1.3115644 1.718250\n5 -0.195996       1        12 1.0260077 0.6621427 0.8183443 1.0523433 1.504029\n6 -0.194995       1        12 0.7589068 0.4100893 0.5914866 0.7800222 1.273008\n          F7          F5        F3        F1        Fz        F2        F4\n1  0.9002055  0.97041392 1.6839135 1.8026822 1.7678664 1.8301742 1.8249546\n2  0.7315209  0.80466543 1.5461834 1.6655317 1.6480326 1.7056625 1.6921552\n3  0.5449364  0.62021693 1.3802533 1.4985812 1.4956989 1.5476508 1.5259558\n4  0.3422518  0.41866843 1.1874231 1.3030307 1.3121651 1.3574391 1.3284565\n5  0.1255673  0.20221994 0.9697930 1.0812802 1.0997314 1.1373274 1.1027571\n6 -0.1025173 -0.02662856 0.7301629 0.8365297 0.8614976 0.8909157 0.8531577\n         F6        F8         FT7       FC5       FC3       FC1       FCz\n1 1.6807566 1.6442472  0.79175532 1.0834129 1.1766302 1.2950722 1.6606659\n2 1.5340938 1.4931450  0.62251420 0.9396381 1.0489977 1.1714287 1.5515010\n3 1.3574309 1.3124428  0.43667307 0.7727634 0.8951652 1.0198851 1.4089360\n4 1.1532681 1.1046406  0.23643195 0.5843886 0.7166327 0.8417415 1.2341711\n5 0.9250052 0.8733384  0.02489083 0.3771138 0.5154001 0.6391979 1.0294061\n6 0.6768424 0.6229362 -0.19425030 0.1538391 0.2946676 0.4151543 0.7976412\n        FC2      FC4       FC6       FT8         T7          C5        C3\n1 1.7052435 1.828386 1.6453833 1.5302436  0.6388098  0.82305823 1.1149519\n2 1.6106100 1.725298 1.5067077 1.3838023  0.4772181  0.66489982 1.0142226\n3 1.4794766 1.593309 1.3373322 1.2091609  0.2951264  0.48534141 0.8858932\n4 1.3130431 1.433921 1.1395566 1.0086196  0.0949347  0.28668300 0.7310638\n5 1.1138097 1.249232 0.9165811 0.7854783 -0.1200570  0.07182459 0.5512344\n6 0.8853762 1.042444 0.6724056 0.5438370 -0.3464487 -0.15573382 0.3492050\n         C1         Cz        C2        C4        C6        T8        TP7\n1 1.1306227 0.63096230 1.3182707 1.4574666 1.4729369 1.3580312 -0.1544125\n2 1.0314048 0.57397554 1.2505935 1.3564641 1.3762993 1.2646492 -0.2040742\n3 0.9031868 0.48648877 1.1480163 1.2251617 1.2511616 1.1442672 -0.2693359\n4 0.7472689 0.36920200 1.0112391 1.0652592 1.0988239 0.9982852 -0.3505976\n5 0.5654510 0.22341524 0.8417619 0.8787567 0.9212863 0.8285032 -0.4477593\n6 0.3606331 0.05132847 0.6424847 0.6691542 0.7215486 0.6379212 -0.5599209\n         CP5       CP3         CP1        CPz        CP2       CP4       CP6\n1 0.79635392 0.7767967  0.48835529 0.58856683 0.50392506 0.8973196 0.8844379\n2 0.69718724 0.6999467  0.42408573 0.53621461 0.46937197 0.8189115 0.8011065\n3 0.57452056 0.5954966  0.33161617 0.45326240 0.40481887 0.7127035 0.6903752\n4 0.42925389 0.4640466  0.21144661 0.33981018 0.31056577 0.5798955 0.5537438\n5 0.26278721 0.3069966  0.06507705 0.19715797 0.18771268 0.4227875 0.3934124\n6 0.07762053 0.1267466 -0.10529251 0.02740575 0.03845958 0.2442795 0.2123810\n        TP8          P7         P5          P3          P1          Pz\n1 0.9127783  0.44240355  0.5012748  0.50060479  0.43724552  0.44452279\n2 0.8270859  0.38499520  0.4463851  0.43989524  0.38284579  0.39195007\n3 0.7139935  0.30168685  0.3652955  0.35238568  0.30064606  0.30987735\n4 0.5753011  0.19277851  0.2583059  0.23837612  0.19084634  0.19880464\n5 0.4131088  0.05907016  0.1263162  0.09886657  0.05454661  0.05993192\n6 0.2306164 -0.09753819 -0.0289734 -0.06424299 -0.10655312 -0.10434080\n            P2        P4        P6        P8         PO7         PO5\n1  0.468759124 0.8508531 0.8558191 0.8671508  0.35547460  0.36233623\n2  0.432358313 0.7741135 0.7808388 0.7798563  0.29344668  0.29959166\n3  0.365657503 0.6698739 0.6783584 0.6649618  0.20641876  0.21124709\n4  0.268956693 0.5394343 0.5496781 0.5242673  0.09449085  0.09760252\n5  0.143555882 0.3848946 0.3967978 0.3598728 -0.04133707 -0.04054205\n6 -0.008344928 0.2089550 0.2223175 0.1750783 -0.19966498 -0.20118662\n          PO3         POz        PO4         PO6         PO8        CB1\n1  0.28232490 -0.03600213 -0.7460777  0.52166620  0.49329201 -0.2268531\n2  0.22316096 -0.06933023 -0.7465746  0.45118577  0.42236878 -0.2837141\n3  0.13819702 -0.13065833 -0.7646715  0.35510534  0.32644555 -0.3584751\n4  0.02763308 -0.21928642 -0.8008684  0.23462491  0.20662231 -0.4507361\n5 -0.10743085 -0.33401452 -0.8546652  0.09154448  0.06479908 -0.5599971\n6 -0.26529479 -0.47284262 -0.9249621 -0.07133595 -0.09642415 -0.6844582\n          O1         Oz         O2        CB2\n1 -0.2783048 -0.6701305 -0.8214062 -0.8811375\n2 -0.3294788 -0.6538060 -0.8241543 -0.8791028\n3 -0.4000527 -0.6546815 -0.8441024 -0.8940680\n4 -0.4900266 -0.6739570 -0.8817505 -0.9264333\n5 -0.5987006 -0.7125325 -0.9365985 -0.9760985\n6 -0.7245745 -0.7706080 -1.0072466 -1.0419638\n\n\nData from SPSS\nThe haven package can be used to read directly .sav files from SPSS.\nAs an example again, let‚Äôs open a file saved in SPSS from another ERP (Event Related Potential) study of code-switching in Dutch.\n\n# If not yet available, install the package using install.packages(\"haven\")\n\n# Load library\nlibrary(haven)\n\ndata_sample_spss &lt;- read_sav(\"./data/EEG_DataSet_LongFormat_pablos.sav\")\nhead(data_sample_spss)\n\n# A tibble: 6 √ó 6\n  Participant Language CS    Congruency Electrode Amplitude\n        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n1           7 Dutch    no_CS congruent  Fp1           0.712\n2           7 Dutch    no_CS congruent  AF3           0.313\n3           7 Dutch    no_CS congruent  F7            0.982\n4           7 Dutch    no_CS congruent  F3            0.587\n5           7 Dutch    no_CS congruent  FC1           0.711\n6           7 Dutch    no_CS congruent  FC5           2.83 \n\n\nAgain to preserve memory, let‚Äôs clean the workspace by removing the variables we created using :\n\nrm(data_sample_spss, data_sample_r)\n\nOther formats\nIn your own research, if you have data in other specific formats, you may have to develop your own function to read it, although it is highly likely there is already a solution available. Always search first in the internet‚Ä¶ For example, imagine you have data in a file saved in the software Matlab, typing ‚Äúread Matlab files in R‚Äù in any search engine points to the R package R.matlab that offers the functions readMat() and writeMat() to read and write respectively Matlab files.\nA few examples of useful packages for data reading in linguistics research are listed below:\n\n\nPackage\nDescription\n\n\n\neegUtils\n\nUtilities for Electroencephalographic (EEG) Analysis:\nIncludes import functions for EEG files from several EEG acquisition and analysis software suites: ‚ÄòBioSemi‚Äô (.BDF), ‚ÄòNeuroscan‚Äô (.CNT), ‚ÄòBrain Vision Analyzer‚Äô (.VHDR), ‚ÄòEEGLAB‚Äô (.set) and ‚ÄòFieldtrip‚Äô (.mat)\n\n\n\nrprime\n\nPackage for parsing¬†.txt¬†generated by E-Prime, a program for running psychological experiments.\nSupport functions to read and clean data created in E-Prime\n\n\n\nchildesr\nPackage to access data in the childes-db, an open database of child language datasets from the CHILDES (Child Language Data Exchange System) data bank.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reading and Saving data in R"
    ]
  },
  {
    "objectID": "tidyverse_intro.html",
    "href": "tidyverse_intro.html",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "üåê What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. These packages share an underlying philosophy, grammar, and data structures, making it easier to learn and use them together. Tidyverse simplifies tasks like data manipulation, visualization, and modeling.\nA full description of the tidyverse packages and functions is available in https://dplyr.tidyverse.org/",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#what-is-the-tidyverse",
    "href": "tidyverse_intro.html#what-is-the-tidyverse",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "",
    "text": "Why Use tidyverse?\nUsing the functions in tidyverse have a number of advantages versus the use of individual packages or the available base R functions:\n\nConsistent and readable syntax.\nPipe operator (%&gt;%) for chaining operations (see later)\nDesigned for tidy data principles as described in Wickham, √áetinkaya-Rundel, and Grolemund (2024).\nIt has a strong user community and documentation.\nüì¶ Core tidyverse Packages\n\nWhen you install and load the tidyverse package, you get access to the core packages listed in the table below without needing to load them individually\n\n\nPackage\nUsage\n\n\n\nggplot2\nData visualization\n\n\ndplyr\nData manipulation\n\n\ntidyr\nData tidying\n\n\nreadr\nReading rectangular data (CSV, etc.)\n\n\npurrr\nFunctional programming\n\n\ntibble\nModern data frames\n\n\nstringr\nString manipulation\n\n\nforcats\nWorking with categorical data (factors)\n\n\n\nWe have already seen readr when loading files. We will explore today the tidyr functions.\nWorkflow Using tidyverse\nA generic workflow using tidyverse can be represented as follows:\n\n\n\n\n\nflowchart LR\n  load[\"Load tidyverse\"]\n  read[\"Read data\"]\n  clean[\"Clean and transform data\"]\n  plot[\"Visualize\"]\n  load --&gt; read\n  read --&gt; clean\n  clean --&gt; plot\n\n\n\n\n\n\nThis flow result in a code as the following example:\n\n\n\n\n\n\nImportant\n\n\n\nYou are not expected to understand all the code below now. We will explain different elements during the course. It is just intended as an example of the workflow.\n\n\n\n# Load tidyverse\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.4     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read data\n\ndf &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\", show_col_types = FALSE)\n\n# filter, clean and transform data\n\ncleandata &lt;- df %&gt;%\n  filter(length&gt;3) %&gt;%\n  mutate(rt_per_character = RT / length)\n\n# Visualize\n\nggplot(cleandata, aes(x = Log10Freq, y = RT)) + geom_point(color='grey') + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "href": "tidyverse_intro.html#a-grammar-of-data-manipulation",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "A grammar of data manipulation",
    "text": "A grammar of data manipulation\n\ndplyr is a package within the tidyverse set of functions that allow to manipulate data. You can think of the functions in the package as a sort of ‚Äúgrammar of data manipulation‚Äù, providing a consistent set of verbs that help you solve the most common data manipulation challenges:\n\nmutate() adds new variables that are functions of existing variables -&gt; operates on columns.\nrelocate(): moves variables to a different position (change order of columns) -&gt; operates on columns.\nselect() picks variables based on their names. -&gt; operates on columns.\nrename(): change variable names -&gt; operates on columns\narrange() changes the ordering of the rows. -&gt; operates on rows.\nfilter() picks cases based on their values. -&gt; operates on rows.\nsummarise() reduces multiple values down to a single summary (we will look at this in next workgroup).\n\n\n\n\n\n\n\nNote\n\n\n\nYou will see that the functions in tidyverse libraries return a so called tibble . The details are beyond the scope of this course, but you can think of a tibble as a version of a data frame.\n\n\nLet‚Äôs explore how to use the functions above with another of the datasets contained in the languageR package by Baayen (2008). We will use the DurationsOnt dataset that contains durational measurement of the Dutch prefix -ont from a study on a Spoken Dutch Corpus ( Pluymaekers, Ernestus, and Baayen (2005)).\n\n#load library\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\n#load dataset in the environment\ndata(\"durationsOnt\")\n\n#display the structure of the dataset\nstr(durationsOnt)\n\n'data.frame':   102 obs. of  12 variables:\n $ Word                 : Factor w/ 102 levels \"ontbeten\",\"ontbijt\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Frequency            : num  3.09 4.84 0 3.76 1.95 ...\n $ Speaker              : Factor w/ 63 levels \"N01001\",\"N01002\",..: 44 15 53 55 46 42 25 25 63 4 ...\n $ Sex                  : Factor w/ 2 levels \"female\",\"male\": 1 1 2 1 2 1 2 2 2 1 ...\n $ YearOfBirth          : num  72 80 52 60 74 70 76 76 76 66 ...\n $ DurationOfPrefix     : num  0.113 0.1 0.14 0.161 0.161 ...\n $ DurationPrefixVowel  : num  0.0695 0.0354 0.0474 0.0959 0.064 ...\n $ DurationPrefixNasal  : num  0.0439 0.0651 0.0925 0.0648 0.0973 ...\n $ DurationPrefixPlosive: num  0 0 0 0 0 0 0 0 0 0 ...\n $ NumberOfSegmentsOnset: int  1 1 1 1 1 1 1 1 2 2 ...\n $ PlosivePresent       : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ SpeechRate           : num  3.92 6.88 3.53 5.39 6.73 ...\n\n#look at the description of the variables\n?durationsOnt\n\nSelecting variables: select()\n\n\nThe function select() allows to choose a subset of variables (columns of interest). The function call includes as first parameter the dataset followed by the list of columns you would like to keep.\nFor this example we would like to look only at the duration of the Prefix in seconds and are not interested in the other variables.\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word, Frequency, Speaker, Sex, YearOfBirth, DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\nThe new dataframe durationsOnt_simplified contains now only the columns we specified.\n\n\n\n\n\n\nTip\n\n\n\nIf you have many columns, there is a simpler way to specify ‚Äúkeep from column X to column Y‚Äù without having to list each one of the individually by using : as in the example below that provides the same result. Of course this does not work if you want to select non-contiguous columns.\n\n\n\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\nhead(durationsOnt_simplified)\n\n             Word Frequency Speaker    Sex YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\nChanging variable names: rename()\n\n\nYou can change the naming of the variable using rename as follows - You call the function with as first parameter the dataset and after with the list of variables to be renamed.\\\n\n\n\n\n\n\nWarning\n\n\n\nNote that you write first the new name and then the old one.\n\n\n\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\nhead(durationsOnt_simplified_renamed)\n\n             Word Frequency Speaker Gender YearOfBirth DurationOfPrefix\n24       ontbeten 3.0910425  N01143 female          72         0.113372\n58        ontbijt 4.8441871  N01041 female          80         0.100478\n40  ontbijtbuffet 0.0000000  N01157   male          52         0.139806\n42      ontbijten 3.7612001  N01162 female          60         0.160739\n60      ontbijtje 1.9459101  N01145   male          74         0.161283\n21 ontbijtservies 0.6931472  N01134 female          70         0.176658\n\n\nMoving variable position: relocate()\n\n\nWith big datasets including many variables (columns), sometimes it is useful to change the order of the columns. This can be done with the relocate() function.\nYou call the function providing again as first parameter the dataset and then the column you want to move. By default the function will move the column specified to become the first. Let‚Äôs say that we want to have the Speaker Identifier as the first column, we will use the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker           Word Frequency Gender YearOfBirth DurationOfPrefix\n24  N01143       ontbeten 3.0910425 female          72         0.113372\n58  N01041        ontbijt 4.8441871 female          80         0.100478\n40  N01157  ontbijtbuffet 0.0000000   male          52         0.139806\n42  N01162      ontbijten 3.7612001 female          60         0.160739\n60  N01145      ontbijtje 1.9459101   male          74         0.161283\n21  N01134 ontbijtservies 0.6931472 female          70         0.176658\n\n\nThe Speaker variable is now the first. If we want to define a specific location, we can use the arguments .after or .before . For instance, if we want to move the Gender to be just after the Speaker column we will call the following:\n\ndurationsOnt_reordered &lt;- relocate(durationsOnt_reordered, Gender, .after = Speaker)\nhead(durationsOnt_reordered)\n\n   Speaker Gender           Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female       ontbeten 3.0910425          72         0.113372\n58  N01041 female        ontbijt 4.8441871          80         0.100478\n40  N01157   male  ontbijtbuffet 0.0000000          52         0.139806\n42  N01162 female      ontbijten 3.7612001          60         0.160739\n60  N01145   male      ontbijtje 1.9459101          74         0.161283\n21  N01134 female ontbijtservies 0.6931472          70         0.176658\n\n\nNow that we have selected the variables that we want and in the order that we want them, let‚Äôs filter the data to select some cases\nSelecting cases: filter()\n\n\nWe can select cases or observations based on a criteria using the filter() function. The filter function is called providing as a first parameter the dataset, followed by a condition or criteria to use for the selection.\nLet‚Äôs say that we want to make an analysis on the duration of people born after 1970\n\ndurationsOnt_reordered_filtered_age &lt;- filter(durationsOnt_reordered,YearOfBirth &gt; 70)\n\n#check on the minimum value of the Year of Birth with the function min()\nmin(durationsOnt_reordered$YearOfBirth)\n\n[1] 23\n\nmin(durationsOnt_reordered_filtered_age$YearOfBirth)\n\n[1] 71\n\n\nAs you can see on the example, the filter() selected the cases based on the condition specified.\nIf we wanted to select for example only the cases for Females, we will have used the filter as Gender == \"female\" . What about if we want to select the females born after 1970 in a single step? You can combine conditions using the operators & (meaning ‚Äúand‚Äù ) and | (meaning ‚Äúor‚Äù). For example:\n\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\nhead(durationsOnt_reordered_filtered_age_gender)\n\n   Speaker Gender      Word Frequency YearOfBirth DurationOfPrefix\n24  N01143 female  ontbeten  3.091042          72         0.113372\n58  N01041 female   ontbijt  4.844187          80         0.100478\n46  N01103 female   onthoud  3.433987          77         0.168078\n43  N01089 female  onthulde  2.079442          75         0.165369\n55  N01065 female onthullen  1.945910          78         0.185638\n78  N01096 female  ontkende  2.833213          78         0.186745\n\n\nReordering cases: arrange()\n\nFinally, the arrange() function orders the dataset by a selected value. For example to order the dataframe by Frequency:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, Frequency)\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n    Speaker Gender             Word Frequency YearOfBirth DurationOfPrefix\n102  N01037 female ontstekingsvocht 0.0000000          81         0.099240\n20   N01051 female       ontmaskerd 0.6931472          79         0.190070\n93   N01171 female    ontploffingen 1.3862944          76         0.114520\n32   N01051 female      ontmaskeren 1.6094379          79         0.143948\n88   N01110 female        ontvoeren 1.6094379          77         0.098185\n55   N01065 female        onthullen 1.9459101          78         0.185638\n\n\nWhat about if we want to use a reverse order? This can be done use the desc() modified on the variable to be used:\n\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nhead(durationsOnt_reordered_filtered_age_gender_ordered)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "tidyverse_intro.html#pipe-operator-or",
    "href": "tidyverse_intro.html#pipe-operator-or",
    "title": "Introduction to tidyverse 1: data manipulation",
    "section": "Pipe operator (%>% or |> )",
    "text": "Pipe operator (%&gt;% or |&gt; )\n\nAs you can see, in all the examples above we pass as first argument to the function the dataframe on which we want to make the operation.\nPutting all the calls before together :\ndurationsOnt_simplified &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_simplified_renamed &lt;- rename(durationsOnt_simplified, Gender = Sex)\ndurationsOnt_reordered &lt;- relocate(durationsOnt_simplified_renamed, Speaker)\ndurationsOnt_reordered_filtered_age_gender &lt;- filter(durationsOnt_reordered,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_reordered_filtered_age_gender_ordered &lt;- arrange(durationsOnt_reordered_filtered_age_gender, desc(Frequency))\nIn this examples I created on every step a new variable with a different name to save the output of the functions, which created a lot of new variables. This might be useful only in cases when you want to save the intermediate steps. In reality this is rarely the case since anyway you can always execute again the complete flow if you want to change something, and it is a better practice.\nThat being the case, we could just use on variable for example called durationsOnt_processed that we overwrite in every step, resulting in something like this:\ndurationsOnt_processed &lt;- select(durationsOnt,  Word:DurationOfPrefix)\ndurationsOnt_processed &lt;- rename(durationsOnt_processed, Gender = Sex)\ndurationsOnt_processed &lt;- relocate(durationsOnt_processed, Speaker)\ndurationsOnt_processed &lt;- filter(durationsOnt_processed,(YearOfBirth &gt; 70) & (Gender==\"female\"))\ndurationsOnt_processed &lt;- arrange(durationsOnt_processed, desc(Frequency))\nIn this process though, still everytime the output of the one call is the input for the next step (in the end it is an analysis workflow).\nIf we were to write textually what we did to the data would be something like:\n\n‚ÄúTake the durationsOnt dataset then\nSelect columns ‚Ä¶ then\nrename column Sex to Gender then\nmove column Speaker to the first column then\nmove column Gender after Speaker then\nselect cases of Females born after 1970 then\narrange by frequency in descending order.‚Äù\n\nThis is where the concept of a pipe was introduced in R. The pipe operator allows you to¬†pass the result of one function directly into the next function¬†as its first argument, without the need to explicitly write it. It‚Äôs widely used in the¬†tidyverse, especially with¬†dplyr.\nThe operator can be used with two syntax %&gt;% or the new |&gt; .\nUsing this, the examples before could be executed as below.\n\ndurationsOnt_processed &lt;- durationsOnt %&gt;% select(Word:DurationOfPrefix) %&gt;%\n                          rename(Gender = Sex) %&gt;%\n                          relocate(Speaker) %&gt;%\n                          relocate(Gender, .after = Speaker) %&gt;%\n                          filter((YearOfBirth &gt; 70) & (Gender==\"female\")) %&gt;%\n                          arrange(desc(Frequency))\n\nhead(durationsOnt_processed)\n\n   Speaker Gender        Word Frequency YearOfBirth DurationOfPrefix\n71  N01041 female    ontstaan  6.068426          80         0.205519\n31  N01212 female     ontmoet  5.087596          83         0.119349\n58  N01041 female     ontbijt  4.844187          80         0.100478\n89  N01041 female  ontspannen  4.672829          80         0.143707\n12  N01090 female    ontstond  4.574711          76         0.127984\n11  N01092 female ontzettende  4.189655          80         0.117997\n\n\nAs you can see, the result is the case, but the code and data processing flow is much more readable, and there is no need to introduce intermediate variables.\nYou will become more familiar along the course with the basic usage of the tidyverse data manipulation.s\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge University Press.\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005. ‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù The Journal of the Acoustical Society of America 118 (4): 2561‚Äì69.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Introduction to tidyverse 1: data manipulation"
    ]
  },
  {
    "objectID": "using_notebooks.html",
    "href": "using_notebooks.html",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "",
    "text": "R Scripts\nUp to now we have described the usage of R and RStudio as an interactive data analysis environment where we introduce commands and get outputs on the Console.\nAn important part of research and data analysis is being able to reproduce the analysis and work you produce and report.\nIn order to that, reproducible research includes:\nScripts are used to collect the commands and steps used in a data analysis. In the R language, a script is a text file with commands saved with extension .R.\nTo create a script, select File -&gt; New File -&gt; R Script\nThis will open a file in the Editor where you can type a series of commands and comments. Comments are lines starting with # .\nYou can execute the code you want to run highlighting it and pressing¬†Ctrl + Enter¬†(Windows) or¬†Cmd + Enter¬†(Mac) or with the Green ‚ÄúRun‚Äù button on the top right of the Editor window.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#r-markdown-and-notebooks",
    "href": "using_notebooks.html#r-markdown-and-notebooks",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "R Markdown and Notebooks",
    "text": "R Markdown and Notebooks\nAn alternative to scripts are Notebooks, which are a powerful way of documenting work by producing documents that mix plain text and code.¬†\nNotebooks include chunks of code that can be executed and the output displayed and included in the document, together with textual input formatted using R Markdown language. In fact the current workbook is written using this approach.\nA full description of R Markdown is beyond the scope of this course, but also not required to follow the content and exercises. In the section below a basic introduction is given to create a Notebook, use the Visual editor and generate PDF output to hand-in your assignments.\n\nNotebook instructions\n\n\n\n\n\n\nCaution\n\n\n\nBeware that the environment of a Notebook is not the same as the R session environment!\n\n\nTo create a notebook in R Markdown, you can select File -&gt; New File -&gt; R Notebook\n\nTo show the capabilities of the Notebook, we will look at the assignments to be delivered:\n\nOpen the file Assignment1.Rmd by clicking on the file on the name in the file tab.\nIf not active, select ‚ÄúVisual‚Äù in the edit mode on the upper left.\nEnter your name and Student ID in the author field.\n\n\n\nEnter the code to answer each of the questions in the relevant code section, that appear in grey and with a {r} marking.\nAfter entering the code, run it pressing the green arrow on the top right corner of the code section\nRemember to save from time to time!\n\n\nWhen you are ready to hand in your assignment you can generate a PDF to upload in Brightspace. This is the procedure to do it:\n\nClick on the small arrow next to the Knit button (see figure below).\nSelect ‚ÄúKnit to PDF‚Äù\nSave the PDF file generated somewhere in your drive or computer.\nUpload it to Brightspace on the relevant assignment.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "using_notebooks.html#additional-reading",
    "href": "using_notebooks.html#additional-reading",
    "title": "Reproducible research using Scripts and Notebooks",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nIf you are interested into knowing more on the capabilities of R notebooks, you can use this resources.\n\n\n\n\n\n\nImportant\n\n\n\nThis material is for your own development but it is not required for the course. I add it here as additional information in case you need to use it in your own research\n\n\n\nR Notebooks guide.\nDatacamp R Notebook tutorial",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Reproducible research using Scripts and Notebooks"
    ]
  },
  {
    "objectID": "assignment1_key.html",
    "href": "assignment1_key.html",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "Task#1: Creating a dataframe\nYou were asked to create a dataframe made up sample data according to the following prescription:\nThis can be done exactly as in the example we followed in Workgroup 1, with the steps replicated below:",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task1-creating-a-dataframe",
    "href": "assignment1_key.html#task1-creating-a-dataframe",
    "title": "Assignment #1 - Answer key",
    "section": "",
    "text": "4 columns/variables: one column with a factor with two levels, another column with a factor with 3 levels and two columns with numeric values.\n6 rows or observations\n\n\n1.1. Create variables with the data.\nAs you were asked to create 6 observations, every variable contains six entries.\n\nNounAnimacy&lt;-c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\")\nNounGender&lt;-c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\")\nNounLength&lt;-c(6,7,4,5,8,6)\nNounFrequency&lt;-c(638,799,390,569,567,665) \n\n1.2. Create factors\nThe variables created above with categorical entries are considered as character vectors. We should convert them into factors using the function factor() .\n\nNounAnimacy &lt;- factor(NounAnimacy)\nNounGender &lt;- factor(NounGender)\n\n\n\n\n\n\n\nTip\n\n\n\nThe following could be performed in a single step by nesting functions as in the code below:\nNounAnimacy &lt;- factor(c(\"animate\",\"inanimate\",\"inanimate\",\"animate\",\"animate\",\"animate\"))\nNounGender&lt;- factor(c(\"feminine\",\"masculine\",\"neuter\",\"masculine\",\"feminine\",\"feminine\"))\n\n\n1.3. Create a dataframe\nWe can define a dataframe based on the variables we created as:\n\ndf_example&lt;-data.frame(NounAnimacy,NounGender,NounLength,NounFrequency)\ndf_example\n\n  NounAnimacy NounGender NounLength NounFrequency\n1     animate   feminine          6           638\n2   inanimate  masculine          7           799\n3   inanimate     neuter          4           390\n4     animate  masculine          5           569\n5     animate   feminine          8           567\n6     animate   feminine          6           665\n\n\nIf you look at the Environment window, a new variable was created called example with the specified contents:\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that when creating the dataset using the data.frame() function it builds a copy of the data. Modifying the original variables will not change df_example . You can remove the variables to clean up the workspace using rm()\n\nrm(NounAnimacy,NounFrequency, NounGender, NounLength)\n\n\n\nOnly df_example is left in the environment now:\n\nand you can access the individual columns in the dataframe using the $ operator. For example to see the column NounGender\n\ndf_example$NounGender\n\n[1] feminine  masculine neuter    masculine feminine  feminine \nLevels: feminine masculine neuter",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task2-loading-required-libraries",
    "href": "assignment1_key.html#task2-loading-required-libraries",
    "title": "Assignment #1 - Answer key",
    "section": "Task#2: loading required libraries",
    "text": "Task#2: loading required libraries\nLooking at the content of the assignment tasks, we will use data from the languageR package and functions from the dyplr package, which is part of the tidyverse environment.\nTo use both of them we have to load the libraries first\n\n# add code to load the required libraries\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.4     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "href": "assignment1_key.html#task3-working-with-and-exploring-dataframes",
    "title": "Assignment #1 - Answer key",
    "section": "Task#3: Working with and exploring dataframes",
    "text": "Task#3: Working with and exploring dataframes\nFrom the languageR library a sample dataframe called lexdec is used in this assignment. This dataset contains lexical decision latencies elicited from 21 subjects for 79 English concrete nouns.\n3.1. Load dataset in the environment\nTo use the lexdec dataset, we load it in the environment using the function data() . Although this step is not needed to access the data, it conveniently includes is as any other variable in the Environment window, so that it is possible to inspect it.\n\ndata(lexdec)\n\n\nYou can browse the information on the dataset in the documentation included with the package. You can do that with the help() function, using ? or searching in the help tab.\nhelp(lexdec)\n?lexdec\nThe help tab shows the description of each of the variables in the dataset\n\n3.2. Explore contents of the dataset\nYou were asked to show the first rows of the dataset.\nTyping the name of the dataset will show you the full contents, which is not too handy. Instead you can use View(lexdec) to load it in the RStudio Viewer, or, just to inspect he first rows, you can use the head() function as follows:\n\nhead(lexdec)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect\n1      A1 6.340359    23   F        English correct     word     correct\n2      A1 6.308098    27   F        English correct  nonword     correct\n3      A1 6.349139    29   F        English correct  nonword     correct\n4      A1 6.186209    30   F        English correct     word     correct\n5      A1 6.025866    32   F        English correct  nonword     correct\n6      A1 6.180017    33   F        English correct     word     correct\n        Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1        owl  4.859812  1.3862944   0.6931472      3 animal           54\n2       mole  4.605170  1.0986123   1.9459101      4 animal           69\n3     cherry  4.997212  0.6931472   1.6094379      6  plant           83\n4       pear  4.727388  0.0000000   1.0986123      4  plant           44\n5        dog  7.667626  3.1354942   2.0794415      3 animal         1233\n6 blackberry  4.060443  0.6931472   1.3862944     10  plant           26\n  FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1         74       0.7912 simplex -0.3101549 6.3582     3.12   3.4758\n2         30       0.6968 simplex  0.8145080 6.4150     2.40   2.9999\n3         49       0.4754 simplex  0.5187938 6.3426     3.88   1.6278\n4         68       0.0000 simplex -0.4274440 6.3353     4.52   1.9908\n5        828       1.2129 simplex  0.3977961 6.2956     6.04   4.6429\n6         31       0.3492 complex -0.1698990 6.3959     3.28   1.5831\n  meanWeight      BNCw      BNCc       BNCd BNCcRatio BNCdRatio\n1     3.1806 12.057065  0.000000   6.175602  0.000000  0.512198\n2     2.6112  5.738806  4.062251   2.850278  0.707856  0.496667\n3     1.2081  5.716520  3.249801  12.588727  0.568493  2.202166\n4     1.6114  2.050370  1.462410   7.363218  0.713242  3.591166\n5     4.5167 74.838494 50.859385 241.561040  0.679589  3.227765\n6     1.1365  1.270338  0.162490   1.187616  0.127911  0.934882\n\n\nAs can be seen from the output, by default it displays 6 rows. Looking at the documentation of the head() function, it describes that the function can take an argument n specifying how many rows to display.\n\nhead(lexdec, n = 4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency FamilySize SynsetCount Length  Class FreqSingular FreqPlural\n1  4.859812  1.3862944   0.6931472      3 animal           54         74\n2  4.605170  1.0986123   1.9459101      4 animal           69         30\n3  4.997212  0.6931472   1.6094379      6  plant           83         49\n4  4.727388  0.0000000   1.0986123      4  plant           44         68\n  DerivEntropy Complex      rInfl meanRT SubjFreq meanSize meanWeight      BNCw\n1       0.7912 simplex -0.3101549 6.3582     3.12   3.4758     3.1806 12.057065\n2       0.6968 simplex  0.8145080 6.4150     2.40   2.9999     2.6112  5.738806\n3       0.4754 simplex  0.5187938 6.3426     3.88   1.6278     1.2081  5.716520\n4       0.0000 simplex -0.4274440 6.3353     4.52   1.9908     1.6114  2.050370\n      BNCc      BNCd BNCcRatio BNCdRatio\n1 0.000000  6.175602  0.000000  0.512198\n2 4.062251  2.850278  0.707856  0.496667\n3 3.249801 12.588727  0.568493  2.202166\n4 1.462410  7.363218  0.713242  3.591166\n\n\n3.3 Extract the column names from the dataframe.\nAs per the workgroup notes, the names of the variables in the dataframe can be extracted using the colnames() function.\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n3.4. Sort dataframe\nYou were asked to sort the dataframe by the reaction time (RT variable). Sorting can be done in several ways, but we will use the tidyverse arrange() function as described in the workgroup.\n\nlexdec_ordered &lt;- arrange(lexdec,RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\nTo instead arrange in reducing order:\n\nlexdec_ordered &lt;- arrange(lexdec, desc(RT))\nhead(lexdec_ordered, n = 4)\n\n     Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n1194      T2 7.587311    44   F          Other incorrect  nonword     correct\n1619      M2 7.443664   105   F          Other   correct     word     correct\n1381      R1 7.425358   116   F        English   correct  nonword     correct\n1620      M2 7.403670   106   F          Other   correct     word     correct\n         Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n1194  gherkin  2.079442          0    1.098612      7  plant            4\n1619     leek  3.332205          0    1.098612      4  plant            5\n1381 beetroot  3.555348          0    1.098612      8  plant           15\n1620 hedgehog  3.637586          0    1.098612      8 animal           21\n     FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n1194          3            0 simplex  0.2231435 6.5161     3.16   2.2484\n1619         22            0 simplex -1.3437348 6.4200     3.68   1.8898\n1381         19            0 complex -0.2231435 6.4468     3.88   1.8705\n1620         16            0 simplex  0.2578291 6.5924     3.32   3.5920\n     meanWeight     BNCw    BNCc     BNCd BNCcRatio BNCdRatio\n1194     1.8185 0.111433 0.16249 0.237523  1.458184  2.131531\n1619     1.4590 0.846892 0.16249 2.375231  0.191866  2.804646\n1381     1.4442 0.635169 0.16249 0.475046  0.255822  0.747906\n1620     3.2545 2.718969 0.64996 3.325324  0.239047  1.223009\n\n\nAs an example, the previous can also be done using pipes. It does not make much difference in this task but it makes the code more readable when you perform several steps:\n\nlexdec_ordered &lt;- lexdec %&gt;% arrange(RT)\nhead(lexdec_ordered, n = 4)\n\n    Subject       RT Trial Sex NativeLanguage   Correct PrevType PrevCorrect\n542      A2 5.828946   159   M        English incorrect  nonword     correct\n815       K 5.852202    83   F        English incorrect     word     correct\n822       K 5.894403    99   F        English incorrect     word     correct\n73       A1 5.899897   174   F        English   correct     word     correct\n       Word Frequency FamilySize SynsetCount Length  Class FreqSingular\n542     pig  6.660575  2.7725887    2.302585      3 animal          320\n815   lemon  5.631212  0.6931472    1.609438      5  plant          233\n822  potato  6.461468  0.0000000    1.098612      6  plant          206\n73  chicken  6.599870  1.0986123    1.791759      7 animal          534\n    FreqPlural DerivEntropy Complex      rInfl meanRT SubjFreq meanSize\n542        460       1.6313 simplex -0.3619569 6.2783     4.48   3.4075\n815         45       0.6285 simplex  1.6266797 6.2900     5.04   1.8119\n822        433       0.0000 simplex -0.7403257 6.3833     5.80   2.8516\n73         200       0.0798 simplex  0.9789618 6.3028     5.68   4.3832\n    meanWeight      BNCw      BNCc      BNCd BNCcRatio BNCdRatio\n542     3.1200 12.636518 10.886833  17.57671  0.861537  1.390946\n815     1.4315 11.834199  3.087311  22.08965  0.260880  1.866595\n822     2.6131  7.934039  8.286993  25.89002  1.044486  3.263158\n73      4.1964 17.651009  7.312052 100.70981  0.414257  5.705612\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that on these examples, we saved the output in a new variable. If we were just calling the function as:\narrange(lexdec, RT)\nit would display on the console the output which is rather long and not easy to see.\n\n\nWe now have a new variable in the environment called lexdec_ordered containing a copy of the data from lexdec ordered by the value of RT .\n3.5. Select columns and rows\nYou were asked to create a new dataframe called lexdec_reduced with only the variables from Subject to Frequency and selecting only the entries from native English speakers (as coded in the NativeLanguage variable).\nAs described in the workgroup materials, two functions are used to select the data, one selecting the variables/columns of interest (select()) and another selecting the observations/rows based on a criteria (filter())\nLet‚Äôs see below how to do the same operation with and without using pipes.\n\n# Without pipes\n\nlexdec_reduced &lt;- select(lexdec,Subject:Frequency)\nhead(lexdec_reduced, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith this first command, we have selected a few columns from the original dataset. We can check that using for example colnames()\n\ncolnames(lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\n\ncolnames(lexdec_reduced)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"     \n\n\nAs you can see only the first columns (from Subject to Frequency) are retained in lexdec_reduced\nTo now select only the English native speakers, we use the function filter() :\n\nlexdec_filtered &lt;- filter(lexdec_reduced,NativeLanguage==\"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nTo check we can use the function unique() that tells us how many unique values are in a variable. The output below shows that NativeLanguage is a factor with two levels and both are present in the dataframe lexdec_reduced.\n\nunique(lexdec_reduced$NativeLanguage)\n\n[1] English Other  \nLevels: English Other\n\n\nIf we looked at the lexdec_filtered dataframe, it specifies that the variable is still a factor with two levels, but only ‚ÄúEnglish‚Äù is present in the data.\n\nunique(lexdec_filtered$NativeLanguage)\n\n[1] English\nLevels: English Other\n\n\nThe steps above could be performed using pipes:\n\n#Using pipes\n\nlexdec_filtered &lt;- lexdec %&gt;% \n                  select(Subject:Frequency) %&gt;% \n                  filter(NativeLanguage == \"English\")\nhead(lexdec_filtered, n=4)\n\n  Subject       RT Trial Sex NativeLanguage Correct PrevType PrevCorrect   Word\n1      A1 6.340359    23   F        English correct     word     correct    owl\n2      A1 6.308098    27   F        English correct  nonword     correct   mole\n3      A1 6.349139    29   F        English correct  nonword     correct cherry\n4      A1 6.186209    30   F        English correct     word     correct   pear\n  Frequency\n1  4.859812\n2  4.605170\n3  4.997212\n4  4.727388\n\n\nWith identical results and a more readable code.\n3.6. Save dataframe\nFinally you were asked to save the filtered dataframe into a file in the /data directorate.\nLet‚Äôs save the data for example in text format using the write.table() function:\n\nwrite.table(lexdec_reduced,file = \"./data/reduced_data.txt\")\n\nThis command creates a file in the /data directory called reduced_data.txt\n\nWe can also save a copy in R format using saveRDS() if using R for further analysis.\n\nsaveRDS(lexdec_reduced,\"./data/reduced_data.Rda\")\n\n\n\nAnd this concludes the first assignment of the course! You will be gaining familiarity with R and RStudio along the workgroups.",
    "crumbs": [
      "Workgroup 1: Introduction to R and RStudio",
      "Assignment #1 - Answer key"
    ]
  },
  {
    "objectID": "Workgroup2.html",
    "href": "Workgroup2.html",
    "title": "Workgroup 2: Data Exploration with R",
    "section": "",
    "text": "In this session we will continue to introduce basic concepts of data manipulation and plotting using the tidyverse package and start performing data exploration and descriptive statistics with R.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nPerform basic data transformation and reorganization.\nUnderstand concept of grammar of graphics and create basic plots.\nPerform basic descriptive statistics in R.\nCreate histograms and boxplot.\nCheck datasets for normality.\nWrite dataset summaries.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R"
    ]
  },
  {
    "objectID": "data_organization.html",
    "href": "data_organization.html",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Adding columns: mutate()\nAs mentioned in the previous workgroup, it is not in the scope of the course to cover all the functions available in the tidyverse framework. We introduce a subset of them useful to illustrate the concepts in the course.\nThe mutate() function, part of the dplyr package, allows to create new columns or variables in a dataset. It is used normally for values based in an existing column.\nAs an example, let‚Äôs use again the data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1. Dataset is in the file /ELP_full_length_frequency.csv in the /data directory.\nWe load the data as indicated in workgroup1\nlibrary(readr)\n\ndfELP &lt;- read_csv(\"./data/ELP_full_length_frequency.csv\",show_col_types = FALSE)\nhead(dfELP)\n\n# A tibble: 6 √ó 4\n  Word   Log10Freq length    RT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.\n2 zephyr      1.70      4  875.\n3 zeroed      1.30      5  929.\n4 zeros       1.70      5  625.\n5 zest        1.54      4  659.\n6 zigzag      1.36      6  785.\nLet‚Äôs say we want to add two new calculated values:\nWe can add the two columns using mutate() as follows\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî purrr     1.0.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndfELP_extended &lt;- dfELP %&gt;% mutate(RT_per_char = RT/length, logRT = log10(RT))\nhead(dfELP_extended)\n\n# A tibble: 6 √ó 6\n  Word   Log10Freq length    RT RT_per_char logRT\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 zenith      1.34      5  754.        151.  2.88\n2 zephyr      1.70      4  875.        219.  2.94\n3 zeroed      1.30      5  929.        186.  2.97\n4 zeros       1.70      5  625.        125.  2.80\n5 zest        1.54      4  659.        165.  2.82\n6 zigzag      1.36      6  785.        131.  2.90",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#adding-columns-mutate",
    "href": "data_organization.html#adding-columns-mutate",
    "title": "Data organization and transformation",
    "section": "",
    "text": "Reading time per character: calculated dividing RT by length\nlogarithmic Reading Time : calculated using the log10() function\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTo avoid filling up the memory in the system, we will remove the variables after completing the examples when not using them again for a while\n\nrm(dfELP, dfELP_extended)",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#data-organization-long-and-wide-format",
    "href": "data_organization.html#data-organization-long-and-wide-format",
    "title": "Data organization and transformation",
    "section": "Data organization: long and wide format",
    "text": "Data organization: long and wide format\nA common task when preparing the analysis steps is to reshape the data in a way that can be used for the analysis.\nWe can widely characterize the data into two groups wide and long. (see section 3.9.4 of Field (2026) for details)\n\n\nFrom A. Field\n\nThe tidyr package contains two useful functions to allow to transform the data from one format to the other: pivot_wider() and pivot_longer() .\nAgain let‚Äôs look at one example based on data from an Event Related Potential (ERP) experiment. In ERP analysis one common approach is to compare the average amplitude of the signal measured in several electrodes on a time window of interest for the ERP component expected. The file EEG_DataSet_Wide.Rda in the /data directory contains an example data set with the average voltage measured in a set of electrodes in a time window of 250-500ms to investigate the so-called N400 ERP component.\nLet‚Äôs load and look at the data:\n\ndfEEG_wide &lt;- readRDS(\"./data/EEG_DataSet_Wide.Rda\")\nhead(dfEEG_wide, n=8)\n\n# A tibble: 8 √ó 64\n# Groups:   Subject [2]\n  Subject Condition    Fp1     Fpz    Fp2     AF3    AF4     F7     F5     F3\n  &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1       CondA     -3.05  -1.04   -2.88  -3.06   -3.31  -2.05  -2.06  -2.89 \n2 1       CondB     -1.03  -1.57   -0.842 -1.84   -0.361 -1.08  -1.06  -0.158\n3 1       CondC      0.575  1.52   -0.198  0.818   0.235  0.200  0.226  1.04 \n4 1       CondD     -2.20  -1.02   -1.63  -2.75   -1.90  -1.63  -1.62  -1.79 \n5 2       CondA      0.655  0.251   0.133  0.606   0.190  0.503  0.741  0.762\n6 2       CondB     -1.80  -2.13   -2.31  -2.33   -2.45  -1.23  -2.12  -2.12 \n7 2       CondC      0.145  0.0108  0.122  0.0548 -0.172  0.216  0.226  0.180\n8 2       CondD     -1.38  -1.66   -1.74  -0.980  -1.43  -0.797 -0.963 -0.944\n# ‚Ñπ 54 more variables: F1 &lt;dbl&gt;, Fz &lt;dbl&gt;, F2 &lt;dbl&gt;, F4 &lt;dbl&gt;, F6 &lt;dbl&gt;,\n#   F8 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FC5 &lt;dbl&gt;, FC3 &lt;dbl&gt;, FC1 &lt;dbl&gt;, FCz &lt;dbl&gt;, FC2 &lt;dbl&gt;,\n#   FC4 &lt;dbl&gt;, FC6 &lt;dbl&gt;, FT8 &lt;dbl&gt;, T7 &lt;dbl&gt;, C5 &lt;dbl&gt;, C3 &lt;dbl&gt;, C1 &lt;dbl&gt;,\n#   Cz &lt;dbl&gt;, C2 &lt;dbl&gt;, C4 &lt;dbl&gt;, C6 &lt;dbl&gt;, T8 &lt;dbl&gt;, TP7 &lt;dbl&gt;, CP5 &lt;dbl&gt;,\n#   CP3 &lt;dbl&gt;, CP1 &lt;dbl&gt;, CPz &lt;dbl&gt;, CP2 &lt;dbl&gt;, CP4 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   TP8 &lt;dbl&gt;, P7 &lt;dbl&gt;, P5 &lt;dbl&gt;, P3 &lt;dbl&gt;, P1 &lt;dbl&gt;, Pz &lt;dbl&gt;, P2 &lt;dbl&gt;,\n#   P4 &lt;dbl&gt;, P6 &lt;dbl&gt;, P8 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO5 &lt;dbl&gt;, PO3 &lt;dbl&gt;, POz &lt;dbl&gt;, ‚Ä¶\n\n\nThe table contains 64 columns, with one row representing the measurements on a particular condition per subject.\n\nColumn 1 - ‚ÄúSubject‚Äù - ID of the participant\nColumn 2 - ‚ÄúCondition‚Äù - factor representing the condition with four levels (CondA, CondB, CondC, CondD).\nColumns 3-64: Average Voltages in 250-500 ms time window after stimuli onset at 61 electrodes.\n\nFor the data analysis, we would like to actually see the effect on the Voltage measurement at different electrode sites, so we would like to actually have the data organized as:\n\n\nSubject\nCondition\nElectrode\nAmplitude\n\n\n\n1\nCondA\nFp1\n-3.047\n\n\n1\nCondA\nFpz\n-1.037\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nWe can do this with the pivot_longer() function, that has the following syntax:\npivot_longer(data, cols, names_to = \"name\", values_to = \"value\")\n\ndata : dataframe to be transformed\ncols: columns to be converted into longer format\nnames_to: name of the variable that will contain as values the names of the cols argument\nvalues_to: name of the variable that will contain values\n\nIn our example, we want to pivot all columns with electrode name. Normally you could provide a list, but since here we have several columns we can specify a range: ‚Äúfrom column Fp1 to CB2‚Äù\n\ndfEEG_long &lt;- pivot_longer(dfEEG_wide,cols = Fp1:CB2, names_to = \"Electrode\",values_to= \"AvgVoltage\") \nhead(dfEEG_long,n=8)\n\n# A tibble: 8 √ó 4\n# Groups:   Subject [1]\n  Subject Condition Electrode AvgVoltage\n  &lt;fct&gt;   &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 1       CondA     Fp1            -3.05\n2 1       CondA     Fpz            -1.04\n3 1       CondA     Fp2            -2.88\n4 1       CondA     AF3            -3.06\n5 1       CondA     AF4            -3.31\n6 1       CondA     F7             -2.05\n7 1       CondA     F5             -2.06\n8 1       CondA     F3             -2.89\n\n\nAs can be seen from the output, we have the data in the format expected\nTransforming into wide format is done similarly with the pivot_wider() function, that takes the following arguments:\n\ndata : dataframe to be transformed\nnames_from : column (or columns) to get the name of the output column\nvalues_from : column (or columns) to get the cell values from\n\nLet‚Äôs imagine that we would like to transform our last dataframe into a table of the form:\n\n\nSubject\nElectrode\nCondA\nCondB\nCondC\nCondD\n\n\n\n1\nFp1\n\n\n\n\n\n\n1\nFpz\n\n\n\n\n\n\n‚Ä¶\n‚Ä¶\n\n\n\n\n\n\n\nWe can do the following:\n\ndfEEG_wide_2 &lt;- pivot_wider(dfEEG_long,names_from = \"Condition\",values_from= \"AvgVoltage\") \nhead(dfEEG_wide_2,n=8)\n\n# A tibble: 8 √ó 6\n# Groups:   Subject [1]\n  Subject Electrode CondA  CondB  CondC CondD\n  &lt;fct&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1       Fp1       -3.05 -1.03   0.575 -2.20\n2 1       Fpz       -1.04 -1.57   1.52  -1.02\n3 1       Fp2       -2.88 -0.842 -0.198 -1.63\n4 1       AF3       -3.06 -1.84   0.818 -2.75\n5 1       AF4       -3.31 -0.361  0.235 -1.90\n6 1       F7        -2.05 -1.08   0.200 -1.63\n7 1       F5        -2.06 -1.06   0.226 -1.62\n8 1       F3        -2.89 -0.158  1.04  -1.79\n\n\n\n\n\n\n\n\nNote\n\n\n\nPivoting tables can be confusing at first. You will master it with practice on your own data. This section is included to illustrate a typical data manipulation and provide tips for your own analysis but will not be used in exercises or exams.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_organization.html#further-reading",
    "href": "data_organization.html#further-reading",
    "title": "Data organization and transformation",
    "section": "Further reading",
    "text": "Further reading\nIf you want to go more into details here are links to good resources:\n\nR for data Science ( Wickham, √áetinkaya-Rundel, and Grolemund (2024)) Chapter 5 - Data tyding\nTidyverse article on Pivoting\n\n\n\n\n\nField, Andy P. 2026. Discovering Statistics Using R and RStudio. London: SAGE Publications.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024. R for Data Science. O‚ÄôReilly.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Data organization and transformation"
    ]
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "üìäData visualization in R",
    "section": "",
    "text": "Introduction to ggplot2\nR provides several ways to plot data, from simple plots using basic functions like plot() to complex visualization with specialized packages.\nFor this course we have chosen to introduce plotting based on the ggplot2 package, part of the tidyverse environment because of its versatility to generate almost any required visualization in the linguistics field. A full treatment of the capabilities of ggplot is beyond the scope of this course and you are not expected to know how to use it for the exams, but will need it for the assignments.\nFor a full description refer to Wickham (2016), available online.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#grammar-of-graphics",
    "href": "data_visualization.html#grammar-of-graphics",
    "title": "üìäData visualization in R",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\n\nThe name, ggplot, it is due to the underlying concept based on the notion of Grammar of Graphics ( Wilkinson (2011))\nThe concept behind ggplot2 divides a plot into different fundamental composable parts:\n\n\nImage extracted from ggplot introduction\n\nThe main components are\nPlot = Data + Mapping + Layers (or Geometry)\n\nData is a data frame with the source of the data to be represented.\nMapping is used to indicate with of the data elements are mapped to the aesthetics of the graph (i.e.¬†the x and y axis. It can also be used to control the color/ size / shape of points, the height of bars, etc.\nGeometry defines the type of graphics (i.e., histogram, box plot, line plot, density plot, dot plot, etc.).\n\nTo explain these concepts, let‚Äôs walk through an example of the steps to build a plot. We will make use again of the lexdec dataset in the languageR package.\n\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\ndata(\"lexdec\")\n\nTo build a plot, first we have to indicate the data that will be used to ggplot\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.4     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec)\n\n\n\n\n\n\n\nThis command has created an empty canvas, since we have not mapped the data to the plot. We have to provide a mapping of which variables are to be displayed. We do this mapping with the aes() function. Let‚Äôs plot the Reading Time as a function of the Frequency of the word.\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT))\n\n\n\n\n\n\n\nNow we get a plot, with axis indicating the two elements we have mapped, but nothing is displayed. This is because we have not specified the geometry or representation.\nIf we want to make a scatter plot, we use the geom_point() function:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\nThe basic geometries in ggplot are:\n\ngeom_point() : scatter plot\ngeom_line() : line plot\ngeom_bar() : bar chart\ngeom_histogram() : histogram\ngeom_boxplot() : boxplot\n\nAdditional variables beyond x and y, can be considered using the colour or fill arguments. For instance, in the example above, we could color the points differently according to the NativeLanguage of the subject in the observation:\n\nggplot(lexdec, mapping = aes(x = Frequency, y = RT, color=NativeLanguage)) + geom_point()\n\n\n\n\n\n\n\nIn the following we look at a couple of plots that we addressed in the first lectures",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#histograms",
    "href": "data_visualization.html#histograms",
    "title": "üìäData visualization in R",
    "section": "Histograms",
    "text": "Histograms\n\nA histogram is a representation of the frequency distribution of the data. It is created using the geom_histogram() function. Note that for this plot, only one variable is needed to be specified for the x-axis, since the other coordinate is the count of cases.\nFor example, to look at the distribution of the reaction time data:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nWe can see that the data is only slightly right skewed, since it corresponds to a log transformed Reaction Time.\nIn a histogram, we can adjust the arguments bins and binwidth to determine the resolution of our grouping. The default is 30 bins distributed equally between the min and maximum value. Let‚Äôs see what happens if we change the value to 70:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\nLet‚Äôs illustrate another capability of the layered concept to build graphs in ggplot: if we were interested to see if the distribution of reaction times is different between males and females, we could filter the data for each of the groups and plot two histograms (let‚Äôs do this using pipes this time)\n\n#histogram for females\n\nlexdec %&gt;% filter(Sex==\"F\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n#histogram for males\nlexdec %&gt;% filter(Sex==\"M\") %&gt;% \n           ggplot(aes(x=RT)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nHowever we can also do it using facets in ggplot, which allows us to create multiple graphs based on a given variable. See the example below:\n\nggplot(lexdec, aes(x=RT)) + geom_histogram() + facet_wrap(~Sex)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNow both plots are places next to each other and, more importantly, with the same scale on the axis. By looking at the graphs, the distributions are similar, but the Female is higher, pointing to the fact that there were likely more female participants than males (remember the y-axis in a histogram is a count).\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying\n\n\n\n\n\n\nNote\n\n\n\nAdvanced:\nThis can be solved by using a ‚Äònormalization‚Äô, that is, asking R to plot the histogram divided by the maximum count (this is call a density function). This can be done specifying in plot_histogram() a specific aesthetic aes(y =..density..)\n\nggplot(lexdec, aes(x=RT)) + geom_histogram( aes( y =..density..)) + facet_wrap(~Sex)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "data_visualization.html#boxplot",
    "href": "data_visualization.html#boxplot",
    "title": "üìäData visualization in R",
    "section": "Boxplot",
    "text": "Boxplot\n\nAnother representation of the distribution of the data that we discussed in the lecture is the boxplot. Boxplots can be plotted in ggplot using the geom_boxplot() function.\nIf we look again at the RT as a function of Sex can simply change the call as follows:\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot() \n\n\n\n\n\n\n\nOne observation is that the plot shows a number of points (circles) beyond the end of the ‚Äôwhisker‚Äô lines. If we look back at our definition in the lecture for the boxplot:\n\nThe box plot representation was based on the 5-point summary (min, max, median, Q1 and Q3) with the whiskers representing the min and max values.\nSoftware packages however, have a different implementation. By default geom_boxplot() places whisker edges at 1.5 times the Inter Quartile Range (IQR).\n\nYou can change the default behaviour by using the coef argument. For example, the call below extends the whiskers to 2*IQR.\n\nggplot(lexdec, aes(x=Sex, y = RT)) + geom_boxplot( coef = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways check the manual of a function if you are not sure what is calculating/plotting.\nIn this case, looking at the help of page of geom_boxplot() you will find the following:\n‚ÄúThe upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called ‚Äúoutlying‚Äù points and are plotted individually‚Äù\n\n\n\n\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In Handbook of Computational Statistics: Concepts and Methods, 375‚Äì414. Springer.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "üìäData visualization in R"
    ]
  },
  {
    "objectID": "descriptive_stats.html",
    "href": "descriptive_stats.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Measures of central tendency\nWe described in the lecture three main measures of central tendency: the mean, the median and the mode.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-central-tendency",
    "href": "descriptive_stats.html#measures-of-central-tendency",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Mean\nThe mean is described in mathematically as:\n\\[\n\\overline{X}=\\frac{\\sum_{i=1}^{N}{X_i}}{N}\n\\]\nIf we have a list of numbers, we could calculate it using the function sum() as in\n\nx &lt;- c(1,3,5,6,8,2)\nsum(x) / 6\n\n[1] 4.166667\n\n\nThere is however a built in function mean() that does this calculation:\n\nmean(x)\n\n[1] 4.166667\n\n\nMedian\nSimilarly, the median can be calculated by ranking the values and selecting those in the middle, but there is a convenience function median()\n\nmedian(x)\n\n[1] 4\n\n\nIf we have a dataset with a large outlier, the median is a more robust measurement of central tendency compared to the mean.\n\ny &lt;- c(1,3,5,6,8,2,200)\nmean(y)\n\n[1] 32.14286\n\nmedian(y)\n\n[1] 5\n\n\nMode\nThere is no built in function to calculate the mode in R. It is simply the value repeated the most in a dataset. The function table() can be used to create a count of the number of elements in a dataframe as a function of variable (so-called contingency tables). We will see in the next sections how to do that with tidyverse.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#point-summaries",
    "href": "descriptive_stats.html#point-summaries",
    "title": "Descriptive Statistics",
    "section": "5-point summaries",
    "text": "5-point summaries\nThere are several ways to calculate the five point summaries discussed in the lecture for a dataset. Using individual functions:\n\nmin() : minimum of a set of numbers\nmax() : maximum of a set of numbers\n\nquantile() : calculates the quantile based on a threshold.\n\nquantile(x, prob=0.25) provides the first quartile Q1\nquantile(x, prob=0.75) provides the third quartile Q3\nquantile(x, prob=0.5) corresponds to the median\n\n\nIQR() : provides the Inter-Quartile Range, that is equivalent to Q3 - Q1\nrange() : difference between maximum and minimum values\nmean()\nmedian()\n\nTaking again the example of the lexdec dataset and the Reaction Time RT\n\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\ndata(lexdec)\n\nmin(lexdec$RT)\n\n[1] 5.828946\n\nmax(lexdec$RT)\n\n[1] 7.587311\n\nrange(lexdec$RT)\n\n[1] 5.828946 7.587311\n\n\n\nquantile(lexdec$RT, prob = 0.25)\n\n     25% \n6.214608 \n\nquantile(lexdec$RT, prob = 0.75)\n\n    75% \n6.50204 \n\n#IQR calculated from quantiles\nquantile(lexdec$RT, prob = 0.75) - quantile(lexdec$RT, prob = 0.25)\n\n     75% \n0.287432 \n\nIQR(lexdec$RT)\n\n[1] 0.287432\n\n\nHowever, most of the values above can be calculated using the convenient summary() function\n\nsummary(lexdec$RT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.829   6.215   6.346   6.385   6.502   7.587",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#measures-of-dispersion",
    "href": "descriptive_stats.html#measures-of-dispersion",
    "title": "Descriptive Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nStandard deviation and variance\nAs described in the lecture the standard deviation of a dataset is calculated as follows:\n\nfor the full population: \\(\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\mu)^2}}{N}}\\)\nfor a sample of the population with the un-biased estimator: \\(s = \\sqrt{\\frac{\\sum_{i=1}^{N}{(X_i-\\overline{X})^2}}{N-1}}\\)\n\nIn R, the function sd() calculates the sample standard deviation. There is no built in function to calculate the full population \\(\\sigma\\) , but in general, for N &gt; 30 they are very close\n\nsd(lexdec$RT)\n\n[1] 0.2415091",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-summaries-with-tidyverse",
    "href": "descriptive_stats.html#data-summaries-with-tidyverse",
    "title": "Descriptive Statistics",
    "section": "Data summaries with tidyverse\n",
    "text": "Data summaries with tidyverse\n\nIn most cases, we have a most complex data structure where we want to calculate the summary statistics not only globally but in several cases of conditions.\nFor example, if we want to calculate the average and number of cases of the Reaction Time in the lexdec dataset for Male and Female and also between Native and non-native English speakers. We could perform it by individually filtering each group combination and calculating the mean and number of cases (length() provides the number of rows) as below:\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.4     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Male and Native English Speaker\nlexdec_M_english &lt;- lexdec %&gt;% filter( (Sex==\"M\") & (NativeLanguage==\"English\"))\nmean(lexdec_M_english$RT)\n\n[1] 6.361977\n\nlength(lexdec_M_english$RT)\n\n[1] 395\n\n\nAnd this should be repeated for each of the four groups (Male-English, Male-Other, Female-English, Female-Other)\nR has several ways to simplify these calculations. We will look at the approach with tidyverse packages by making use of the functions group_by() and summarize().\ngroup_by() allows to specify grouping variables that would be applied to the next operation on the pipe. The function summarize() allows to create summary variables with the statistics of choice, including mean, sd, IQR, median, min, max, etc‚Ä¶ (look at the online help).\nThe two functions can be combined as in the example below, where we group by Sex and NativeLanguage and ask to create three columns with names numObs , avgRT and sdRT containing the number of observations, mean Reaction Time and Standard Deviation Time respectively.\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 5\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 F     English           553  6.29 0.195\n2 F     Other             553  6.47 0.274\n3 M     English           395  6.36 0.195\n4 M     Other             158  6.50 0.224\n\n\nWhen reporting summary descriptive statistics, it is common practice to provide the Standard Error (SE) that is the error on the sampling of the mean, which is calculated as\n\\[\ns_{\\overline{X}} = \\frac{s}{\\sqrt{N}}\n\\]\nWe can extend the function above to add a column with the SE calculation, which can be done on the basis of the new columns sdRT and numObs:\n\nlexdec %&gt;% group_by(Sex, NativeLanguage) %&gt;% \n           summarize( numObs = n(), avgRT = mean(RT), sdRT = sd(RT), seRT = sdRT / sqrt(numObs))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 6\n# Groups:   Sex [2]\n  Sex   NativeLanguage numObs avgRT  sdRT    seRT\n  &lt;fct&gt; &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 F     English           553  6.29 0.195 0.00830\n2 F     Other             553  6.47 0.274 0.0117 \n3 M     English           395  6.36 0.195 0.00981\n4 M     Other             158  6.50 0.224 0.0178",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "href": "descriptive_stats.html#reporting-summary-descriptive-statistics",
    "title": "Descriptive Statistics",
    "section": "Reporting Summary Descriptive Statistics",
    "text": "Reporting Summary Descriptive Statistics\nFor the reporting of each group of data, either a tabular form or descriptive paragraph can be used, providing the values as calculated. An example for the lexdec example we used could be as follows:\n\n\n\n\n\n\nNote\n\n\n\n‚ÄúThe logarithmically transformed reaction time of Female English Native Speakers (M = 6.29, SE = 0.01, n = 553) was faster than Male English Native Speakers (M=6.36, SE = 0.01, n=395), which was faster than both Female (M=6.47, SE=0.01, n=553) and Male (M=6.50, SE=0.02, n=158) non-native Speakers.‚Äù\n\n\nNote that the mean is denoted by a capital M. In this case we don‚Äôt have yet a statistical analysis to determine if the difference observed is significant. That should be reported as well and we will address it in the next lectures.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "descriptive_stats.html#data-normality-checks",
    "href": "descriptive_stats.html#data-normality-checks",
    "title": "Descriptive Statistics",
    "section": "Data Normality checks",
    "text": "Data Normality checks\nWe mentioned in the lectures the importance to be able to check the normality of data or, as we will see of the residuals of a model fit, to ensure that our conclusions based on the statistical assessment of the results of a linear model are valid. In this section we briefly present how to perform in R some of the measures to determine deviation from normality.\nSkewness and Kurtosis\nSkewness and Kurtosis of a distribution can be computed using functions in the moments package. Before using it, we have to install the package\n\n#install.packages(\"moments\")\nlibrary(moments)\n\nWarning: package 'moments' was built under R version 4.3.3\n\n\nThe functions are easily called, kurtosis() and skewness() . Let‚Äôs see an example based on the lexdec dataset:\n\nskewness(lexdec$RT)\n\n[1] 0.9930124\n\n\nA positive number implies a right-tailed distribution. If we plot again the histogram for the RT data, this is clearly visible.\n\nlexdec %&gt;% ggplot(aes(x=RT)) + geom_histogram(bins = 70)\n\n\n\n\n\n\n\nIf now we calculate the kurtosis:\n\nkurtosis(lexdec$RT)\n\n[1] 4.405579\n\n\nA positive value indicates a distribution that is more concentrated in the center than a normal distribution.\nSo both measures point to a distribution deviating from normality on the data.\nQ-Q plots\nA Q-Q plot or ‚ÄúQuantile-Quantile‚Äù Plot, can be used to compare two distributions. It plots the quantiles from the measured data against what a theoretical or other distribution quantile would look like.\nA straight line implies that the two distributions compared are very similar. this is done in R using the qqplot() function. To test for normality, we want to compare the data against a thoretical normal distribution. This a particular case implemented with the function qqnorm() .\nIf we look in our running example\n\nqqnorm(lexdec$RT)\nqqline(lexdec$RT)\n\n\n\n\n\n\n\nqqline() adds a reference line for comparison. Clearly the data deviates from the line noting again the non-normality of the data.\nAn alternative to the above two commands is to use the qqPlot() function in the car package that produces charts with an error area more adequate for publication and reporting.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nqqPlot(lexdec$RT)\n\n\n\n\n\n\n\n[1] 1273  750\n\n\nShapiro-Wilk tests\nFinally, a number of tests can be used to provide a quantified assessment of the normality or not of the data.\nHere we present the Shapiro-Wilk test. This test considers a null hypothesis that the data is normally distributed. A significant outcome implies that the null hypothesis is not maintained and that the data is not normally distributed. It is implemented by the shapiro.test() function\n\nshapiro.test(lexdec$RT)\n\n\n    Shapiro-Wilk normality test\n\ndata:  lexdec$RT\nW = 0.94738, p-value &lt; 2.2e-16\n\n\nA p-value lower than a defined threshold implies that the null hypothesis is rejected. In this case, p&lt;0.001 so we will assess the data as not being normally distributed.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "assignment2_key.html",
    "href": "assignment2_key.html",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "",
    "text": "Task#1: Introduction to the dataset\nExercises in this assignment used a dataset with real data subset from a real Self-Paced Reading study on Negative Polarity Items and complementizer agreement in Basque (see Pablos, L., & Saddy, D. (2009). Negative polarity items and complementizer agreement in Basque. in Alter, K., Horne, M., Lindgren, M., Roll, M., & von Koss Torkildsen, J.(Eds.), Papers from Brain Talk: Discourse with and in the Brain. The 1st Birgit Rausing Language Program Conference in Linguistics. Lund: Lund University, Media Tryck. ISBN: 978-91-633-5561-5.)\nThis data is a real set with the outputs generated by the software Linger created by the MIT Ted Lab to easily conduct Self Paced reading experiments",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task1-introduction-to-the-dataset",
    "href": "assignment2_key.html#task1-introduction-to-the-dataset",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "",
    "text": "1.1 Load required libraries for the assignment\nThe packages required for this assignment are tidyverse and moments for the kurtosis and skewness calculations.\n1.2 Load dataset\nThe data for the exercise is contained in the file BasqueNPI.Rda in the /data folder.\nThis is a R format file, containing a data table. The data was saved using saveRDS() function so it can be read using readRDS() as described in the first workgroup section describing how to load a R format file.\n\n#Read dataset\ndfBasqueNPI &lt;- readRDS('./data/BasqueNPI.Rda')\n\n\n\n\n\n\n\nTip\n\n\n\nThe file location can be indicated with absolute or relative paths. The relative paths refers to the current working directory, that is indicated with ./ , therefore ./data/BasqueNPI.Rda can be read as: ‚Äúfile BasqueNPI.Rda in the data folder under the current directory‚Äù).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA few of you got an error when trying to open the file, saying that the file was corrupted when using the load() function in R.\nsaveRDS() and readRDS() allow to save a single R object to a file and to restore the object, . This differs from save() and load(), which save and restore one or more variables into the environment.\n\n\nAfter this step you have a dataframe in the environment, with the following variables\n\n\nEXPT - Specifies the type of data of the item collected, it contains three different values:\n\npractice: items presented at the beginning of the experiment to practice the method\nfiller: items included in the experiment to ‚Äòdisguise‚Äô the real measured target sentences.\nbasquenpi: target items corresponding to the experimental manipulation.\n\n\nItem ‚Äì Factor identifying the sentence used (coded as a number from 1 to 72).\nSubject ‚Äì Factor identifying the participant on the experiment (coded as a number from 1 to 32)\n\nEmbeddedSubject ‚Äì Factor/predictor indicating the nature of the embedded subject with the following levels:\n\n[Empty] ‚Äì for practice and filler items\nNP ‚Äì for target sentences with a Noun Phrase as subject\nNPI ‚Äì for target sentences with a Negative Polarity Item as subject\n\n\n\nAgreementMorphology ‚Äì Factor/predictor indicating the nature of agreement with the following levels:\n\n[Empty] ‚Äì for practice and filler items\nDeclarative ‚Äì for target sentences that contained a complementizer with declarative morphology\nPartitive ‚Äì for target sentences that contained a complementizer with partitive morphology\n\n\nSequenceBin ‚Äì the position of the item in the sequence seen by the subject.\nWordNumber ‚Äì the word number in the sentence (starting with 1)\nWord ‚Äì Actual word presented\nRegionNumber ‚Äì Region number for the analysis of the reading time\nRWRT ‚Äì RaW (recorded) Reading Time of the word\nRWZS ‚Äì Z-Score of the raw reading time\nRSRT ‚Äì Calculated Residual Reading Time\nRSZS ‚Äì Z-Score of the residual reading time\nQPCT ‚Äì Correctness of the comprehension response (100 ‚Äì correct; 0 ‚Äì incorrect)",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task2-explore-data-calculate-summary-statistics",
    "href": "assignment2_key.html#task2-explore-data-calculate-summary-statistics",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "Task#2: Explore Data & Calculate Summary Statistics",
    "text": "Task#2: Explore Data & Calculate Summary Statistics\n2.1 Select a subset of the data\nSelect a subset of data with only the target items from the experiment\n\n#Your code\nbasquenpi_data &lt;- dfBasqueNPI %&gt;% filter(EXPT==\"basquenpi\")\nhead(basquenpi_data)\n\n       EXPT Item Subject SequenceBin WordNumber       Word RegionNumber RWRT\n1 basquenpi    5       1          11          1      Kepak            1  554\n2 basquenpi    5       1          11          2   ziurtatu            2  702\n3 basquenpi    5       1          11          3         du            3  412\n4 basquenpi    5       1          11          4 ikastolara            4  552\n5 basquenpi    5       1          11          5       inor            5  393\n6 basquenpi    5       1          11          6  autobusez            6  642\n    RWZS    RSRT   RSZS QPCT EmbeddedSubject AgreementMorphology\n1 -0.611   14.48 -0.049  100             NPI           Partitive\n2 -0.025   -0.71  0.224  100             NPI           Partitive\n3 -0.636   35.67 -0.259  100             NPI           Partitive\n4 -0.687 -259.50 -0.624  100             NPI           Partitive\n5 -0.704  -92.13 -0.321  100             NPI           Partitive\n6 -0.301 -115.11 -0.243  100             NPI           Partitive\n\n\n2.2 Plot histograms of the data.\nPlot histogram of Raw Reading Time (RWRT)\nTo plot a histogram of the Raw Reading Time (RWRT) we use ggplot()\n\n#Plot histogram\nbasquenpi_data %&gt;% ggplot(aes(x = RWRT)) + geom_histogram()\n\n\n\n\n\n\n\nWhat can we observe in this distribution?\n\nIt is a disribution skewed to the right with a lower limit of 0 -&gt; Since RWRT measures the Raw Reading Time, the minimum reading time is zero, obviously, as it can not contain negatives values.\nThe graph extends above 10000. Although not visible, that implies that there are\n\nMost values seem to be between 0 and 2000: the default plot created by geom.histogram() uses 30 subdivisions or bins. To have a more fine grouping, we can indicate the number of bins used with the argument bins\n\nbasquenpi_data %&gt;% ggplot(aes(x = RWRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\n\n\nNow it can be clearly seen that the bigger proportion of reading times is between 500 and 1000 ms.\nThe distribution is very right skewed because there are probably a few measurements with very long reading time (e.g.¬†possibly a participant loosing attention and not pressing a key).\n\nWe can check the histogram only for values of the RWRT less than a certain limit.\nWe will normally perform a filtering of outliers based on the knowledge of our experiement and the intial data exploration. For example, looking at the plot above, we could eliminate those trials with RWRT &gt; 3000 ms considering that they are too long to be valid. Let‚Äôs plot a new histogram with only those trials with RWRT &lt; 3000.\n\nbasquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% ggplot(aes(x = RWRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\nNow we can see that the distribution is more symmetric, after removing outlayer points with RWRT &gt; 3000. If we want to see how many trials we have removed with the filtering, we can calculate the percentage of trial removed\n\n\\(Percentage\\,removed\\,trials = \\dfrac{n_{total}-n_{filtered}}{n_{total}}*100\\).\nThere are many ways to know the size of observations in a dataframe (i.e.¬†number of rows). The example below uses the function nrow() . We also demonstrate how you can write the results with a message on the screen using cat()\n\n# calculation\nn_total &lt;- basquenpi_data %&gt;% nrow()\nn_filtered &lt;- basquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% nrow()\n\nn_removed &lt;- (n_total - n_filtered)/n_total * 100\n\ncat(\"Total number of observations: \", n_total, \"\\n\")\n\nTotal number of observations:  7328 \n\ncat(\"Number of observations with RWRT &gt; 3000: \", (n_total - n_filtered) , \"\\n\")\n\nNumber of observations with RWRT &gt; 3000:  44 \n\ncat(\"Percentage of data removed: \", n_removed, \"%\",\"\\n\")\n\nPercentage of data removed:  0.6004367 % \n\n\nRemoving data points with RWRT &gt; 3000, only results in 0.6% of the data being discarded.\nPlot a histogram of the Residual Reading Time (RSRT)\n\nThe Residual Reading Time (RSRT) is a way to correct for sentence length, word length, and individual differences between participants‚Äô reading speeds. It is referenced to the average reading time per participant, and can have negative values (indicating faster reading than the average). We will look at the way it is calculated in an assignment once we have gone over the principles of regression.\nFor the purpose of this exercise we can plot the histogram to compare with the RWRT.\n\nbasquenpi_data %&gt;% ggplot(aes(x = RSRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\nProvide your observations on the distribution:\n\nThe distribution is roughly centered around 0, and it is more symmetric than the RWRT.\n\nLet‚Äôs plot the distribution of RSRT filtering the data as above based on the RWRT (RWRT&lt;3000).\n\nbasquenpi_data %&gt;% filter(RWRT&lt;3000) %&gt;% ggplot(aes(x = RSRT)) + geom_histogram(bins = 60)\n\n\n\n\n\n\n\n\n,We can see that the distribution is centered around 0 and still skewed to the right.\n2.3 Calculate Skewness and kurtosis for both RWRT and RSRT\nSkewness and kurtosis values using the functions with the same name in the library moments.\nCalculate Skewness for RWRT\n\nskewness(basquenpi_data$RWRT)  \n\n[1] 6.682073\n\n\nWhat does the output value indicate?\n\nA positive skewness value means that the distribution is right skewed. This is quite clear looking at the histogram above.\n\nCalculate Kurtosis for RWRT\n\nkurtosis(basquenpi_data$RWRT)\n\n[1] 96.25985\n\n\nWhat does the output value indicate?\n\nA positive kurtosis indicates a distribution more peaked than a normal distribution with the same mean and standard deviation.\n\nBelow we calculate the same values for the RSRT\n\nskewness(basquenpi_data$RSRT) \n\n[1] 7.348828\n\nkurtosis(basquenpi_data$RSRT)\n\n[1] 118.0003\n\n\n\nAs can be seen the RSRT is even more skewed and with higher kurtosis (more ‚Äòpointy‚Äô) than the RWRT distribution.\n\nFinally let‚Äôs look at the values on the dataset for values of RT &lt; 3000 ms\n\nbasquenpi_filtered&lt;-basquenpi_data %&gt;% filter(RWRT &lt; 3000)\n\nskewness(basquenpi_filtered$RWRT)\n\n[1] 2.377025\n\nkurtosis(basquenpi_filtered$RWRT)\n\n[1] 10.67131\n\nskewness(basquenpi_filtered$RSRT)\n\n[1] 2.226802\n\nkurtosis(basquenpi_filtered$RSRT)\n\n[1] 11.14173\n\n\nThe value show that the distribution is closer to a normal (not normal though) in both measures after the filtering as expected.\n2.4 Calculate mean and standard deviation\nCalculate the mean and standard deviation of the Raw Reading Time (RWRT) for the different experimental conditions in Region Number 5 to generate a summary table.\nThere are many ways to do this in R. Below we include one way using tidyverse summarize function:\n\nbasquenpi_data %&gt;% filter(RegionNumber==5) %&gt;% group_by(EmbeddedSubject,AgreementMorphology) %&gt;% summarize(m_rwrt = mean(RWRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 √ó 3\n# Groups:   EmbeddedSubject [2]\n  EmbeddedSubject AgreementMorphology m_rwrt\n  &lt;fct&gt;           &lt;fct&gt;                &lt;dbl&gt;\n1 NP              Declarative           751.\n2 NP              Partitive             751.\n3 NPI             Declarative           662.\n4 NPI             Partitive             587.\n\n\n\n\n\n\n\n\n\n\nMean\n\nEmbedded Subject\n\n\n\n\n\n\nNP\nNPI\n\n\nAgreement Morphology\nDeclarative\n750.70\n661.67\n\n\n\nPartitive\n751.33\n586.60\n\n\n\nA similar approach can be followed to calculate the Standard deviations table\n\nbasquenpi_data %&gt;% filter(RegionNumber==5) %&gt;% group_by(EmbeddedSubject,AgreementMorphology) %&gt;% summarize(sd_rwrt = sd(RWRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 √ó 3\n# Groups:   EmbeddedSubject [2]\n  EmbeddedSubject AgreementMorphology sd_rwrt\n  &lt;fct&gt;           &lt;fct&gt;                 &lt;dbl&gt;\n1 NP              Declarative            357.\n2 NP              Partitive              441.\n3 NPI             Declarative            808.\n4 NPI             Partitive              275.\n\n\n\n\n\n\n\n\n\n\nStd Dev\n\nEmbeddedSubject\n\n\n\n\n\n\nNP\nNPI\n\n\nAgreementMorphology\nDeclarative\n357.30\n807.98\n\n\n\nPartitive\n440.71\n274.91\n\n\n2.5 Calculate five-points summary\nCalculate and create a table with the five-point summary (i.e., min, max, median, 1st quartile(Q1) and 3rd quartile(Q3)) for the Residual Reading Time (RSRT) for the following subset of data:\n\nEmbeddedSubject = NP\nAgreementMorphology = Partitive\nRegion Number = 5\n\nWe can use the summary() function to calculate the 5-point summary.\n\nbasquenpi_data_subset &lt;- basquenpi_data %&gt;% filter( (RegionNumber == 5) & (AgreementMorphology==\"Partitive\") &(EmbeddedSubject == \"NP\")) \n\nsummary(basquenpi_data_subset$RSRT)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-495.320  -90.130    0.985   83.313  120.080 3089.090 \n\n\n\n\nMin\nMax\nmedian\n1st Quartile\n3rd Quartile\n\n\n-495.32\n3089.09\n0.985\n-90.13\n120.08\n\n2.6 Produce a boxplot for RWRT\n2.6.1. Create a boxplot for RSRT\nProduce a boxplot for the RWRT in different conditions reflected in the table of task 2.4: in Region 5, for the conditions Declarative NP, Declarative NPI, Partitive NP, Partitive NPI.\n\nbasquenpi_data %&gt;% filter(RegionNumber == 5) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RWRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\nWhat can you observe in the plot?:\n\nThere is a clear outlier in the NPI-Declarative condition.\nThe NP-Partitive condition seems to have more dispersion that the other ones.\n\nIf we plot again the same filtering the data to keep only the points with RWRT &lt; 2500 :\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RWRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\nWhat differences do you observe with the previous plot?\n\nNow it is clearer to appretiate what is the difference between the different conditions.\nThis plot allows a clear comparison between the different levels of AgreementMorphology, but what is we wanted to compare NPI vs NP within each level of AgreementMorphology? We can exchange the variables between x and the fill arguments.\n\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=AgreementMorphology, y= RWRT, fill = EmbeddedSubject)) + geom_boxplot()\n\n\n\n\n\n\n\n2.6.3. Create a boxplot for RSRT\n\nAs in the previous section, we create now the boxplot for RSRT as follows:\n\nbasquenpi_data %&gt;% filter(RegionNumber == 5) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RSRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\nAgain if we filter the data with a RWRT &lt; 2500\n\nbasquenpi_data %&gt;% filter( (RegionNumber == 5) & (RWRT &lt; 2500) ) %&gt;% \n            ggplot(aes(x=EmbeddedSubject, y= RSRT, fill = AgreementMorphology)) + geom_boxplot()\n\n\n\n\n\n\n\n2.6.4. Comment on the differences between the RWRT and RSRT plot.\n\nIn the RWRT plots it can be seen that NPI conditions hae lower dispersion than NP conditions (boxes are smaller)\nthe RSRT is centered around 0 ms, as expected, however there are still a very significant number of outliers, with residuals over 1000 ms. This requires assessment during data analysis. We will look at this in future assignments.\n2.7 Normality checks\nTo check the distribution of the data, we saw in the lecture that a Quantile-Quantile Plot (qqplot) can be used. The function qqplot() is generic and compares some data with any type of distribution. To compare your data with a normal distribution and check normality, you can use qnorm().\nWe generate the Quartile-Quartile plot for RWRT using the command below\n\nqqnorm(basquenpi_data$RWRT)\nqqline(basquenpi_data$RWRT, col = 'red',lty='dashed')\n\n\n\n\n\n\n\nThe plot deviates clearly from a straight line, as expected by the skewness and kurtosis values calculated in task 2.2.\n\n\n\n\n\n\nNote\n\n\n\nIn the code we have used qqline() to plot a reference straight line for visual comparison of the separation from the expected line in a normal distribution.\n\n\nFor comparison, let‚Äôs look at the q-q plot for the filtered data\n\nbasquenpi_data_filtered &lt;- basquenpi_data %&gt;% filter(RWRT&lt;2500)\nqqnorm(basquenpi_data_filtered$RWRT)\nqqline(basquenpi_data$RWRT, col = 'red',lty='dashed')\n\n\n\n\n\n\n\nAlthough the plot is more linear, still deviates from the normal distribution.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#task-3---calculate-data-summaries",
    "href": "assignment2_key.html#task-3---calculate-data-summaries",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "Task #3 - Calculate data summaries",
    "text": "Task #3 - Calculate data summaries\nThe assignment asked to generate a summary dataframe (basquenpi_data_summary ) using the filter(), group_by() and summarize()functions in dplyr with the data grouped by the factors RegionNumber, EmbeddedSubject and AgreementMorphology, and summarized via the addition of columns specifying the mean and standard deviations of the Raw Reading Time and Residual Reading Time.\n\n\n\n\n\n\nWarning\n\n\n\nSeveral of you pointed out that the filter() function was not required to be used in this task, which is correct starting from the data already filtered in task 2.1.\nIn the code below I assumed to perform a full processing chain from raw data to illustrate how using pipes can render readable code with all operations of the data together.\nOf course both approaches are correct.\n\n\nWe can generate the requested output with the following code line:\n\nbasquenpi_data_summary &lt;- dfBasqueNPI %&gt;% filter(EXPT == \"basquenpi\") %&gt;%   #filter the data\n              group_by(EmbeddedSubject,AgreementMorphology, RegionNumber) %&gt;% #apply a grouping\n              summarize(mean_rt = mean(RWRT), sd_rt=sd(RWRT), mean_rs = mean(RSRT), sd_rs= sd(RSRT))\n\n`summarise()` has grouped output by 'EmbeddedSubject', 'AgreementMorphology'.\nYou can override using the `.groups` argument.\n\nhead(basquenpi_data_summary,20)\n\n# A tibble: 20 √ó 7\n# Groups:   EmbeddedSubject, AgreementMorphology [2]\n   EmbeddedSubject AgreementMorphology RegionNumber mean_rt sd_rt mean_rs sd_rs\n   &lt;fct&gt;           &lt;fct&gt;                      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 NP              Declarative                    1    789.  436.   41.4   388.\n 2 NP              Declarative                    2    727.  395.  -36.3   360.\n 3 NP              Declarative                    3    611.  414.  104.    369.\n 4 NP              Declarative                    4    893.  492.   16.1   406.\n 5 NP              Declarative                    5    751.  357.   84.3   330.\n 6 NP              Declarative                    6    759.  352.  -28.2   312.\n 7 NP              Declarative                    7    627.  292.  -68.2   268.\n 8 NP              Declarative                    8    515.  186.  -82.3   177.\n 9 NP              Declarative                    9    681.  406.  -71.6   352.\n10 NP              Declarative                   10    700.  554.  -79.8   557.\n11 NP              Partitive                      1    745.  371.   -3.58  338.\n12 NP              Partitive                      2    713.  273.  -49.1   277.\n13 NP              Partitive                      3    573.  288.   65.7   260.\n14 NP              Partitive                      4    971.  716.  102.    664.\n15 NP              Partitive                      5    751.  441.   83.3   413.\n16 NP              Partitive                      6    786.  405.   -1.84  362.\n17 NP              Partitive                      7    621.  299.  -74.4   288.\n18 NP              Partitive                      8    613.  321.  -29.2   297.\n19 NP              Partitive                      9    852.  950.  112.    911.\n20 NP              Partitive                     10    865.  560.   56.2   562.",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment2_key.html#extra-credit---task4-plotting-with-ggplot2",
    "href": "assignment2_key.html#extra-credit---task4-plotting-with-ggplot2",
    "title": "Assignment #2 Data Exploration with R - Answer Key",
    "section": "EXTRA CREDIT - Task#4: Plotting with ggplot2",
    "text": "EXTRA CREDIT - Task#4: Plotting with ggplot2\nThe plot we want to create shows the mean RWRT for each region for the conditions where EmbeddedSubject==NP, for the two different levels of AgreementMorphology.\nWe present below the step-by-step code to build up the plot, and at the end we put it all together in a single code line.\nFrom the dataframe created in task 3.2 first we select the subset of cases where EmbeddedSubject==NP\n\nbasque_npi_summary_NP &lt;- basquenpi_data_summary %&gt;% filter(EmbeddedSubject==\"NP\")\n\nFirst step is to create a definition for a plot. With the command below, we create a plot based on the data in basque_npi_summary_NP and save the plot definition in a variable called p.¬†Typing the name of the variable shows the information from the plot as we build it.\n\np&lt;-ggplot(basque_npi_summary_NP) \np\n\n\n\n\n\n\n\nAt this point, nothing is shown, because we only specified that we will make a plot based on the data. The next step is to add the Aesthetics. This specifies what data the plot will use:\n\np&lt;-p+aes(x=RegionNumber, y=mean_rt) \np\n\n\n\n\n\n\n\nWith the command above, we informed that we will plot the mean_rt as a function of the RegionNumber. Still nothing is displayed, because we haven‚Äôt said yet how we plot it (lines, points, bars, etc). As you can see, we don‚Äôt need to provide in the commad the name of the variable containing the columns mean_rt and RegionNumber, because p knows which is the underlying data of the graph.\nWith the next command, we specify that we want to have a line using geom_line(), where each type of line corresponds one of the levels of AgreementMorphology\n\np+geom_line(aes(linetype=AgreementMorphology))\n\n\n\n\n\n\n\nNow, as you can see we get a graph!\nOn purpose, I did not saved the result back in the p variable, to illustrate a point. What if we wanted to have instead a barchart? We just call geom_bar() instead.\n\np+geom_bar(stat=\"identity\",aes(fill=AgreementMorphology),position=position_dodge() )\n\n\n\n\n\n\n\nComing back to our example, let‚Äôs add data points to the chart using geom_point(), where each point shape is also dependent on the level of Agreement Morphology.\n\np&lt;-p + geom_line(aes(linetype=AgreementMorphology))+\n       geom_point(aes(shape=AgreementMorphology)) \np\n\n\n\n\n\n\n\nWe have now all the information we wanted in the chart and stored in the variable p. For the axis, the RegionNumber only makes sense as a discrete value. The command below adds information on the x scale, defining it a continuous with values in a sequence from 1 to 10 in steps of 1.\n\np &lt;- p + scale_x_continuous(breaks = seq(1,10,1)) \np\n\n\n\n\n\n\n\nThe last step is to complete the decorations of the graph. We can do that using a number of functions from the ggplot2 library.\n\np + scale_color_grey() + theme_classic()\n\n\n\n\n\n\n\nThis is the plot requested to reproduce.\nIf we put the code together, we could do the following\n\np &lt;- basquenpi_data_summary %&gt;% filter(EmbeddedSubject==\"NP\") %&gt;%\n                                ggplot(aes(x = RegionNumber, y = mean_rt)) +\n                                geom_line(aes(linetype=AgreementMorphology)) + \n                                geom_point(aes(shape=AgreementMorphology)) +\n                                scale_x_continuous(breaks = seq(1,10,1)) +\n                                scale_color_grey() + \n                                theme_classic() \n\np\n\n\n\n\n\n\n\nSame plot as before.\n\n\n\n\n\n\nNote\n\n\n\nNote that in this exercise we maintain black and white (and grey) color with dashed and solid lines that is a format normally used for many publications.\n\n\n\nAs an illustration, let‚Äôs expand the example to include also error bars in the plot with the Standard Error of each point.\nAs a reminder, the standard error can be calculated as\n\\(SE = \\frac{\\sigma_{\\bar{X}}}{\\sqrt{n}}\\)\nFirst, let‚Äôs modify the summary variable to also calculate the SE in the variables se_rt and se_rs\n\nbasquenpi_data_summary_new &lt;- dfBasqueNPI %&gt;% filter(EXPT == \"basquenpi\") %&gt;%   #filter the data\n                            group_by(EmbeddedSubject,AgreementMorphology, RegionNumber) %&gt;%       \n                            summarize( mean_rt = mean(RWRT), \n                                       sd_rt=sd(RWRT), \n                                       se_rt = sd_rt/sqrt(n()), \n                                       mean_rs = mean(RSRT), \n                                       sd_rs= sd(RSRT), \n                                       se_rs = sd_rs/sqrt(n()))\n\n`summarise()` has grouped output by 'EmbeddedSubject', 'AgreementMorphology'.\nYou can override using the `.groups` argument.\n\n\nIn our plot, we want to display for every category an error bar. This can be done with the ggplot function geom_errorbar(). That function has two main arguments to specify the range of the error bar: ymin and ymax . The values of the error bars are \\(\\overline{X}\\pm SE\\)\n\np &lt;- basquenpi_data_summary_new %&gt;% filter(EmbeddedSubject==\"NP\") %&gt;%\n          ggplot(aes(x = RegionNumber, y = mean_rt)) +\n          geom_line(aes(linetype=AgreementMorphology)) + \n          geom_point(aes(shape=AgreementMorphology)) +\n          scale_x_continuous(breaks = seq(1,10,1)) +\n          scale_color_grey() + \n          theme_classic() +\n          geom_errorbar(aes(ymin = mean_rt-se_rt, ymax = mean_rt+se_rt) , width= 0.2,color = 'grey')\n\np\n\n\n\n\n\n\n\nThis is a more adequate plot to assess the data, as it shows that in some regions, the difference is within the error (e.g.¬†Region 6) and might not be significant. We will look in future workgroups at how to confirm this statistically with a linear model.\n\nEnd of the assignment #2",
    "crumbs": [
      "Workgroup 2: Data Exploration with R",
      "Assignment #2 Data Exploration with R - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup3.html",
    "href": "Workgroup3.html",
    "title": "Workgroup 3: Simple Linear Regression with R",
    "section": "",
    "text": "In this session we will start exploring the analysis of correlations between variables and applying the linear model concepts explained in the lecture with R.\nWe start by looking in a bit more detail into the visual exploration of relationships using scatter plots.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate and format scatter plots with ggplot.\nVisualize linear model fits with data.\nUnderstand concept, calculation and significance checking of Pearson‚Äôs Correlation Coefficient in R.\nUnderstand basic linear models formula nomenclature.\nFit linear model with a continuous or interval predictor using lm()\nFamiliarize with the output of the lm() function.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R"
    ]
  },
  {
    "objectID": "plot_xy.html",
    "href": "plot_xy.html",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Scatter plot with two variables\nIn Workgroup #2, we introduced plotting in R with ggplot2 package and practiced generating a plot in assignment #2.\nIn this section, we will expand with a few additional concepts useful to explore visually the relationship between two variables.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#scatter-plot-with-two-variables",
    "href": "plot_xy.html#scatter-plot-with-two-variables",
    "title": "Plotting relationship between two variables",
    "section": "",
    "text": "Basic scatter plot\nA¬†scatter plot¬†or XY plot displays the values of two variables along two axes, showing the relationship between them and possible correlations.\nScatter plots can be generated in ggplot using the geom_point() function.\nLet‚Äôs look again at the lexdec dataset in the languageR package as used Workgroup #2 and explore the relationship between the lexical decision latency reaction time (RT) and the word Frequency .\n\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.2     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.4     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point()\n\n\n\n\n\n\n\nA few things can be noticed on the plot above:\n\nThere is not a continuous range of values of Frequency in the dataset, as it can be seen by the gaps between data. This is common in our field of research, where we have limited examples of continuous predictors, and those available like the word Frequency have several data points measured on the same value.\nSeveral points are ‚Äòoverplotted‚Äô on the same area, thus not providing a clear view of around which values there are more data points.\nOverplotting\nThere are several ways to address overplotting and improve the visualization of the data.\n\nWe can change the shape of the data points from a solid circle to a hollow circle. The shape is controlled with the shape argument. The value for an empty circle is shape = 1 . Correspondence between values and shapes are in the reference manual of the function, although it is not immediate to find them. I leave here an image for reference:\n\n\n\nFrom: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(shape = 1)\n\n\n\n\n\n\n\nNow you can see there are areas with bigger overlap of points (more measurements) in the center band of the plot.\nIn case of a large dataset, it is better to use the alpha (transparency) argument. This can be specified as a ratio. For example, a value alpha = 1/5 is to be interpreted as ‚Äú5 points to be overplotted to get a full solid color‚Äù. That means that points are slightly transparent and overlapping them increases the shade.\nCompare the plots below with two different values\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/5)\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT)) + geom_point(alpha = 1/10)\n\n\n\n\n\n\n\nDiscrete variables\nThe most common case in linguistics research is the use of categorical predictors with a limited set of levels. A scatter plot with categorical data would look like the plot below, where we display the same Reaction Time data as a function of the word class.\n\nggplot(lexdec,  aes(x = Class, y = RT)) + geom_point(alpha=1/50)\n\n\n\n\n\n\n\nIn those cases with very limited number of categories, a boxplot or violin plot will be a better option to display the relationship.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#mapping-additional-variables-to-features",
    "href": "plot_xy.html#mapping-additional-variables-to-features",
    "title": "Plotting relationship between two variables",
    "section": "Mapping additional variables to features",
    "text": "Mapping additional variables to features\n\nA scatter plot shows the relationship between two variables in X and Y, but it can be used to reflect the relationship with more variables by mapping them to specific aesthetic features.\nFour commonly used:\n\nShape\nColor\nSize\nTransparency\n\nLet‚Äôs add in the plots above on the RT relationship with Frequency, a mapping to the word semantic category in the variable Class.\n\nggplot(lexdec,  aes(x = Frequency, y = RT,shape = Class)) + geom_point()\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class)) + geom_point()\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,size = Class)) + geom_point(alpha = 1/10)\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\nggplot(lexdec,  aes(x = Frequency, y = RT,alpha = Class)) + geom_point()\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\nWe can also add two mapping to the same variable. For example, if we want both different colors and different shape of the data points we can do:\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, shape=Class)) + geom_point()\n\n\n\n\n\n\n\nWe can also add another variable, for example NativeLanguage encoding the native on non-native nature of the speakers. Readability of the graph may be difficult though .\n\nggplot(lexdec,  aes(x = Frequency, y = RT,color = Class, alpha=NativeLanguage)) + geom_point() \n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\nAlways try different visualizations to find what is the best way to convey the message on your dataset.\nGood resources for inspiration and recommendations on data visualization types:\nR Graph Gallery: https://r-graph-gallery.com/\nFrom Data to Viz: https://www.data-to-viz.com/",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "plot_xy.html#plotting-linear-models",
    "href": "plot_xy.html#plotting-linear-models",
    "title": "Plotting relationship between two variables",
    "section": "Plotting linear models",
    "text": "Plotting linear models\nWe will look in next workgroups at different ways to represent the results of linear modeling, but we introduce here a useful feature of ggplot to plot fits to the data as another layer.\nThe function geom_smooth() adds a layer to a plot with aid the visualization of trends and relationships. It has two main arguments:\n\nmethod : function to use to calculate the ‚Äòsmoothed‚Äô version of the data. The value that is relevant to us is method = 'lm' , that would calculate a linear model fit and overplot it.\nformula: Formula of the linear fit model. The default value is formula = y ~ x . We will come back to this point in later lectures. For now we can use the default with no need to specify it.\n\n\nggplot(lexdec, aes(x = Frequency, y = RT)) + \n                geom_point(alpha = 1/5) +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWhat we can see now in the plot is a linear fit to the data, showing a negative correlation between Frequency and RT, with higher frequency words (more common) showing faster reaction times.\nAn interesting characteristic of this feature is that it applies the fit to the data as defined in ggplot aes(), so if we mapped another variable, like NativeLanguage to an aesthetic, we will get two fits, one per group:\n\nggplot(lexdec, aes(x = Frequency, y = RT, color = NativeLanguage))+\n                geom_point() +\n                geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThe plot above shows :\n\nIn general there is a trend that words are comprehended/processed faster the most frequent they are.\nThe strength of this effect is more pronounced in Non-native English speakers.\n\n\nNow that we are able to observe the relationship between two variables let‚Äôs move to quantifying it and assessing their statistical significance.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Plotting relationship between two variables"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Pearson‚Äôs correlation coefficient\nCorrelation is a measure of the dependence between two variables, and to which degree they are linearly related. It is not an indication of causality just of relationship.\nThe Pearson‚Äôs correlation coefficient, \\(r\\), is used to quantify the direction and magnitude of the association between two variables X, Y. It is calculated based in the sum of cross products of their values, normalized by their standard deviation:\n\\[\nr= \\frac{\\sum_{i=1}^N (x_i-\\bar{X})(y_i-\\bar{Y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\bar{X})^2}\\sqrt{\\sum_{i=1}^N (y_i-\\bar{Y})^2}}\n\\]\nThe coefficient takes values between -1 and 1:\nThe image below shows examples of correlation values and scatter plot of two variables in X and Y.\nIn R, we can calculate the Pearson correlation coefficient using the cor() function.\nContinuing with the previous example:\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\ncor(lexdec$Frequency, lexdec$RT, method = \"pearson\")\n\n[1] -0.2263358\nThe result, \\(r=-0.23\\), is negative, reflecting an inverse relationship (higher Frequency results in lower RT) and the level of correlation is moderate.\nA question however is how do we know that the calculated value is significant? A NHST test is performed following the approach defined in the lecture:\nIn the case our example, the number of data points, n, is \\(n=1659\\)\nnrow(lexdec)\n\n[1] 1659\nThe t-statistics will be :\n\\[ t= \\frac{r\\times \\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{-0.23 \\times \\sqrt{1659-2}}{\\sqrt{1-(-0.23)^2}} = -9.46\n\\]\nIf we select a confidence level of 5%, \\(\\alpha = 0.05\\), we can extract the value of \\(t_{critical}\\) considering the \\(df = 1659-2=1657\\). This can be calculated in R using the function qt() that returns a value of the Student t distribution. Note that since we are running a two-tailed test, we use 0.025 for p (half of 0.05).\nqt(p = 0.025, df = 1657)\n\n[1] -1.961397\nThe t-value is smaller than the critical value, so we reject the null hypothesis: there is a correlation between our variables.\nThe complete process above, can be performed in R using the function cor.test()\ncor.test(lexdec$Frequency,lexdec$RT,method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  lexdec$Frequency and lexdec$RT\nt = -9.4587, df = 1657, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2715046 -0.1801720\nsample estimates:\n       cor \n-0.2263358\nIf we look at the results provided by the function, the calculated t-value, df and correlation coefficient are the same. In addition, the function returns a p-value (\\(p&lt;0.001\\)) and a confidence interval for the correlation coefficient (\\(95CI_r = [-0.27,-0.18]\\) )",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "correlation.html#pearsons-correlation-coefficient",
    "href": "correlation.html#pearsons-correlation-coefficient",
    "title": "Correlation",
    "section": "",
    "text": "r=‚àí1 a perfect negative relationship: when one variable increases the other one decreases.\nr = 1, a perfect positive relationship.\nr=0, indicates no relationship at all between the two variables\n\n\n\n\nPearson correlation coefficient\n\n\n\n\n\n\n\n\n\nStep 1: Define statistical hypotheses\nOur null hypothesis is that the two variables are uncorrelated, so \\(r=0\\)\n\\[\nH_0: r=0 \\\\\nH_a: r\\neq 0\n\\]\n\n\nStep 2: Define sampling distribution\nFor the Pearson coefficient, \\(r\\), the sampling distribution is normal with a mean \\(\\mu_{r}=0\\) if \\(H_0\\) is true\n\n\nStep 3: Identify the test statistic\nThe test statistic is following a t-distribution with \\(df = n-2\\) and value\n\\[\nt=\\frac{r\\times\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\nNote: details beyond the scope of the course.\n\n\n\n\n\n\n\n\nStep 4: Determine critical value\n\n\n\n\n\nStep 5: reach statistical conclusion:\n\\[\nt_{value} = -9.62 \\lt t_{critical} = -1.96\n\\]",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Correlation"
    ]
  },
  {
    "objectID": "lm_intro.html",
    "href": "lm_intro.html",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "Formula notation in R\nAs explained in the lecture, a linear model of a dataset fits a line that minimizes the sum square errors between the predicted values and the data.\nThe generic function in R used to calculate a linear model fit based on data is lm() with the main arguments formula and data.\nA linear model with one predictor variable can be in general expressed as:\n\\[\nY_i = b_0 + b_1X_i+ \\epsilon_i\n\\]\nwhere \\(b_0\\) and \\(b_1\\) are referred to as the coefficients of the model and \\(\\epsilon_i\\) is the error term of the fit.\nIn our running example:\nWe would like to build a model as :\n\\[\nRT_i = b_0+b_1Frequency_i+\\epsilon_i\n\\]\nModels in R are specified using a notation in the form response~terms where response is the (numeric) dependent variable and terms are the linear predictors. A few notes:\nSo both formulas below are equivalent and can represent our model :\nRT ~ Frequency\nRT ~ 1 + Frequency\nThe formula can be extended to add more predictors (workgroup #4):\nRT ~ 1 + Frequency + NativeLanguage\nor to include interactions between predictors, using the operator : (workgroup #5):\nRT ~ 1 + Frequency:Length + NativeLanguage\nFor now, we will focus on a single predictor.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "lm_intro.html#formula-notation-in-r",
    "href": "lm_intro.html#formula-notation-in-r",
    "title": "Linear Models in R with lm()",
    "section": "",
    "text": "\\(b_0\\): intercept - represents the value of the outcome variable Y, when X=0.\n\\(b_1\\): slope - represents the change on the value of Y, due to a change of 1 unit of the value of X.\n\n\n\n\n\n\n\nNote\n\n\n\nNomenclature: Sometimes you will see a linear model with coefficients indicated with a ‚Äúhat‚Äù notation (\\(\\hat{}\\)). This is to reflect that the model we build is an estimation of the real model. So \\(b_0, b_1\\) would represent the population parameters while \\(\\hat{b}_0, \\hat{b}_1\\) would be the estimated parameters based on our sample.\nThis is equivalent to the difference between the population and sample means (\\(\\mu\\) and \\(\\bar{X}\\)) and standard deviation (\\(\\sigma\\) and \\(s\\)).\nIn the course for simplicity I will use the non-hat notation.\n\n\n\n\n\n\n\n\nFormulas have an implicit intercept, so it is not necessary to include it.\nError is considered by the function and calculated and it is also not included in the definition",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "lm_intro.html#interpreting-lm-output",
    "href": "lm_intro.html#interpreting-lm-output",
    "title": "Linear Models in R with lm()",
    "section": "Interpreting lm() output",
    "text": "Interpreting lm() output\n\nThe function lm() has a simple nomenclature and usage. The data it returns is nevertheless extensive and we will focus on the interpretation of the output.\nLet‚Äôs look at out example:\n\nlm(RT~1+Frequency, data=lexdec)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nCoefficients:\n(Intercept)    Frequency  \n    6.58878     -0.04287  \n\n\nRunning the lm() function returns a simple set of data, with the formula reflecting again and the value of the coefficients. This result means that our model would be written as\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\nThe formula however calculates more elements. Instead, it is normally executed saving the output in a new variable.\n\nmodel1 &lt;- lm(RT~1+Frequency, data = lexdec)\n\nnames(model1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\nAs we can see, the function returns a dataframe with many elements and variables. We will go through them at different points in the course.\nA simple way to display the results is to call the summary() function on the dataframe returned by lm() :\n\nsummary(model1)\n\n\nCall:\nlm(formula = RT ~ 1 + Frequency, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55407 -0.16153 -0.03494  0.11699  1.08768 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.588778   0.022296 295.515   &lt;2e-16 ***\nFrequency   -0.042872   0.004533  -9.459   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2353 on 1657 degrees of freedom\nMultiple R-squared:  0.05123,   Adjusted R-squared:  0.05066 \nF-statistic: 89.47 on 1 and 1657 DF,  p-value: &lt; 2.2e-16\n\n\nLet‚Äôs look at each of the output values individually:\nModel residuals\n\nThe first output of the function recalls the formula used for the fit and provides the 5-point summary of the model residuals.\n\n\n\n\nWe will see in the next lecture that one of the conditions for the validity of a linear model is the normality (normal distribution) of the model residuals, with a mean around 0. The 5-point summary provides an indication, nevertheless a check should be performed on the data. The residuals are included in the data structure as output of the model in a field called residuals . We can assess the normality of the residuals by for example using a Q-Q Plot as we described in workgroup #2.\n\nqqnorm(model1$residuals)\nqqline(model1$residuals)\n\n\n\n\n\n\n\nThe model residuals are still significantly deviating from normality. In Workgroup #4 we will explore in details the model assessment.\nModel coefficients\n\nThe second part of the table, provides the coefficients :\n\n\n\n\nThe table provides the two coefficients (\\(b_0\\) : intercept) and (\\(b_1\\): slope), and for each of them the following columns with the values and the results of a statistic test to check their significance.\n\nEstimate: Coefficient value.\nStd. Error: standard error of the coefficient estimate (\\(\\sigma_{b_i}\\))\n\nt-value: t-statistic calculated based on the value, to test the significance of the coefficient\n\\[\nt=\\frac{b_i}{\\sigma_{b_i}}\n\\]\n\n\n\\[\nH_0:b_i=0\n\\] \\[\nH_a:b_i \\ne 0\n\\]\n\n\n\\(Pr(&gt;|t|)\\) or p-value: probability of the t-value in case \\(H_0\\) is true.\n\nFrom the above, in our example we can see that both coefficients are significantly different from 0, so the model is confirmed to be:\n\\[\nRT_i = 6.59 -0.04\\times Frequency_i+\\epsilon_i\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nModel validity and Intercept interpretation:\nA model is always valid around the values used to generate them, and it is important to be careful to interpret the outcome.\nThe intercept value of 6.59 means that a word with a lemma log Frequency value of 0, is expected to have a logarithmic reaction time of 6.59 (in lexdec dataset both RT and Frequency are logarithmically transformed values) This might not make sense, as at words with extremely low frequency, the reaction time might be even longer.\nConsider the range of the data used to fit the model:\n\nmin(lexdec$Frequency)\n\n[1] 1.791759\n\nmax(lexdec$Frequency)\n\n[1] 7.77191\n\n\nWe should not use the model to predict values beyond this range.\n\n\nData fit quality\n\nThe statistical analysis shown before provides the significance assessment at individual coefficient level.\nThe last section of the output on the other hand, provides the statistical assessment of the model as a whole\n\n\n\n\nWe will describe these values in the next Lecture and Workgroup.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Linear Models in R with *lm()*"
    ]
  },
  {
    "objectID": "assignment3_key.html",
    "href": "assignment3_key.html",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "",
    "text": "Task #1: Load required libraries\nFor this assignment we use tidyverse\n#load libraries\nlibrary(tidyverse)",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-2-open-dataset",
    "href": "assignment3_key.html#task-2-open-dataset",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #2: Open dataset",
    "text": "Task #2: Open dataset\nFor this exercise we used a simulated dataset from a hypothetical experiment measuring the duration of vowels during production, generated following an example from Winter (2019).\nThe dataset is in the file ./data/extended_vowel_duration.csv and contains data from 30 participants, each recorded pronouncing 20 different words.\n\n#Open file\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nThe data contains the following columns:\n\nSubjectID: ID of the participant from 1 to 30\nWord: ID of the word from 1 to 20\nlogFreqs: logarithmic word frequency\nDuration: vowel utterance duration measurement in milliseconds.\n\nThe SubjectID and Word columns are numerical, but should be treated as factors. It would not affect this exercise, but it is good practice to define categorical factors when reading the data to avoid changing them inadvertently.\nConvert SubjectID and Word to factors\n\ndfVowel$SubjectID &lt;- as.factor(dfVowel$SubjectID)\ndfVowel$Word &lt;- as.factor(dfVowel$Word)",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-3-plot-dataset",
    "href": "assignment3_key.html#task-3-plot-dataset",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #3: Plot dataset",
    "text": "Task #3: Plot dataset\nYou were asked to create a scatter plot of the measured vowel duration as a function of word frequency, including a linear fit. As per the example in the workgroup, we use ggplot\n\ndfVowel %&gt;% ggplot(aes(x=logFreqs, y=Duration)) + \n       geom_point(alpha = 1/4) +\n       geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nObservations on the plot:\n\nThere is an apparent linear relationship between the duration of vowels in produced words and the frequency of the word, expressed in logarithmic scale.\nMore frequent words, have a shorter duration, so we would expect a negative Pearson‚Äôs correlation coefficient value.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-4-calculate-correlation-coefficient",
    "href": "assignment3_key.html#task-4-calculate-correlation-coefficient",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #4: Calculate correlation coefficient",
    "text": "Task #4: Calculate correlation coefficient\nCalculate Pearson‚Äôs correlation coefficient between logFreqs and Duration\n\nr = cor(dfVowel$logFreqs, dfVowel$Duration)\nprint(r)\n\n[1] -0.6745614\n\n\n\nAs expected the negative Pearson‚Äôs coefficent value \\(r = -0.67\\) indicates a relatively high inverse correlation between the word frequency and the produced vowel duration.\n\nTask #4.1 (EXTRA CREDIT): Assess significance of the Correlation Coefficient\nTo confirm the significance of the calculated correlation coefficient \\(r\\), we use the NHST methodology following the procedure practiced in Workgroup 3 and recalled here:\n\nManual calculation\n\n\nStep 1: Define statistical hypotheses\nOur null hypothesis is that the two variables are uncorrelated, so \\(r=0\\)\n\\[\nH_0: r=0 \\\\\nH_a: r\\neq 0\n\\]\n\n\nStep 2: Define sampling distribution\nFor the Pearson coefficient, \\(r\\), the sampling distribution is normal with a mean \\(\\mu_{r}=0\\) if \\(H_0\\) is true\n\n\nStep 3: Identify the test statistic\nThe test statistic is following a t-distribution with \\(df = n-2\\) and value\n\\[\nt=\\frac{r\\times\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\n\n\nIn the exercise, the number of data points, n, is \\(n=600\\)\n\nn = nrow(dfVowel)\n\nThe t-statistics will be :\n\n t_stat = (r * sqrt(n-2)) / sqrt(1-r^2)\n t_stat\n\n[1] -22.34534\n\n\n\\[ t= \\frac{r\\times \\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{-0.67 \\times \\sqrt{600-2}}{\\sqrt{1-(-0.67)^2}} = -22.35\n\\]\n\n\nStep 4: Determine critical value\n\nIf we select a confidence level of 5%, \\(\\alpha = 0.05\\), we can extract the value of \\(t_{critical}\\) considering the \\(df = 600-2=598\\). This can be calculated in R using the function qt() that returns a value of the Student t distribution.\n\n\n\n\n\n\nImportant\n\n\n\nNote that we use a two-tailed test, so for a confidence level \\(\\alpha = 0.05\\) we use 0.025 for p (half of 0.05). The reason to use a two-tailed test is that our hypothesis is that \\(r \\ne 0\\), not whether it is larger or smaller than a certain value.\n\n\n\nqt(p = 0.025, df = 598)\n\n[1] -1.963939\n\n\n\n\nStep 5: reach statistical conclusion:\n\\[\nt_{value} = -22.35 \\lt t_{critical} = -1.96\n\\]\n\n\n\n\n\n\nNote\n\n\n\nPlease note how the \\(t_{critical}\\) value is virtually the same as for a normal distribution for an \\(\\alpha\\) level of 0.05, since the number of samples is high (&gt;&gt; 50).\n\n\n\n\nWe can conclude that the correlation coefficient is significantly different from 0. In other words, there is a statistically significant relationship between the variables.\nUsing cor.test()\n\nThe same test of significance can be done using the function available in R:\n\ncor.test(dfVowel$logFreqs,dfVowel$Duration)\n\n\n    Pearson's product-moment correlation\n\ndata:  dfVowel$logFreqs and dfVowel$Duration\nt = -22.345, df = 598, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7159482 -0.6284500\nsample estimates:\n       cor \n-0.6745614 \n\n\nThe function provides the same results as in the manual calculation, with the addition of providing a 95% Confidence Interval for the coefficient.\n\n\n\n\n\n\nReporting correlation\n\n\n\nThe results could be reported following APA style as:\nThe produced vowel duration showed a negative relationship with the log transformed word frequency (\\(r(598) = -.67, p&lt;.01\\))",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "assignment3_key.html#task-5-build-a-linear-model-based-for-vowel-duration",
    "href": "assignment3_key.html#task-5-build-a-linear-model-based-for-vowel-duration",
    "title": "Assignment #3 Linear fitting with R - Answer Key",
    "section": "Task #5: Build a linear model based for vowel duration",
    "text": "Task #5: Build a linear model based for vowel duration\nTo generate a linear model for the vowel duration as a function of frequency we use the lm() function.\n\nmodel &lt;- lm(Duration~logFreqs, data = dfVowel)\nsummary(model)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nNote that the above produces the same output as if using the model with an explicit intercept:\n\nmodel &lt;- lm(Duration~ 1 + logFreqs, data = dfVowel)\nsummary(model)\n\n\nCall:\nlm(formula = Duration ~ 1 + logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nThe fitted model can be expressed as :\n\\[\nDuration_i = 307.51 -5.64 \\times logFreqs_i + \\epsilon_i\n\\]\nwhere the intercept coefficient is \\(b_0 = 307.51\\) and the slope is \\(b_1 = -5.64\\).\nThe output includes the t statistic and p-value of each of the coefficients, that indicate that they are different from 0.\nIn the next assignment we will explore the fit qualify measures and the check of the model validity.\n\nEnd of assignment 3\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 3: Simple Linear Regression with R",
      "Assignment #3 Linear fitting with R - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup4.html",
    "href": "Workgroup4.html",
    "title": "Workgroup 4: Simple Linear Regression II",
    "section": "",
    "text": "In this session we will continue to look at the interpretation of simple linear regression models outputs, including the quality of the data fit and the checking of model assumptions.\nWe also look at examples of models with a binary categorical predictor and the interpretation of its coefficients.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nUnderstand the quality of a model fit.\nCheck linear model assumptions.\nFit and interpret linear model with categorical predictors using lm()\nReport simple linear model results.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II"
    ]
  },
  {
    "objectID": "model_fit.html",
    "href": "model_fit.html",
    "title": "Linear Model output",
    "section": "",
    "text": "Model fit quality\nWe start from where we left in workgroup #3, looking at the last section of the output generated by the lm() function, that provides the statistical assessment of the model as a whole\nLet‚Äôs look at the different elements of the output\nWe know now how to create linear models fitting the data of interest, but how do we quantify how good is that fit, and if it adequately represents the data?",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_fit.html#sec-modelfit",
    "href": "model_fit.html#sec-modelfit",
    "title": "Linear Model output",
    "section": "",
    "text": "\\(R^2\\) coefficient\n\nAs seen in the lecture, the \\(R^2\\) value of a model represents the level of data variance explained by the model and can be expressed as:\n\\[R^2 = \\frac{SS_M}{SS_T}=\\frac{SS_T-SS_R}{SS_T} = 1-\\frac{SS_R}{SS_T}\\] where:\n\n\\(SS_T\\): Total sum of squares (deviation of data points from the data mean)\n\\(SS_M\\): Model sum of squares (deviation of regression line from the data mean)\n\\(SS_R\\): Residual sum of squares (deviation of data points from regression line)\n\nAs an exercise let‚Äôs calculate manually the \\(R^2\\) value of the linear model created in Assignment #3. We load the data again and fit the model for the vowel Duration as a function of logFreqs\n\nlibrary(tidyverse)\n\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nmod &lt;- lm(Duration~1+logFreqs, data=dfVowel)\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ 1 + logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nTo calculate \\(SS_T\\), we just add the squared difference between all of Duration points from their mean value:\n\nSS_t &lt;- sum( (dfVowel$Duration - mean(dfVowel$Duration))^2 )\nSS_t\n\n[1] 1994385\n\n\nFor \\(SS_R\\), the residuals from the model are available in the object returned by the lm() function:\n\nSS_r &lt;- sum (mod$residuals^2)\nSS_r\n\n[1] 1086874\n\n\nThe resulting value of R2 is then:\n\nR2 &lt;- (SS_t - SS_r)/SS_t\nR2\n\n[1] 0.455033\n\n\nThis is exactly the value returned in the summary(mod) output below and labelled Multiple R squared\nThe value indicates that the model explains 45.5% of the data variance.\n\n\n\n\n\n\nRelationship with Pearson‚Äôs correlation coefficient (r)\n\n\n\nNote that in the case of a single continuous variable, the \\(R^2\\) value corresponds to the square of the correlation coefficient r:\n\nr&lt;-cor(dfVowel$Duration,dfVowel$logFreqs)\nr^2\n\n[1] 0.455033\n\n\n\n\nAdjusted \\(R^2\\)\n\nIn the output of the model summary function there is another value labelled Adjusted R-squared\n\n\n\n\nThe \\(R^2_{Adjusted}\\) value is a correction applied to the case where more than one predictor variable is included in the model as we will see in the next lectures. It prevents the \\(R^2\\) value to increase with additional predictors while not improving the model fit. The adjustment is based on the number of data points (N) and the number of predictors (K):\n\\[\nR^2_{Adjusted} = 1-\\left( \\frac{SS_R}{SS_T}\\times \\frac{N-1}{N-K-1}\\right)\n\\]\nFor the cases we have seen so far with \\(K=1\\), the values are not too different.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_fit.html#model-f-statistic",
    "href": "model_fit.html#model-f-statistic",
    "title": "Linear Model output",
    "section": "Model F-statistic",
    "text": "Model F-statistic\nThe F-statistic displayed at the end of the output is from the so-called F-test for regression.\n\n\n\n\nIn essence, the test follows a NHST to identify if any regression slope coefficient (other than the intercept) is different from 0.\nFollowing our general NHST process:\n\n\n\n\n\n\nStep 1: Define statistical hypotheses\nThe null hypothesis here is that for a model, no slope coefficient is different from 0\n\\[\nH_0: b_1=b_2=b_3=...=0 \\\\\\\\\nH_a: b_k \\neq 0, \\text{for at least one value}\n\\]\nIn the case of single linear regression as we are looking at by now, this simplifies to\n\n\n\\[\nH_0: b_1=0 \\\\\\\\\nH_a: b_1\\neq 0\n\\]\n\nStep 2: Define sampling distribution: in an F-test, if \\(H_0\\) is true the ratio of explained and unexplained variance follow an F-distribution.\nStep 3: F-test statistic can be defined as the ratio of the mean squared errors\n\n\\[\nF = \\frac{MS_{mod}}{MS_{res}}\n\\]\nThe mean standard error are calculated from the Sum-squared errors and the degrees of freedom. The degrees of freedom of a model is the number of predictors (K), while for the residuals, the degrees of freedom depends on the number of data points (N-K-1).\n\\[\nMS_{mod} = \\frac{SS_{mod}}{df_{mod}} = \\frac{SS_{mod}}{K}\n\\]\n\\[\nMS_{res} = \\frac{SS_{res}}{df_{res}}=\\frac{SS_{res}}{N-K-1}\n\\]\nIn our example, \\(N=600, K=1\\), and \\(SS_{mod}\\) we can calculate:\n\nSS_mod &lt;- SS_t-SS_r\nMS_mod &lt;- SS_mod / 1\nMS_res &lt;- SS_r / (600 - 1 -1)\n\nF_val &lt;- MS_mod / MS_res\nF_val\n\n[1] 499.3142\n\n\nAs you can see, the value is the same as in the output from model fit in the figure above.\n\n\nStep 4: determine the critical value. Here the F-distribution in R can be calculated with the function qf() (similar to the function qt() that we used to determine the critical t-value in the Pearson‚Äôs coefficient significant testing). Three arguments are required: the probability (1- confidence level, so 1-0.05), and the two degrees of freedom (k, N-k-1), where k is the number of predictors and N the number of samples.\n\nF_critical&lt;-qf(1-0.05,1,600-1-1)\nF_critical\n\n[1] 3.857056\n\n\n\nStep 5: reach statistical conclusion\n\nThe calculated F-value is much higher than the threshold F_critical, therefore, the null hypothesis is rejected and the model is better than the model with only an intercept.\n\n\n\n\n\n\nImportant\n\n\n\nNote that the example above was intended for you to know where the numbers come from in the R output. You will not have to make the calculations step by step, as the lm() function already provides it in the output.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model output"
    ]
  },
  {
    "objectID": "model_comparison.html",
    "href": "model_comparison.html",
    "title": "Model selection",
    "section": "",
    "text": "Model comparison based on F-test\nUp to now we have created a model fitting data with one predictor, trying to assess whether it explains the data ‚Äúbetter‚Äù than a simpler model simply based on the mean of the data.\nWe can write the two models as:\nWe can compare which models best fits the data by comparing the amount of residual variance explained by one or the other, as measured by the \\(R^2\\) parameter. However, as explained in the previous section, the \\(R^2\\) value tends to improve as we add more parameters to the model, while their explanation value is not improved.\nWe will look in the following sections to three other methods: model comparison i) based on F-test, ii) based on Akaike‚Äôs Information Criterion (AIC) and iii) based on Bayesian Information Criterion (BIC).\nOne approach to check if adding elements to a model improve the model fit is to make a F-test of the difference in residual Sum of Squares.\nThis can be done using the anova() function.\nAs example, let‚Äôs again look at the data from Assignment 3 and create two models, one with only the intercept (null model), and one with our predictor (logFreqs)\nlibrary(tidyverse)\n\ndfVowel&lt;-read.csv('./data/extended_vowel_duration.csv')\n\nnull_model &lt;- lm(Duration~1,data=dfVowel)\nmod &lt;- lm(Duration~logFreqs, data = dfVowel)\nWe can compare if there is a significant difference between the two models residual sum of squares.\nanova(null_model, mod)\n\nAnalysis of Variance Table\n\nModel 1: Duration ~ 1\nModel 2: Duration ~ logFreqs\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    599 1994385                                  \n2    598 1086874  1    907511 499.31 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe output of the anova() function call, provides a F-test comparing the two models provided as input.\nWhat we can observed in this case is that the F test value is \\(F(1,598) = 499.31, p&lt;.01\\) indicating that there is a significant difference in the unexplained variance between both models. In other words, introducing the logFreqs predictor improved the model fit.\nAlthough this might is a trivial example with a single predictor, as we will see in the next lessons, model comparison will be extensively used when evaluating the explanatory value of more than one variable in Multiple Regression models.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#model-comparison-based-on-f-test",
    "href": "model_comparison.html#model-comparison-based-on-f-test",
    "title": "Model selection",
    "section": "",
    "text": "Note\n\n\n\nNote that in this case, with a single predictor, the value of this test is the same as the F-value provided by lm() when fitting a model to data and explained in the previous section:\n\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#akaike-information-criterion-aic",
    "href": "model_comparison.html#akaike-information-criterion-aic",
    "title": "Model selection",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\n\nThe model comparison based on F-test using anova() allows us to evaluate if adding a predictor term changes the model fit.\nA mathematical indicator that can be used to evaluate a model performance is the Akaike Information Criterion (AIC).\nDetails of the AIC mathematical basis are beyond the objectives of this course and are not required for its use in model selection, but the basic ideas are the following:\n\nAIC estimates the prediction error of a model\nA lower AIC indicates a model with less information loss, therefore a better fit to the data.\nAIC balances goodness of fit and complexity of the model, but risks overfitting.\n\nTo calculate the AIC value, we use the AIC() function (in capital letters). You can pass as arguments to the function more than one model, which will provide as output a table with the comparison of the values. In our running example:\n\nAIC(null_model,mod)\n\n           df      AIC\nnull_model  2 6572.076\nmod         3 6209.858\n\n\nAs you can see, the model with the logFreqs predictor has a lower AIC (6209) that the null model (6572) confirming it is better explaining the data.\nNote that the absolute value of AIC has no practical significance, but it is used always as a relative comparison between two or more models.\nCriteria: a change in AIC between models &gt;2 is considered significant (corresponds to ~35% probability that the lower AIC model improves the explanatory power of the model).",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_comparison.html#bayesian-information-criterion-bic",
    "href": "model_comparison.html#bayesian-information-criterion-bic",
    "title": "Model selection",
    "section": "Bayesian Information Criterion (BIC)",
    "text": "Bayesian Information Criterion (BIC)\n\nA similar indicator to AIC is the Bayesian Information Criterion (BIC). Again the details are beyond the scope of this course, but a few characteristics:\n\nBIC based on Bayesian framework\nLower BIC indicates a better model fit\nPenalized strongly complex models, so it is more adequate to select the simplest possible model explaining the data, avoiding overfitting.\n\nIt is implemented in R by the BIC() function:\n\nBIC(null_model,mod)\n\n           df      BIC\nnull_model  2 6580.870\nmod         3 6223.049\n\n\nAgain in this case, the model with logFreqs predictor has a lower BIC than the null model.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model selection"
    ]
  },
  {
    "objectID": "model_assumptions.html",
    "href": "model_assumptions.html",
    "title": "Model assumptions",
    "section": "",
    "text": "Linearity\nIn the last lecture, we discussed that linear regression has three main assumptions:\nLet‚Äôs look at how to check each of them.\nThe principle of linear models and correlation as explained in the course is based on the premises that there is a linear relationship between the predictor and outcome variables.\nTo a large extent this is checked visually. Let‚Äôs look at two examples:\nOn the running example of assignment 3\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\nIf we plot the linear model and the data:\ndfVowel %&gt;% ggplot(aes(x=logFreqs, y=Duration)) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nThe relationship appears to a large extent linear between the two variables.\nLet‚Äôs look in comparison to file ‚ÄúELP_full_length_frequency.csv‚Äù in the /data directory with data from a psycholinguistic study on the reading comprehension of words as a function of frequency part of the English Lexicon Project, described in the the Winter (2019) textbook Chapter 4.1.\nelp_data &lt;- read.csv(\"./data/ELP_full_length_frequency.csv\")\nelp_data %&gt;% ggplot(aes(x = Log10Freq, y = RT)) + geom_point(alpha=1/20) + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\nIn this dataset, particularly at larger values of Log10Freq the data appears to deviate from the best fit straight line, suggesting a deviation from linearity.\nWhen the data is not linear and fitted with a straight line as the one in green in the figure below, one observation that can be made is that the residuals will not be distributed equally around the fitted values, but in some areas will be larger than in others as in the figure in the right below.\nWe can plot the residuals against the fitted value, and if the resulting plot deviates from points centered around a flat line, the relationship between the variables is not linear.\nLet‚Äôs see an example based on the last dataset. We fit a model to the data and plot the residuals vs the fitted values. For this, we will use the function plot(). The function plot() when used on a model can generate 6 different diagnostic plots. We can select which one to plot using the which argument. which=1 plots the residuals against the fitted data.\nelp_model&lt;-lm(RT~Log10Freq, data=elp_data)\nplot(elp_model, which=1)\nAs you can see in the output, the data is curved and deviates from a flat line.\nIn case of the data not meeting the linearity assumption, data analysis requires the application of a data transformation or the use a general linear model. This is beyond the scope of this introductory course.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#normality-of-residuals",
    "href": "model_assumptions.html#normality-of-residuals",
    "title": "Model assumptions",
    "section": "Normality of residuals",
    "text": "Normality of residuals\n\nWe already looked at how to check normality of data sets in Workgroup 2. To check a model assumption, we apply the same approach for the model residuals.\nWe can use histograms, Q-Q plots and the Shapiro-Wilk test. Let‚Äôs look at our running example from assignment 3 with the mod model.\n\nsummary(mod)\n\n\nCall:\nlm(formula = Duration ~ logFreqs, data = dfVowel)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-122.464  -27.642    0.916   29.627  127.628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 307.5055     2.2525  136.52   &lt;2e-16 ***\nlogFreqs     -5.6397     0.2524  -22.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 42.63 on 598 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.4541 \nF-statistic: 499.3 on 1 and 598 DF,  p-value: &lt; 2.2e-16\n\n\nFirst we can plot a histogram of the residuals. I use first here the base R function hist() as ggplot() implementation requires a few tweaks 1 .\n\nhist(mod$residuals)\n\n\n\n\n\n\n\nAs can be seen, the residuals are apparently quite normally distributed.\nLet‚Äôs now use a Q-Q plot:\n\nqqnorm(mod$residuals)\nqqline(mod$residuals)\n\n\n\n\n\n\n\nThe model residuals are largely on the diagonal line indicating high degree of normality.\nYou can also use the function plot() on a model with the parameter which=2 to generate a QQ-plot. This version includes identification for the data points that are outliers with respect to the line:\n\nplot(mod,which=2)\n\n\n\n\n\n\n\nFinally, we can apply the Shapiro-Wilk test for normality that also confirms the observations.\n\nshapiro.test(mod$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mod$residuals\nW = 0.9986, p-value = 0.9221\n\n\nThe model normality of residuals assumptions are met in this example data.\nIn real world data, this is not always the case though. Looking instead to the ELP data, we see a clear deviation from the residuals‚Äô normality assumption.\n\nhist(elp_model$residuals)\n\n\n\n\n\n\nplot(elp_model, which = 2)\n\n\n\n\n\n\n\nIn this case the residuals distribution is skewed and non-normal as clearly shown by the Q-Q plot.\nIn those cases we will need to apply a data transformation or use a general linear model. This is beyond the scope of this introductory course.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#homogeneity-of-variance-of-residuals-homoscedasticity",
    "href": "model_assumptions.html#homogeneity-of-variance-of-residuals-homoscedasticity",
    "title": "Model assumptions",
    "section": "Homogeneity of variance of residuals (Homoscedasticity)\n",
    "text": "Homogeneity of variance of residuals (Homoscedasticity)\n\n\nFinal assumption to check is that the residuals have a homogeneous variance.\nThis can be done using the functions ncvTest() function in the car() package.\nncvTest() performs a Non-constant Variance Score Test. A significant test implies that the variance is not constant and the assumption is therefore not met. If we apply to the model for the data in Assignment 3, we see that the test is not significant (p = .42), implying the assumption is met.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nncvTest(mod)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.6488197, Df = 1, p = 0.42053\n\n\nIn comparison, the same test on the ELP data shows that the assumption is not met.\n\nncvTest(elp_model)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 2461.434, Df = 1, p = &lt; 2.22e-16\n\n\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_assumptions.html#footnotes",
    "href": "model_assumptions.html#footnotes",
    "title": "Model assumptions",
    "section": "",
    "text": "ggplot() requires as data input a dataframe or a variable that can be converted to a dataframe. The model generated by lm() is not suitable to directly pass into ggplot. If you want to plot the histogram of residuals with ggplot() you can use the fortify() function that creates a dataframe based on the model. In the created dataframe, the residuals are in a variable called .resid .\nggplot(data = fortify(mod), aes(x=.resid)) + geom_histogram()\n‚Ü©Ô∏é",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Model assumptions"
    ]
  },
  {
    "objectID": "model_categorical.html",
    "href": "model_categorical.html",
    "title": "Linear Model with Categorical predictors",
    "section": "",
    "text": "Categorical predictor with two levels\nUp to now we have been looking at model fits with continuous predictors. In a large majority of linguistics research however, we use predictors that are not continuous but rather categorical in nature.\nWe already showed an example in Workgroup 2 of plotting the relationship between a categorical predictor and the outcome variable.\nWhen we fit a model, R applies dummy coding to represent the levels of a categorical factor.\nIn case of a categorical predictor with two levels, R uses a single dummy variable with two values. In the example above:\nIf we fit a linear model, it will have the following expression:\n\\[\nRT_i = b_0 + b_1 \\times NativeLanguage_i\n\\]\nWhen NativeLanguage = 0 (English), the model becomes \\(RT = b_0+b_1\\times0 = b_0\\). This implies that the intercept (\\(b_0\\)) will represent the value for English.\nWhen NativeLanguage =1 (Other), the model becomes \\(RT = b_0+b_1\\times1=b_0+b_1\\). That means that the slope (\\(b_1\\)) represents the change for the Other category with respect to English.\nLet‚Äôs look at how this appears if we fit a model in R:\nm1&lt;-lm(RT~1+NativeLanguage, data = lexdec)\nsummary(m1)\n\n\nCall:\nlm(formula = RT ~ 1 + NativeLanguage, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56877 -0.15289 -0.03231  0.11480  1.11318 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.318309   0.007435  849.78   &lt;2e-16 ***\nNativeLanguageOther 0.155821   0.011358   13.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2289 on 1657 degrees of freedom\nMultiple R-squared:  0.102, Adjusted R-squared:  0.1015 \nF-statistic: 188.2 on 1 and 1657 DF,  p-value: &lt; 2.2e-16\nWhat you see on the output above is that we have the (Intercept) value, as for continuous predictors, but the parameter for the predictor is now called NativeLanguageOther. That is to indicate that the dummy variable created has a value 1 for the ‚ÄúOther‚Äù level.\nSo our model would be:\n\\[RT = 6.32+0.16\\times NativeLanguageOther\\] The fitted values for each of the levels of the variable would be:\n\\[\nRT_{English} = 6.32+0.16 \\times 0=6.32\n\\]\n\\[\nRT_{Other}=6.32+0.16 \\times 1= 6.48\n\\]",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_categorical.html#categorical-predictor-with-two-levels",
    "href": "model_categorical.html#categorical-predictor-with-two-levels",
    "title": "Linear Model with Categorical predictors",
    "section": "",
    "text": "NativeLanguage = 0, represents ‚ÄúEnglish‚Äù\nNativeLanguage = 1, represents ‚ÄúOther‚Äù",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_categorical.html#categorical-predictor-with-more-than-two-levels",
    "href": "model_categorical.html#categorical-predictor-with-more-than-two-levels",
    "title": "Linear Model with Categorical predictors",
    "section": "Categorical predictor with more than two levels",
    "text": "Categorical predictor with more than two levels\n\nWhen the categorical predictor has more than two levels, R creates a number of dummy variables to encode them. The number of variables used in the number of levels - 1.\nLet‚Äôs assume that NativeLanguage in the lexical decision experiment had three levels: English, Dutch, Other. R will use to Dummy Variables: NativeLanguageDutch and NativeLanguageOther .\n\n\n\nNativeLanguageDutch\nNativeLanguageOther\n\n\n\nEnglish\n0\n0\n\n\nDutch\n1\n0\n\n\nOther\n0\n1\n\n\n\nSo a model would look as follows:\n\\[\nRT = b_0 + b_1\\times NativeLanguageDutch + b_2\\times NativeLanguageOther\n\\]\nWe will treat these models with more than one predictor in the next lessons on Multiple Linear Regression.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Linear Model with Categorical predictors"
    ]
  },
  {
    "objectID": "model_reporting1.html",
    "href": "model_reporting1.html",
    "title": "Simple Linear Model reporting",
    "section": "",
    "text": "A report of the results of linear models should include the following elements:\nLet‚Äôs look at an example based again on the example on assignment 3:\nThe result contains all the information required, with the exception of the Confidence Interval for the logFreqs coefficient.\nThis can be conveniently extracted using the function confint() that takes a model as a parameter.\nThe reporting of the analysis could be written as follows:",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Simple Linear Model reporting"
    ]
  },
  {
    "objectID": "model_reporting1.html#footnotes",
    "href": "model_reporting1.html#footnotes",
    "title": "Simple Linear Model reporting",
    "section": "",
    "text": "We will discussed standardized coefficient values in the next workgroups on multiple regression.‚Ü©Ô∏é",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Simple Linear Model reporting"
    ]
  },
  {
    "objectID": "assignment4_key.html",
    "href": "assignment4_key.html",
    "title": "Assignment #4 - Linear fitting with R II - Answer Key",
    "section": "",
    "text": "Task#1 - Load and explore dataset\nFirst we load the relevant packages\nFor the analysis in this assignment we use again a subset of the real data from a Self-Paced Reading study on Negative Polarity Items and complementizer agreement in Basque (see Pablos, L., & Saddy, D. (2009)(Pablos and Saddy 2009). Negative polarity items and complementizer agreement in Basque. Brain Talk, 61.)\nLoad the data provided in the file BasqueNPISampleEx5.Rda :\nload('./data/BasqueNPISampleEx5.Rda')\nThe basquenpi_Ex5 data frame contains only the data from Region Number 8 (complementizer position), which was considered the ‚Äúcritical region‚Äù for analysis in the experiment.\nThe dataframe includes the following variables:",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Assignment #4 - Linear fitting with R II - Answer Key"
    ]
  },
  {
    "objectID": "assignment4_key.html#task1---load-and-explore-dataset",
    "href": "assignment4_key.html#task1---load-and-explore-dataset",
    "title": "Assignment #4 - Linear fitting with R II - Answer Key",
    "section": "",
    "text": "Caution\n\n\n\nIn this exercise, the data was inadvertently saved using the save() function. Using save() and load() saves and loads all the variables in the environment. In this case it will load in the environment a variable called basquenpi_Ex5\n\n\n\n\n\n\n\nItem ‚Äì Factor identifying the sentence used (coded as a number from 1 to 72).\n\nSubject ‚Äì Factor identifying the participant on the experiment (coded as a number from 1 to 32)\n\nEmbeddedSubject ‚Äì Factor/predictor indicating the nature of the embedded subject with the following levels:\n\nNP ‚Äì for target sentences with a Noun Phrase as subject\nNPI ‚Äì for target sentences with a Negative Polarity Item as subject\n\n\n\nAgreement Morphology ‚Äì Factor/predictor indicating the nature of agreement with the following levels:\n\nDeclarative ‚Äì for target sentences that contained a complementizer with declarative morphology\nPartitive ‚Äì for target sentences that contained a complementizer with partitive morphology\n\n\n\nCondition: Factor/predictor indicating the condition of the experiment with four levels (‚ÄúA‚Äù,‚ÄùB‚Äù,‚ÄùC‚Äù,‚ÄùD‚Äù). This factor was introduced for the exercises in this assignment. The conditions correspond to combinations of the two factors before:\n\nA: NPI ‚Äì Partitive\nB: NPI ‚Äì Declarative\nC: NP ‚Äì Partitive\nD: NP-Declarative\n\n\n\nWord ‚Äì Actual word presented\n\nRegionNumber ‚Äì Region number for the analysis of the reading time ( = 8 in this experiment)\n\nRWRT ‚Äì Raw (recorded) Reading Time of the word\n\nRWZS ‚Äì Z-Score of the raw reading time\n\nRSRT ‚Äì Calculated Residual Reading Time\n\nRSZS ‚Äì Z-Score of the residual reading time\n\nQPCT ‚Äì Measured response accuracy for each item with two possible values (100 or 0)",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Assignment #4 - Linear fitting with R II - Answer Key"
    ]
  },
  {
    "objectID": "assignment4_key.html#task2---linear-model-rwrt",
    "href": "assignment4_key.html#task2---linear-model-rwrt",
    "title": "Assignment #4 - Linear fitting with R II - Answer Key",
    "section": "Task#2 - Linear Model RWRT",
    "text": "Task#2 - Linear Model RWRT\n\n\n\nTasks# 2.1 Plot the relationship between the Raw Reading Time RWRT and the EmbeddedSubject predictor.\n\nThis can be done with ggplot() as described in the workgroup.\n\nbasquenpi_Ex5 %&gt;% ggplot(aes(x=EmbeddedSubject, y = RWRT)) + \n  geom_point() + geom_smooth(aes(group=1),method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nTask# 2.2 Create a linear model of RWRT with EmbeddedSubject as predictor\n\n\nm1&lt;-lm(RWRT~EmbeddedSubject, data=basquenpi_Ex5)\nsummary(m1)\n\n\nCall:\nlm(formula = RWRT ~ EmbeddedSubject, data = basquenpi_Ex5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-409.7 -170.4  -81.8   79.3 4318.3 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          564.28      16.29  34.640  &lt; 2e-16 ***\nEmbeddedSubjectNPI    68.38      23.04   2.968  0.00309 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 319.2 on 766 degrees of freedom\nMultiple R-squared:  0.01137,   Adjusted R-squared:  0.01008 \nF-statistic: 8.809 on 1 and 766 DF,  p-value: 0.003091\n\nconfint(m1)\n\n                       2.5 %   97.5 %\n(Intercept)        532.30033 596.2570\nEmbeddedSubjectNPI  23.15084 113.5992\n\n\n- Write down the fitted model, describing the coefficients and fit quality\nThe model that has been fit is:\n\\[\nRWRT_i = 564.28+68.38\\times EmbeddedSubject +\\epsilon_i\n\\]\n\nThe coefficient for EmbeddedSubject is significant, meaning that it is different from 0 (\\(b=68.38,95\\%CI[23.15,113.60], t(2.968), p=0.003\\)) and contributes to the model explanatory power.\nAlso the F-statistics shows that the model is significantly better than a model where the slop coefficient is zero ( \\(F(766,1)=8.81,p=0.03\\) ).\nNevertheless the quality of the fit shows that only ~1% of the residuals variance is explained by the model ( \\(R^2=0.011\\) ). This indicates that the current model does not have a high explanatory power.\n\n- Provide the predicted values from the model for the two levels of the EmbeddedSubject factor.\nEmbeddedSubject = NP (reference value 0): corresponds to the intercept: 564.28 ms\nEmbeddedSubject = NPI (value 1): corresponds to the \\(b_0+b_1=564.28+68.38 = 632.66 ms\\)\nTask# 2.3 Compare the model created in Task 2.2 with a null model with only an intercept , using anova(), AIC() and BIC()\nWe create a null model first\n\nm0&lt;-lm(RWRT~1, data=basquenpi_Ex5)\nsummary(m0)\n\n\nCall:\nlm(formula = RWRT ~ 1, data = basquenpi_Ex5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-375.5 -175.5  -86.5   74.5 4352.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   598.47      11.58   51.69   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 320.8 on 767 degrees of freedom\n\n\nLet‚Äôs compare now the models:\n\nanova(m0,m1)\n\nAnalysis of Variance Table\n\nModel 1: RWRT ~ 1\nModel 2: RWRT ~ EmbeddedSubject\n  Res.Df      RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    767 78952745                                \n2    766 78055118  1    897627 8.8089 0.003091 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBIC(m0,m1)\n\n   df      BIC\nm0  2 11055.94\nm1  3 11053.80\n\nAIC(m0,m1)\n\n   df      AIC\nm0  2 11046.65\nm1  3 11039.87\n\n\n- Describe your observations.\n\nLooking at the results of anova(m0,m1) : The result shows that there is a significance difference between the two models ( \\(F(766,1)=8.81,p=0.03\\) ). Note that this is the same result produced by summary(m1), which produces a test against the null models. we will see that the use of anova() is more relevant when comparing models with different predictors between them for model selection.\nUsing the AIC and BIC factors, we see that in both cases the values are lower for the m1 model, confirming that part of the residual variance can be explained by the EmbeddedSubject factor, although the difference is not too significant, particularly in BIC.\n\n\\[\nAIC_{m_0}=11046.65&gt; AIC_{m_1}=11039.87\\]\n\\[\nBIC_{m_0}=11055.94&gt;BIC_{m_1}=11053.80\n\\]\n\n\n\n\n\n\nTip\n\n\n\nCriteria for selection based on BIC:\nThere is not a universal criteria on how to select models based on BIC in terms of how big a difference between values matter. A commonly use criteria is the one proposed and justified in Raftery (1995) providing the strength of the evidence to select a model:\n\nBIC difference 0-2 : Weak evidence\nBIC difference 2-6: Positive evidence\nBIC difference 6-10: Strong evidence\nBIC difference &gt;10: Very strong evidence\n\n\n\nTask# 2.4 Check the linear model assumptions\nWe checked the assumptions two ways: looking at the residuals linearity and homoscedasticity\n\nplot(m1,which=2)\n\n\n\n\n\n\nncvTest(m1)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 35.30158, Df = 1, p = 2.824e-09\n\n\nAs can be seen from the assumptions checking, there is a significant deviation from the assumptions both of normality and uniformity in the variance of residuals. These strongly suggest that the model can not explain the data adequately and that there is a a likely need for data transformation, since the relationship is not linear with the predictor.\n\n\nTask# 2.5 Write a paragraph report of the analysis as per the guidelines and example and Workgroup 4\n\nReport:\nSimple linear regression was performed to assess the relationship between the Reading Time measured in milliseconds and the nature of the Embedded Subject (NPI or NP).\nThe fitted regression model was:\n\\[\nRWRT_i = 564.28+68.38\\times EmbeddedSubject +\\epsilon_i\n\\]\nIt was found that the nature of the Embedded Subject significantly affected the reading time (\\(\\beta = 68.38\\), \\(95\\%CI[23.15,113.60]\\), \\(t=2.97, p=0.003\\)). Nevertheless, the model explained only \\(1\\%\\) of the variance in the reading time ( \\(F(766,1)=8.81,p=0.03,R^2=0.011\\)) and the model assumption checking revealed significant deviations from both the residual normality and heteroscedasticity, so the analysis should be revised.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Assignment #4 - Linear fitting with R II - Answer Key"
    ]
  },
  {
    "objectID": "assignment4_key.html#task3-extra-credit-linear-model-for-rsrt",
    "href": "assignment4_key.html#task3-extra-credit-linear-model-for-rsrt",
    "title": "Assignment #4 - Linear fitting with R II - Answer Key",
    "section": "Task#3 (Extra credit): Linear model for RSRT",
    "text": "Task#3 (Extra credit): Linear model for RSRT\n\nTask# 3.1 Repeat all the steps in Task# 2 for the Residual Reading Time RSRT instead of RWRT\nWe start by creating the model for RSRT\n\nmRS0&lt;-lm(RSRT~1,data=basquenpi_Ex5)\nmRS1&lt;-lm(RSRT~EmbeddedSubject,data=basquenpi_Ex5)\nsummary(mRS1)\n\n\nCall:\nlm(formula = RSRT ~ EmbeddedSubject, data = basquenpi_Ex5)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-468.2 -147.6  -50.7   68.0 4370.3 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -55.74      15.65  -3.561 0.000392 ***\nEmbeddedSubjectNPI    68.38      22.13   3.089 0.002078 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 306.7 on 766 degrees of freedom\nMultiple R-squared:  0.01231,   Adjusted R-squared:  0.01102 \nF-statistic: 9.545 on 1 and 766 DF,  p-value: 0.002078\n\n\nThe fitted model is:\n\\[\nRSRT_i = -55.74+68.38\\times EmbeddedSubject +\\epsilon_i\n\\] The fit can be visualized as:\n\nbasquenpi_Ex5 %&gt;% ggplot(aes(x=EmbeddedSubject, y = RSRT)) + \n  geom_point() + geom_smooth(aes(group=1),method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nComparison with the null model:\n\nanova(mRS0,mRS1)\n\nAnalysis of Variance Table\n\nModel 1: RSRT ~ 1\nModel 2: RSRT ~ EmbeddedSubject\n  Res.Df      RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    767 72936170                                \n2    766 72038543  1    897627 9.5446 0.002078 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBIC(mRS0,mRS1)\n\n     df      BIC\nmRS0  2 10995.06\nmRS1  3 10992.19\n\nAIC(mRS0,mRS1)\n\n     df      AIC\nmRS0  2 10985.77\nmRS1  3 10978.26\n\n\nWith again similar results as in the RWRT.\nWe finally check the model assumptions:\n\nplot(mRS1,which=2)\n\n\n\n\n\n\nncvTest(mRS1)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 49.22344, Df = 1, p = 2.284e-12\n\n\nTask#3.2 Describe your observations on the main differences in the results of the analysis between Tasks 2 and 3.\n\nThe results are similar in both fits.\nIn particular the slope coefficient (effect of predictor RSRT ) is the same as in the case of the RWRT.\nConsidering that RSRT represents the corrected reading times accounting for the word duration, that correction did not improve the model fit neither improved the model validity as seen in the model assumptions checking.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Assignment #4 - Linear fitting with R II - Answer Key"
    ]
  },
  {
    "objectID": "assignment4_key.html#data-filtering-and-transformation",
    "href": "assignment4_key.html#data-filtering-and-transformation",
    "title": "Assignment #4 - Linear fitting with R II - Answer Key",
    "section": "Data filtering and transformation",
    "text": "Data filtering and transformation\n\nThe exercise above was intended to show a typical case with data from a real experiment, where we often encounter issues with the data failing to meet model assumptions. It is very important to check them to avoid making false claims or drawing incorrect conclusions.\nNow, how will we deal with this sort of problems? I show below a normal process for data analysis.\n\n\nData cleaning: It is common to clean the data before performing the final analysis. A typical approach would be to remove outliers at a certain threshold. For example, we could remove the points a few standard deviations away from the mean. Note doing that we are implicitly assuming normality of the data, so it is good to check how many data points would be removed in that case. In the dataframe, we have a conveniently calculated parameter RWZS that provides for each data point the z-score, i.e., the number of standard deviations a value is away from the mean.\n\n\n\n\n\n\nTip\n\n\n\nThis is automatically produced by the output of the Linger experimental software package, but you could easily reproduce it by a simple function call as mutate(RWZS = (RWRT-mean(RWRT))/sd(RWRT)).\n\n\n\n\nLet‚Äôs filter the data using a z-score limit of 3:\n\ndfFiltered &lt;- basquenpi_Ex5 %&gt;% filter(RWZS&lt;3 & RWZS&gt;-3)\n\nWe check of the amount of data filtered with this threshold:\n\n(nrow(basquenpi_Ex5)-nrow(dfFiltered))/nrow(basquenpi_Ex5)\n\n[1] 0.02213542\n\n\nWe see that we have removed only around 2.2% of the data.\n\n\nData transformation: with the filtering above, we still have a significantly skewed data, which in most cases results in non-normality of the residuals.\n\n\ndfFiltered %&gt;% ggplot(aes(x=RWRT)) +\n  geom_histogram(color=\"#e9ecef\", bins=40) +\n  theme_classic()\n\n\n\n\n\n\n\nIn these cases, we ‚Äòtransform‚Äô the data to try to normalize the distribution. Reading and reaction times are normally right skewed, and a typical transformation is to use an inverse transformation: \\(1/RT\\).\nLet‚Äôs see what is the distribution in our case of \\(1/RWRT\\).\n\ndfFiltered %&gt;% ggplot(aes(x=1/RWRT)) +\n  geom_histogram(color=\"#e9ecef\", bins=40) +\n  theme_classic()\n\n\n\n\n\n\n\nIt is more normal without the skew observed in the raw data.\nLet‚Äôs fit a model on the inverse of the reading time as filtered above:\n\nmTransformed&lt;-lm(1/RWRT~EmbeddedSubject,data=dfFiltered)\nsummary(mTransformed)\n\n\nCall:\nlm(formula = 1/RWRT ~ EmbeddedSubject, data = dfFiltered)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.392e-03 -4.527e-04 -4.350e-06  3.768e-04  2.604e-03 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.062e-03  3.134e-05  65.796  &lt; 2e-16 ***\nEmbeddedSubjectNPI -1.812e-04  4.428e-05  -4.092 4.75e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0006068 on 749 degrees of freedom\nMultiple R-squared:  0.02186,   Adjusted R-squared:  0.02056 \nF-statistic: 16.74 on 1 and 749 DF,  p-value: 4.749e-05\n\nconfint(mTransformed)\n\n                           2.5 %        97.5 %\n(Intercept)         0.0020001993  2.123229e-03\nEmbeddedSubjectNPI -0.0002681327 -9.425776e-05\n\n\nWe still see a significant effect of the EmbeddedSubject factor, with the model performing better than the null model (as per the F-statistic above).\n\ndfFiltered %&gt;% ggplot(aes(x=EmbeddedSubject, y = 1/RWRT)) + \n  geom_point() + geom_smooth(aes(group=1),method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nLet‚Äôs now check the model assumptions:\n\nplot(mTransformed,which=2)\n\n\n\n\n\n\nncvTest(mTransformed)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.002240039, Df = 1, p = 0.96225\n\n\nAs you can see, now the normality of residuals is maintained as well as the homogeneity of variance. We can use this model to make claims on our data and report it.\n\\[\n1/RWRT_i = (2.06\\times10^{-3})-(1.81\\times10^{-4}) \\times EmbeddedSubject +\\epsilon_i\n\\]\nThe interpretation of the coefficients when using a transformation can not be done directly. You can calculate the predicted value in each case and transform it back to report. For example the value when EmbeddedSubject=NP would be:\n\\(1/RWRT_{NP} = (2.06\\times10^{-3})-(1.81\\times10^{-4})\\times0=2.06\\times10^{-3}\\implies RWRT_{NP} = 1/(2.06\\times10^{-3})=485.44ms\\)\nNote that we resolved the issue of the model validity, but still, the model created only explains \\(~2\\%\\) of the variance in the data, which suggest that other explanatory predictors are likely to be needed, or that the random part of the data is dominant (no effect of the manipulation).\nA complete reporting for this analysis would be\n\nReport:\nSimple linear regression was performed to assess the relationship between the Reading Time measured in milliseconds and the nature of the Embedded Subject (NPI or NP).\nThe fitted regression model was:\n\\[\nRWRT_i = 564.28+68.38\\times EmbeddedSubject +\\epsilon_i\n\\]\nModel assumption checking revealed significant deviations from both the residual normality and heteroscedasticity, so an inverse transformation was applied to the data and a new regression analysis performed with the following resulting model fit:\n\\[\n1/RWRT_i = (2.06\\times10^{-3})-(1.81\\times10^{-4}) \\times EmbeddedSubject +\\epsilon_i\n\\]\nIt was found that the nature of the Embedded Subject significantly affected the reading time at the complementizer position ( \\(\\beta=-1.81\\times10^{-4},95\\%CI[-2.68\\times10^{-4},-9.42\\times10^{-5}],p&lt;.001\\)). Nevertheless, the model explained only \\(2.18\\%\\) of the variance in the reading time ( \\(F(749,1)=16.74,p&lt;.001,R^2=0.022\\)).\n\n\n\n\n\nPablos, Leticia, and Douglas Saddy. 2009. ‚ÄúNegative Polarity Items and Complementizer Agreement in Basque.‚Äù Brain Talk, 61.\n\n\nRaftery, Adrian E. 1995. ‚ÄúBayesian Model Selection in Social Research.‚Äù Sociological Methodology, 111‚Äì63.",
    "crumbs": [
      "Workgroup 4: Simple Linear Regression II",
      "Assignment #4 - Linear fitting with R II - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup5.html",
    "href": "Workgroup5.html",
    "title": "Workgroup 5: Multiple Regression with R",
    "section": "",
    "text": "In this session we will examine how to execute and interpret multiple linear regression models.\nAs described in the lecture, multiple linear regression allows us to create models to predict a continuous outcome using two or more predictors (categorical or continuous) or in the case of a single categorical predictor with more than two levels.\nWe will look at examples of models with a combination of categorical and continuous predictor and the interpretation of its coefficients in each of the cases.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nFit and interpret a multiple linear model with categorical and continuous predictors using lm() .\nUnderstand concept of main effects and marginal means.\nVisualize models with several predictors.\nPerform post-hoc tests to evaluate the effects.\nReport multiple linear model results.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R"
    ]
  },
  {
    "objectID": "model_multiple.html",
    "href": "model_multiple.html",
    "title": "Multiple Linear Regression with Categorical predictor with more than two levels",
    "section": "",
    "text": "Model fit and coefficient interpretation\nWe will use the following packages:\nAs described in the lecture, when R fits a model with a categorical variable with more than one level, it creates dummy variables to represent the different value combinations.\nLet‚Äôs look at an example based on the basquenpi_Ex5 dataframe used in Assignment#4, filtering outlier data points with RWRT&gt;2000 ms\nload('./data/BasqueNPISampleEx5.Rda')\ndfFiltered&lt;-basquenpi_Ex5 %&gt;% filter(RWRT&lt;2000)\nIn this case, we will use the Condition factor that encodes the different experimental manipulations with four levels: A, B, C, D\nstr(dfFiltered)\n\n'data.frame':   763 obs. of  16 variables:\n $ EXPT               : Factor w/ 1 level \"basquenpi\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Item               : Factor w/ 24 levels \"1\",\"2\",\"3\",\"4\",..: 5 6 12 2 20 13 18 11 8 23 ...\n $ Subject            : Factor w/ 32 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ SequenceBin        : Factor w/ 94 levels \"11\",\"12\",\"13\",..: 1 5 9 11 17 23 28 30 34 36 ...\n $ WordNumber         : int  8 8 8 8 8 8 8 8 8 8 ...\n $ Word               : chr  \"denik\" \"dela\" \"dela\" \"dela\" ...\n $ RegionNumber       : Factor w/ 1 level \"8\": 1 1 1 1 1 1 1 1 1 1 ...\n $ RWRT               : int  692 443 772 482 632 503 642 493 393 503 ...\n $ RWZS               : num  0.261 -0.435 1.381 -0.348 0.628 ...\n $ RSRT               : num  152.48 -42.13 286.87 -3.13 146.87 ...\n $ RSZS               : num  0.688 -0.19 2.088 -0.101 1.296 ...\n $ QPCT               : Factor w/ 2 levels \"0\",\"100\": 2 2 2 1 2 2 2 2 2 2 ...\n $ EmbeddedSubject    : Factor w/ 2 levels \"NP\",\"NPI\": 2 2 1 2 1 2 2 1 1 1 ...\n $ AgreementMorphology: Factor w/ 2 levels \"Declarative\",..: 2 1 1 1 1 2 1 2 1 2 ...\n $ Condition          : Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 2 4 2 4 1 2 3 4 3 ...\n $ Id                 : Factor w/ 768 levels \"8\",\"18\",\"27\",..: 1 2 3 4 5 6 7 8 9 10 ...\nLet‚Äôs visualize the dataset using ggplot and the geom_boxplot() function this time. We also filter outliers above 2500ms:\ndfFiltered %&gt;% ggplot(aes(x=Condition, y=RWRT)) + \n  geom_boxplot() +theme_classic()\nThe graph shows an apparent difference in the medians of the different conditions. To assess if this difference is significant, we fit a model for RWRT as a function of Condition:\nm1&lt;-lm(RWRT~1+Condition, data=dfFiltered)\nsummary(m1)\n\n\nCall:\nlm(formula = RWRT ~ 1 + Condition, data = dfFiltered)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-403.39 -153.39  -74.53   86.04 1295.30 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   626.39      17.80  35.191  &lt; 2e-16 ***\nConditionB    -29.69      25.27  -1.175    0.240    \nConditionC    -29.86      25.24  -1.183    0.237    \nConditionD   -111.24      25.17  -4.419 1.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 246.6 on 759 degrees of freedom\nMultiple R-squared:  0.0277,    Adjusted R-squared:  0.02386 \nF-statistic: 7.207 on 3 and 759 DF,  p-value: 8.973e-05\nWhat can be observed from the output?\n\\[\nRWRT = b_o+b_1 \\times ConditionB+b_2\\times ConditionC+b_3\\times ConditionD = 626.39-26.69\\times ConditionB-29.86\\times ConditionC-111.24\\times ConditionD\n\\]\nhis allows to encode the Condition levels as follows:\nSo what is the meaning for each of the coefficients?\nWhat is the meaning of the statistic and p-value?: As in the linear model, the t-statistic presented for each coefficient is:\n\\[\nt_{b_i} = \\frac{b_i}{SE_{b_i}}\n\\]\nAnd it tests the hypothesis \\(H_0: b_i=0\\). From the output, we see that only ConditionD has a significant test (p&lt;0.001), implying that only that coefficient is significantly different than 0.\nThat is clearly visible If we show the confident interval for all coefficients, where only ConditionD and the intercept show values not including 0.\nconfint(m1)\n\n                2.5 %    97.5 %\n(Intercept)  591.4479 661.33335\nConditionB   -79.3044  19.91998\nConditionC   -79.4054  19.68731\nConditionD  -160.6561 -61.82310\nSo based on the above output, we can conclude the following (pending model assumptions checking)\nAs in the case of categorical predictors with two levels, R considers the first level of the categorical value as the reference.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with Categorical predictor with more than two levels"
    ]
  },
  {
    "objectID": "model_multiple.html#model-fit-and-coefficient-interpretation",
    "href": "model_multiple.html#model-fit-and-coefficient-interpretation",
    "title": "Multiple Linear Regression with Categorical predictor with more than two levels",
    "section": "",
    "text": "Important\n\n\n\nFor the example we are ignoring by now the model assumptions. For the dataset in reality we saw in assignment 4 that an inverse data transformation (1/RWRT) is required, but to illustrate the interpretation of the coefficients we will run the example without transforming it.\n\n\n\n\n\nMost of the elements are the same as in the simple linear models we saw before, with the exception of additional coefficients. R has created a model with the following dummy variables:\n\n\n\n\n\n\nConditionB\nConditionC\nConditionD\n\n\n\nCondition = A\n0\n0\n0\n\n\nCondition = B\n1\n0\n0\n\n\nCondition = C\n0\n1\n0\n\n\nCondition = D\n0\n0\n1\n\n\n\n\n\n\\(b_0\\): the intercept, corresponds to the predicted value when Condition = A: 626.39 ms\n\\(b_1\\): corresponds to the change of Condition B with respect to the baseline condition (Condition A) when the other variables are 0, so the predicted value for Condition = B is: \\(626.39-29.69=596.7\\)\n\\(b_2\\): corresponds to the change of Condition C with respect to the baseline condition (Condition A) when the other variables are 0, so the predicted value for Condition = C is: \\(626.39-29.86=596.53\\)\n\\(b_3\\): corresponds to the change of Condition D with respect to the baseline condition (Condition A) when the other variables are 0, so the predicted value for Condition = D is: \\(626.39-111.24=515.15\\)\n\n\n\n\n\n\n\n\nThere is a relationship between RWRT and Condition.\nCondition = D is significantly different than the reference condition (Condition = A).\nConditions B and C do not significantly differ from the reference condition A.\nWe cannot say whether there is a difference between Conditions B, C and D.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with Categorical predictor with more than two levels"
    ]
  },
  {
    "objectID": "model_multiple.html#contrast-coding-in-r",
    "href": "model_multiple.html#contrast-coding-in-r",
    "title": "Multiple Linear Regression with Categorical predictor with more than two levels",
    "section": "Contrast coding in R",
    "text": "Contrast coding in R\n\nR uses treatment (dummy) coding by default:\n\nThe first level alphabetically is the reference.\nCoefficients for other levels represent differences from the reference.\n\nTo check the coding we can use the contrasts functions, that reproduces the table we showed above.\n\ncontrasts(dfFiltered$Condition)\n\n  B C D\nA 0 0 0\nB 1 0 0\nC 0 1 0\nD 0 0 1\n\n\nTo change the reference level we can use relevel . Let‚Äôs say that we want to make Condition = B our reference:\n\ndfFiltered2 &lt;- dfFiltered %&gt;% mutate(Condition = relevel(Condition, \"B\"))\ncontrasts(dfFiltered2$Condition)\n\n  A C D\nB 0 0 0\nA 1 0 0\nC 0 1 0\nD 0 0 1\n\n\nIf now we fit the same model on this dataset, the coefficients will be different and so will be the dummy variables encoded:\n\nm2&lt;-lm(RWRT~1+Condition, data=dfFiltered2)\nsummary(m2)\n\n\nCall:\nlm(formula = RWRT ~ 1 + Condition, data = dfFiltered2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-403.39 -153.39  -74.53   86.04 1295.30 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 596.6984    17.9405  33.260  &lt; 2e-16 ***\nConditionA   29.6922    25.2724   1.175  0.24041    \nConditionC   -0.1668    25.3383  -0.007  0.99475    \nConditionD  -81.5474    25.2724  -3.227  0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 246.6 on 759 degrees of freedom\nMultiple R-squared:  0.0277,    Adjusted R-squared:  0.02386 \nF-statistic: 7.207 on 3 and 759 DF,  p-value: 8.973e-05\n\n\nAs you can observe the coefficients values have changed, and also our interpretation. Not we can say that Condition = D is significantly different from Condition = B (reference now).\nThe above are all examples of dummy coding, but there are several contrasts schemes that can be used to yield relevant coefficients values with comparisons of interest. The approach is complex and requires to be careful for adequate interpretation, and it is beyond the scope of this course. If you want to explore further look at the following link:\nUCLA - Advance Research Computing: R Library Contrast Coding Systems for categorical variables\nInstead, to examine the difference between the different levels of a predictor , we will look at post-hoc tests based on marginal means.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with Categorical predictor with more than two levels"
    ]
  },
  {
    "objectID": "model_posthoc.html",
    "href": "model_posthoc.html",
    "title": "Post-hoc testing based on Estimated Marginal Means",
    "section": "",
    "text": "Calculate marginal means\nIn the previous example, our linear model RWRT ~ Condition provided us the intercept (average value of RWRT in conditionA), and the relative effect of conditions B, C and D with respect to A. We also computed based on the model the predicted value of the average on those conditions, but could no determine easily the comparison between all the conditions.\nTraditionally we refer to post-hoc testing as an statistical significance test to be performed to determine which specific levels have significant differences between their means. We can performed those tests based on the empirical data itself, or based on the predicted values from the model. Estimated marginal means (EMM) ((Searle, Speed, and Milliken 1980)) or least-squares means are derived by using a model to make predictions averaged over one or more of predictors. The main benefit of performing post-hoc tests based on EMMs is that the influence of other factors in the model is considered (accounting for covariates).\nLet‚Äôs use modelbased for our exercises here. First install the package if not installed yet.\nLoad the library modelbased:\nOn our example based on the filtered basquenpi_Ex5 data, we can calculate the marginal means for each level of the Condition factor using the function estimate_means()\nm1&lt;-lm(RWRT~1+Condition, data=dfFiltered)\nsummary(m1)\n\n\nCall:\nlm(formula = RWRT ~ 1 + Condition, data = dfFiltered)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-403.39 -153.39  -74.53   86.04 1295.30 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   626.39      17.80  35.191  &lt; 2e-16 ***\nConditionB    -29.69      25.27  -1.175    0.240    \nConditionC    -29.86      25.24  -1.183    0.237    \nConditionD   -111.24      25.17  -4.419 1.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 246.6 on 759 degrees of freedom\nMultiple R-squared:  0.0277,    Adjusted R-squared:  0.02386 \nF-statistic: 7.207 on 3 and 759 DF,  p-value: 8.973e-05\nWe can calculate the marginal means as:\nmeans&lt;-estimate_means(m1, by = \"Condition\")\nmeans\n\nEstimated Marginal Means\n\nCondition |   Mean |    SE |           95% CI | t(759)\n------------------------------------------------------\nA         | 626.39 | 17.80 | [591.45, 661.33] |  35.19\nB         | 596.70 | 17.94 | [561.48, 631.92] |  33.26\nC         | 596.53 | 17.89 | [561.41, 631.66] |  33.34\nD         | 515.15 | 17.80 | [480.21, 550.09] |  28.94\n\nVariable predicted: RWRT\nPredictors modulated: Condition\nAs you can see, the values of the means are the same as we calculated based on the formula and the table also reports the confidence intervals. This is the case in this example, where the marginal means and observed means will be the same in the absence of other predictors.\nThe library includes an implementation of graphical representation of the marginal means, that allows for a quick display using the plot() function on the object returned by estimate_means().\nplot(means) + theme_minimal()",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Post-hoc testing based on Estimated Marginal Means"
    ]
  },
  {
    "objectID": "model_posthoc.html#post-hoc-pairwise-comparisons",
    "href": "model_posthoc.html#post-hoc-pairwise-comparisons",
    "title": "Post-hoc testing based on Estimated Marginal Means",
    "section": "Post-hoc pairwise comparisons",
    "text": "Post-hoc pairwise comparisons\n\nWe can calculate the contrasts and significance between the different variables using the function estimate_contrasts() from the same package.\n\ncomparisons &lt;- estimate_contrasts(m1, contrast=\"Condition\")\ncomparisons\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |    SE |            95% CI |    t(759) |      p\n-----------------------------------------------------------------------------\nB      | A      |     -29.69 | 25.27 | [ -79.30,  19.92] |     -1.17 |  0.240\nC      | A      |     -29.86 | 25.24 | [ -79.41,  19.69] |     -1.18 |  0.237\nD      | A      |    -111.24 | 25.17 | [-160.66, -61.82] |     -4.42 | &lt; .001\nC      | B      |      -0.17 | 25.34 | [ -49.91,  49.57] | -6.58e-03 |  0.995\nD      | B      |     -81.55 | 25.27 | [-131.16, -31.94] |     -3.23 |  0.001\nD      | C      |     -81.38 | 25.24 | [-130.93, -31.83] |     -3.22 |  0.001\n\nVariable predicted: RWRT\nPredictors contrasted: Condition\np-values are uncorrected.\n\n\nNow we have a full pair-wise comparison of all conditions in the data. From the table above we can see that the following contrasts are significant: D vs A, D vs B, D vs C. So the Condition = D seems to be performing differently that the other three.\n\n\n\n\n\n\nWarning\n\n\n\nNote: The output above indicates on the bottom: ‚Äúp-values are uncorrected‚Äù.\nWhen making multiple comparisons, we increase the risk to introduce Type I errors. This can be prevented by adding a correction factor to account for the multiple comparisons. This is done using the ‚Äúp_adjust‚Äù argument.\n\n\nThere are several adjustments for multiple correction. Two of the more popular are Bonferroni and Tukey.\nThe Bonferroni correction changes the limit of significance based on the number of comparisons performed. If our significance \\(\\alpha=0.05\\), and we perform 6 comparisons, we would consider a value significant only if \\(p&lt;\\alpha_{bonferroni}\\text{, with } \\alpha_{bonferroni}=\\alpha / 6=0.05/6=0.008\\).\nEquivalent to the above is to calculate an ‚Äúadjusted‚Äù p-value, calculated by multiplying the uncorrected p-value by the number of comparisons. This is what the function does when specifying a correction:\n\ncomparisons &lt;- estimate_contrasts(m1, contrast=\"Condition\", p_adjust = \"bonferroni\",backend = \"emmeans\")\ncomparisons\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |           95% CI |    SE |   t(759) |      p\n---------------------------------------------------------------------------\nA      | B      |      29.69 | [-37.16,  96.54] | 25.27 |     1.17 | &gt; .999\nA      | C      |      29.86 | [-36.90,  96.62] | 25.24 |     1.18 | &gt; .999\nA      | D      |     111.24 | [ 44.65, 177.83] | 25.17 |     4.42 | &lt; .001\nB      | C      |       0.17 | [-66.86,  67.19] | 25.34 | 6.58e-03 | &gt; .999\nB      | D      |      81.55 | [ 14.70, 148.40] | 25.27 |     3.23 |  0.008\nC      | D      |      81.38 | [ 14.62, 148.14] | 25.24 |     3.22 |  0.008\n\nVariable predicted: RWRT\nPredictors contrasted: Condition\np-value adjustment method: Bonferroni\n\n\nAnother approach to correcting for multiple comparisons is a Tukey‚Äôs adjusted p-value (referred to as Tukey‚Äôs Honestly Significant Difference Test HSD, or ‚ÄòTukey test‚Äô). The idea is common to the Bonferroni t-tests but in this case it is done by using a modified t-test, with a t-distribution called the ‚Äòstudentised range distribution‚Äô.\n\ncomparisons &lt;- estimate_contrasts(m1, contrast=\"Condition\", p_adjust = \"tukey\",backend = \"emmeans\")\ncomparisons\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |           95% CI |    SE |   t(759) |      p\n---------------------------------------------------------------------------\nA      | B      |      29.69 | [-35.38,  94.76] | 25.27 |     1.17 |  0.643\nA      | C      |      29.86 | [-35.12,  94.84] | 25.24 |     1.18 |  0.638\nA      | D      |     111.24 | [ 46.43, 176.05] | 25.17 |     4.42 | &lt; .001\nB      | C      |       0.17 | [-65.07,  65.41] | 25.34 | 6.58e-03 | &gt; .999\nB      | D      |      81.55 | [ 16.48, 146.62] | 25.27 |     3.23 |  0.007\nC      | D      |      81.38 | [ 16.40, 146.36] | 25.24 |     3.22 |  0.007\n\nVariable predicted: RWRT\nPredictors contrasted: Condition\np-value adjustment method: Tukey",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Post-hoc testing based on Estimated Marginal Means"
    ]
  },
  {
    "objectID": "model_posthoc.html#reporting-post-hoc-results",
    "href": "model_posthoc.html#reporting-post-hoc-results",
    "title": "Post-hoc testing based on Estimated Marginal Means",
    "section": "Reporting post-hoc results",
    "text": "Reporting post-hoc results\n\nWhen reporting a post-hoc result include the following elements:\n\nReport that you used Estimated Marginal Means (EMM) as post-hoc test.\nDescribe if the EMM was adjusted for a covariate (we will look at this point later when more than one predictor is used).\nIf reporting a comparison, include the test statistic (t), degrees of freedom and p-value.\nIf reporting a table of the EMMs, include the adjusted means, the SE or Confidence Interval.\nIn comparisons specify the correction applied to account for multiple comparisons.\n\nExample reporting of the analysis performed:\n\n‚ÄúA linear regression analysis was conducted to examine the effect of Condition (&lt;&lt;here you will explain the nature of your experimental manipulation&gt;&gt;) on the observed Raw Reading Time. Results indicated a significant effect for Condition, \\(F(3,759)=7.207, p&lt;.001\\).\nTo examine the nature of the effect, estimated means were calculated. The results show that that the estimated reading time was different across conditions:\n\nCondition A (\\(M=626.29ms,SE=17.80, 95\\%CI[591.45,661.33]\\))\nCondition B (\\(M=596.70ms,SE=17.94, 95\\%CI[561.48,631.92]\\))\nCondition C (\\(M=596.53ms,SE=17.89, 95\\%CI[561.41,631.66]\\))\nCondition D (\\(M=515.15ms,SE=17.80, 95\\%CI[480.21,550.09]\\))\n\nPost-hoc comparisons using Bonferroni correction showed that the estimated reading time for Condition D was significantly lower than Condition A (\\(t(759)=-4.42, p&lt;.001)\\), Condition B (\\(t(759)=-3.23,p=.001\\)) and Condition C (\\(t(759)=-3.22,p=.001\\)) . The larger difference was between conditions A and D (\\(111.24ms,95\\%CI[44.65,177.83]\\)).\n\n\n\n\n\n\n\nCaution\n\n\n\nAs mentioned above, the reporting should also include a check of the model assumptions to ensure the validity of the conclusions.\n\n\n\n\n\n\nLenth, Russell V., and Julia Piaskowski. 2025. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://rvlenth.github.io/emmeans/.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M. Wiernik, Etienne Bacher, R√©mi Th√©riault, and Dominique Makowski. 2022. ‚ÄúEasystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.‚Äù CRAN. https://doi.org/10.32614/CRAN.package.easystats.\n\n\nMakowski, Dominique, Mattan S. Ben-Shachar, Brenton M. Wiernik, Indrajeet Patil, R√©mi Th√©riault, and Daniel L√ºdecke. 2025. ‚Äúmodelbased: An R Package to Make the Most Out of Your Statistical Models Through Marginal Means, Marginal Effects, and Model Predictions.‚Äù Journal of Open Source Software 10 (109): 7969. https://doi.org/10.21105/joss.07969.\n\n\nSearle, S. R., F. M. Speed, and G. A. Milliken. 1980. ‚ÄúPopulation Marginal Means in the Linear Model: An Alternative to Least Squares Means.‚Äù The American Statistician 34 (4): 216‚Äì21. https://doi.org/10.1080/00031305.1980.10483031.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Post-hoc testing based on Estimated Marginal Means"
    ]
  },
  {
    "objectID": "model_multiple2.html",
    "href": "model_multiple2.html",
    "title": "Multiple Linear Regression with several predictors",
    "section": "",
    "text": "Multiple Linear Regression with continuous predictors\nWe look in the next sections at examples and outputs of different combinations of models with more than one predictor, still without considering interactions between them (interactions we will cover in Lecture 6 and Workgroup 6).\nTo illustrate the examples, let‚Äôs use the data file from Winter (2019) ‚ÄúELP_full_length_frequency.csv‚Äù\ndfELP&lt;-read.csv('./data/ELP_full_length_frequency.csv')\nhead(dfELP)\n\n    Word Log10Freq length     RT\n1 zenith  1.342423      5 753.69\n2 zephyr  1.698970      4 874.58\n3 zeroed  1.301030      5 929.23\n4  zeros  1.698970      5 625.09\n5   zest  1.544068      4 658.82\n6 zigzag  1.361728      6 785.26\nWe want to check if the Reading Time (RT) can be predicted based on the word length and word frequency, so we fit a model with two predictors:\nm_ex1&lt;-lm(RT~1+Log10Freq+length,data=dfELP)\nsummary(m_ex1)\n\n\nCall:\nlm(formula = RT ~ 1 + Log10Freq + length, data = dfELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-410.24  -60.52   -9.71   48.01  705.53 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 748.4261     2.1761  343.92   &lt;2e-16 ***\nLog10Freq   -68.0225     0.5939 -114.54   &lt;2e-16 ***\nlength       19.4583     0.2377   81.86   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.09 on 33072 degrees of freedom\nMultiple R-squared:  0.4873,    Adjusted R-squared:  0.4873 \nF-statistic: 1.572e+04 on 2 and 33072 DF,  p-value: &lt; 2.2e-16\nThe fitted model is:\n\\[\nRT =748.43-68.02\\times Log10Freq+19.46\\times length\n\\]\nLet‚Äôs check what every coefficient means:\nWe can use the model to predict values for a particular set of values. E.g., if we had a word of length = 4, with a log Frequency value of Log10Freq=1.6, the predicted RT would be:\n\\[RT=748.43-68.02\\times1.6+19.46\\times 4=717.44\\]\nThe function predict can be used to predict values based on a model:\nnew &lt;- data.frame(Log10Freq=c(1.6),length=c(4))\npredict(m_ex1,new)\n\n       1 \n717.4233\nWhich provides the same value1.\nThe model statistics are as per previous examples, where \\(R^2_{adj} = 0.487\\), indicates the model explains ~48% of the variance in the data, and the statistic \\(F(2,33072)=15720,p&lt;.001\\) indicates that the model is significant compared with a null model with intercept only.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with several predictors"
    ]
  },
  {
    "objectID": "model_multiple2.html#multiple-linear-regression-with-continuous-predictors",
    "href": "model_multiple2.html#multiple-linear-regression-with-continuous-predictors",
    "title": "Multiple Linear Regression with several predictors",
    "section": "",
    "text": "\\(b_0\\) - intercept: corresponds to the reading time when considering a word of 0 length and 0 frequency. As discussed in previous workgroups, the intercept in this cases does not have a real meaning as we can‚Äôt use the model to interpolate beyond the input data range. The intercept is in this case a mathematical construct of the fitted line.\n\\(b_1\\) - slope coefficient for Log10Freq. Represents the change in RT for a change of one unit in Log10Freq at a constant value of length.\n\\(b_2\\) - slope coefficient for length. Represents the change in RT for a change of one unit in word length at a constant value of Log10Freq.\n\n\n\n\n\n\n\nModel visualization with two continuous predictors\nA model as the one build without any interaction, is to be interpreted as follows:\n\n\nGiven a certain value of one of the continuous predictors, we will have a curve predicting the change with the other predictor. E.g. if we consider words of lengths 2, 5 and 10, the model specifies the following fits:\n\\[\nRT_{length=2}=748.43-68.02\\times Log10Freq+19.46\\times2=787.35-68.02\\times Log10Freq\n\\]\n\n\n\\[\nRT_{length=5}=748.43-68.02\\times Log10Freq+19.46\\times5=845.73-68.02\\times Log10Freq\n\\]\n\\[\nRT_{length=10} = 748.43-68.02\\times Log10Freq+19.46\\times10 =943.02-68.02\\times Log10Freq\n\\]\nAll the models above are lines with the same negative slope and with different intercepts.\nThis can not be easily visualized using ggplot() , but there is a function in the package ggiraphExtra developed to do that. To try it, install the package first as:\ninstall.packages(\"ggiraphExtra\")\n\n\n\n\n\n\nNote\n\n\n\nYou may have to also have to update a related library using:\ninstall.packages(\"htmltools\")\nIf asked to restart the R session say ‚ÄúYes‚Äù\n\n\nThen use the ggPredict function to plot the model with two variables.\n\nlibrary(ggiraphExtra)\n\nggPredict(m_ex1,terms=c(\"length\",\"Log10Freq\"))\n\n\n\n\n\n\n\nAs you can see, the plot represents the model at a number of values of length.\nModel Selection\nThe coefficients statistiscs shows that both coefficients are significantly different from 0, indicating that the RT depends on both logarithmically transformed frequency and length.\nLet‚Äôs compare the model with simple linear regression models with only one of the predictors:\n\nm_1&lt;-lm(RT~1+Log10Freq, data=dfELP)\nm_2&lt;-lm(RT~1+length,data=dfELP)\nanova(m_1,m_ex1)\n\nAnalysis of Variance Table\n\nModel 1: RT ~ 1 + Log10Freq\nModel 2: RT ~ 1 + Log10Freq + length\n  Res.Df       RSS Df Sum of Sq      F    Pr(&gt;F)    \n1  33073 315655743                                  \n2  33072 262475389  1  53180354 6700.7 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m_2,m_ex1)\n\nAnalysis of Variance Table\n\nModel 1: RT ~ 1 + length\nModel 2: RT ~ 1 + Log10Freq + length\n  Res.Df       RSS Df Sum of Sq     F    Pr(&gt;F)    \n1  33073 366604248                                 \n2  33072 262475389  1 104128859 13120 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAIC(m_1,m_2,m_ex1)\n\n      df      AIC\nm_1    3 396955.8\nm_2    3 401904.9\nm_ex1  4 390855.7\n\nBIC(m_1,m_2,m_ex1)\n\n      df      BIC\nm_1    3 396981.1\nm_2    3 401930.1\nm_ex1  4 390889.3\n\n\nAs we can see from the anova(), BIC() and AIC() results, the model with both predictors is preferable compared with with the simple linear models.\nEffect size and standardized coefficients\nOne issue with the fitting of model with several predictors on different scales, is that is not possible to compare the relative effect of each of them. What affects more the Reading Time? the word frequency or the length? Frequency and length are variables in different scales and cannot be directly compared.\nTo do that, we can use the concept of standardized coefficients. These coefficients are calculated by fitting a model to the standardized transformed predictors. For each predictor we calculate the z-score, by removing the mean and dividing by the standard deviation of the predictor. We do the same for the dependent variable.\nThis can be done by the scale() function. If we do a fit as follows:\n\nm_std&lt;-lm(scale(RT)~scale(Log10Freq)+scale(length),data=dfELP)\nsummary(m_std)\n\n\nCall:\nlm(formula = scale(RT) ~ scale(Log10Freq) + scale(length), data = dfELP)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2974 -0.4864 -0.0781  0.3859  5.6709 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -3.730e-14  3.937e-03    0.00        1    \nscale(Log10Freq) -4.873e-01  4.254e-03 -114.54   &lt;2e-16 ***\nscale(length)     3.482e-01  4.254e-03   81.86   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7161 on 33072 degrees of freedom\nMultiple R-squared:  0.4873,    Adjusted R-squared:  0.4873 \nF-statistic: 1.572e+04 on 2 and 33072 DF,  p-value: &lt; 2.2e-16\n\nconfint(m_std)\n\n                        2.5 %       97.5 %\n(Intercept)      -0.007717201  0.007717201\nscale(Log10Freq) -0.495642949 -0.478965763\nscale(length)     0.339910785  0.356587971\n\n\nAs you can see, the F-statistic is the same, but now the coefficients have a different interpretation:\n\n\\(\\beta_0\\) - The intercept is 0, as the output has been scaled to have zero mean.\n\\(\\beta_1\\) - The slope for Log10Freq is -0.487, which indicates that a change in 1-standard deviation of Log10Freq results in a negative change of 0.487 standard deviations in RT, for a fixed length.\n\\(\\beta_2\\) - The slope for length is 0.348, which indicates that a change in 1-standard deviation of length results in a positive change of 0.348 standard deviations in RT.\n\nNow we can directly compare as they are all expressed in standard deviations. The effect size of Log10Freq is stronger than that of length, since it results in a larger change in RT.\nStandardized coefficients are also called beta coefficients and make sense with continuous variables, where a change of 1-standard deviation has a meaning. This is obviously not the case with categorical predictors.\n\n\n\n\n\n\nNote\n\n\n\nReporting of standardized coefficients\nWhen calculating standardized coefficients report is as per previous examples, but use the notation.\nEx: Reading time exhibited a significant relationship with both the logarithmically transformed Frequency (\\(\\beta=-0.49,t(33072)=-114.54,95\\%CI[-0.496,-0.479], p&lt;.001\\)) and word length (\\(\\beta=0.35,t(33072)=81.86,95\\%CI[0.340,0.357], p&lt;.001\\)).",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with several predictors"
    ]
  },
  {
    "objectID": "model_multiple2.html#multiple-linear-regression-with-continuous-and-categorical-predictors",
    "href": "model_multiple2.html#multiple-linear-regression-with-continuous-and-categorical-predictors",
    "title": "Multiple Linear Regression with several predictors",
    "section": "Multiple Linear Regression with continuous and categorical predictors",
    "text": "Multiple Linear Regression with continuous and categorical predictors\n\nAll the principles discussed in the sections below apply in the case of a continuous and a categorical predictor.\nIn case of a categorical predictor, often the concept of discriminator predictor is used.\nLet‚Äôs look at an example with the lexdec dataframe in the languageR package and fit a model on the Reading Time as a function of Frequency and NativeLanguage\n\nlibrary(languageR)\n\nWarning: package 'languageR' was built under R version 4.3.3\n\ndata(\"lexdec\")\n\nm_ex2&lt;-lm(RT~Frequency+NativeLanguage, data=lexdec)\nsummary(m_ex2)\n\n\nCall:\nlm(formula = RT ~ Frequency + NativeLanguage, data = lexdec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.64311 -0.14750 -0.03046  0.11347  1.05578 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          6.521998   0.021594  302.03   &lt;2e-16 ***\nFrequency           -0.042872   0.004283  -10.01   &lt;2e-16 ***\nNativeLanguageOther  0.155821   0.011032   14.12   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2224 on 1656 degrees of freedom\nMultiple R-squared:  0.1532,    Adjusted R-squared:  0.1522 \nF-statistic: 149.8 on 2 and 1656 DF,  p-value: &lt; 2.2e-16\n\nconfint(m_ex2)\n\n                         2.5 %      97.5 %\n(Intercept)          6.4796443  6.56435160\nFrequency           -0.0512729 -0.03447073\nNativeLanguageOther  0.1341827  0.17745967\n\n\nThe fitted model is:\n\\[ RT =6.52-0.043\\times Frequency+0.156\\times NativeLanguageOther\\]\nLet‚Äôs check what every coefficient means:\n\n\\(b_0\\) - intercept: corresponds to the reading time when considering an English native of 0 frequency.\n\\(b_1\\) - slope coefficient for Frequency. Represents the change in RT for a change of one unit in Log10Freq for an English native.\n\\(b_2\\) - slope coefficient for length. Represents the change in RT for non-English native languages at a constant value of Frequency.\n\nThe interpretation is easier if we plot it again using ggPredict\n\nggPredict(m_ex2,terms=c(\"Frequency\",\"NativeLanguage\"))\n\n\n\n\n\n\n\nWhat you can see from the output is that there are two curves, one for each level of the NativeLanguage factor, so we can consider that the model represents two curves with same slope and two different intercepts:\n\\[ RT_{English} =6.52-0.043\\times Frequency+0.156\\times 0 = 6.52-0.043\\times Frequency\\]\n\\[ RT_{Other} =6.52-0.043\\times Frequency+0.156\\times 1 = 6.36-0.043\\times Frequency\\]\nAll other concepts from previous examples apply.\nAnd that is all for this workgroup. Next weeks we will look at the last topic of the course adding interactions between predictors to our models.\n\n\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with several predictors"
    ]
  },
  {
    "objectID": "model_multiple2.html#footnotes",
    "href": "model_multiple2.html#footnotes",
    "title": "Multiple Linear Regression with several predictors",
    "section": "",
    "text": "actually not exactly the same, but very close. This is due to rounding in the model parameters above‚Ü©Ô∏é",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Multiple Linear Regression with several predictors"
    ]
  },
  {
    "objectID": "assignment5_key.html",
    "href": "assignment5_key.html",
    "title": "Assignment #5 - An EEG Processing Pipeline - Answer Key",
    "section": "",
    "text": "Task #1: Read dataframe\nIn this assignment we intended to emulate a full EEG data processing pipeline using the concepts introduced across the different workgroups.\nWe load the required packages\nFor the assignment we use a large file containing real EEG data from 17 subjects collected across 4 experimental conditions.\nThe data is contained in the file /data/eegSampleData.Rda .\nLoad the data into the environment using readRDS()\n# read data\neegData&lt;-readRDS('./data/eegSampleData.Rda')\nThe data file contains the following columns:",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Assignment #5 - An EEG Processing Pipeline - Answer Key"
    ]
  },
  {
    "objectID": "assignment5_key.html#task-1-read-dataframe",
    "href": "assignment5_key.html#task-1-read-dataframe",
    "title": "Assignment #5 - An EEG Processing Pipeline - Answer Key",
    "section": "",
    "text": "time: numerical time vector in seconds of the collected data with respect to the visual trigger, ranging from -0.2 (200ms before the trigger) to 0.8 (800 ms after the trigger)\nSubject: Factor with 17 levels from ‚Äú1‚Äù to ‚Äú17‚Äù indicating the participant\nCondition: categorical factor with 4 levels: ‚ÄúCondA‚Äù, ‚ÄúCondB‚Äù, ‚ÄúCondC‚Äù and ‚ÄúCondD‚Äù\nFp1 to CB2: 62 columns with voltage values measured at each electrode on the EEG collection.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the dataset uploaded in the Posit Cloud, Condition was coded as ‚Äú11‚Äù, ‚Äú22‚Äù,‚Äú23‚Äù,‚Äú24‚Äù instead of ‚ÄúCondA‚Äù, ‚ÄúCondB‚Äù, ‚ÄúCondC‚Äù, ‚ÄúCondD‚Äù.\nIf you want to replicate this assignment key, run the following line to recode the levels before continuing.\nlevels(eegData$Condition)&lt;-c(\"CondA\",\"CondB\",\"CondC\",\"CondD\")",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Assignment #5 - An EEG Processing Pipeline - Answer Key"
    ]
  },
  {
    "objectID": "assignment5_key.html#task2-summarize-and-reorganize-the-data",
    "href": "assignment5_key.html#task2-summarize-and-reorganize-the-data",
    "title": "Assignment #5 - An EEG Processing Pipeline - Answer Key",
    "section": "Task#2: Summarize and reorganize the data",
    "text": "Task#2: Summarize and reorganize the data\nWe want to Evaluate the presence or absence of a N400 Event Related Potential (ERP) on the dataset. The N400 component appears in the range of 250 to 550ms after the stimulus onset, and it is considered a broad central-anterior component.\nIn order to do that we have to perform the following:\n\nCalculate the average voltage in the time window 250-550ms for all Electrodes.\n\nSelect the relevant Regions of Interest:\n\nAnterior: electrodes F1, F2, F3, F4, F5, F6, Fz\nCentral: electrodes C1, C2, C3, C4, C5, C6, Cz\n\n\nCompare the average voltage across experimental conditions against the Control Condition, which in this case is Condition D\n\nTask #2.1: Subset data\nSelect subset of the data for the analysis:\n\nTime in the range 0.250 to 0.550 s\nElectrodes for the Anterior and Central regions defined above.\n\n\neegDataFiltered &lt;- eegData %&gt;% \n  select(time,Subject,Condition,F1,F1,F3,F4,F5,F6,Fz,C1,C2,C3,C4,C5,C6,Cz) %&gt;%\n  filter(time &gt;=0.250 & time &lt;=0.550)\n\n# Remove from the memory the full data set now that we have filtered the data\nrm(eegData)\n\nTask #2.2: Rearrange the data from wide to long format\nWe convert the data from wide to long format using the steps practiced in Workgroup 2: https://lpablosrobles.github.io/Fundamentals-Linear-Models-workbook/data_organization.html\n\neegDataFiltered_long&lt;-pivot_longer(eegDataFiltered, \n                                   cols = F1:Cz, \n                                   names_to = \"Electrode\", \n                                   values_to = \"Voltage\")\nhead(eegDataFiltered_long)\n\n# A tibble: 6 √ó 5\n   time Subject Condition Electrode Voltage\n  &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 0.250 1       CondA     F1          -2.93\n2 0.250 1       CondA     F3          -2.90\n3 0.250 1       CondA     F4          -2.56\n4 0.250 1       CondA     F5          -2.33\n5 0.250 1       CondA     F6          -2.64\n6 0.250 1       CondA     Fz          -2.54\n\n\nTask #2.3: Calculate data averages in the N400 window\nCalculate average of data in the N400 window by averaging all points from 250 to 550 ms (already selected before).\nWe calculate the average in the window for each condition per subject\n\neegDataAveraged &lt;- eegDataFiltered_long %&gt;%\n  group_by(Subject,Condition,Electrode) %&gt;%\n  summarize(AvgVoltage = mean(Voltage))\n\n`summarise()` has grouped output by 'Subject', 'Condition'. You can override\nusing the `.groups` argument.\n\nhead(eegDataAveraged)\n\n# A tibble: 6 √ó 4\n# Groups:   Subject, Condition [1]\n  Subject Condition Electrode AvgVoltage\n  &lt;fct&gt;   &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 1       CondA     C1            -1.79 \n2 1       CondA     C2            -2.50 \n3 1       CondA     C3            -1.62 \n4 1       CondA     C4            -2.65 \n5 1       CondA     C5            -0.884\n6 1       CondA     C6            -2.57",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Assignment #5 - An EEG Processing Pipeline - Answer Key"
    ]
  },
  {
    "objectID": "assignment5_key.html#task3-create-a-linear-model-of-the-average-voltage",
    "href": "assignment5_key.html#task3-create-a-linear-model-of-the-average-voltage",
    "title": "Assignment #5 - An EEG Processing Pipeline - Answer Key",
    "section": "Task#3: Create a linear model of the Average Voltage",
    "text": "Task#3: Create a linear model of the Average Voltage\nTask#3.1: Fz Electrode\n\nSelect the data in the Fz electrode\nCreate a model to investigate if the AverageVoltage on the Fz electrode in the N400 window is related to the Condition factor.\n\n\neegData_Fz = eegDataAveraged %&gt;% filter(Electrode==\"Fz\")\nmFz &lt;- lm(AvgVoltage~Condition, data=eegData_Fz)\nsummary(mFz)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition, data = eegData_Fz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7087 -0.7836  0.1005  1.0234  6.0605 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -2.1692     0.5430  -3.995  0.00017 ***\nConditionCondB   1.6390     0.7679   2.135  0.03663 *  \nConditionCondC   2.2767     0.7679   2.965  0.00425 ** \nConditionCondD   0.8042     0.7679   1.047  0.29887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 64 degrees of freedom\nMultiple R-squared:  0.1351,    Adjusted R-squared:  0.09455 \nF-statistic: 3.332 on 3 and 64 DF,  p-value: 0.02487\n\n\nFrom the results, we can infer that:\n\nThere appears to be a significant difference across conditions on the Average voltage in a window.\nCondition B and C average voltages differ from the reference Condition A, however Condition D does not.\nThe model significance indicates that the inclusion of the Condition predictor in the model improved the explanatory power compared with an intercept only (null) model.\n\nThe exercise Task 3.3, however, asked to consider Condition D as a reference condition, while the model by default provides the results considering Condition A as the reference. To correct this, we relevel the Condition factor:\n\neegData_Fz &lt;- eegData_Fz %&gt;% mutate(Condition=relevel(Condition,\"CondD\"))\nmFz &lt;- lm(AvgVoltage~Condition, data=eegData_Fz)\nsummary(mFz)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition, data = eegData_Fz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7087 -0.7836  0.1005  1.0234  6.0605 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)     -1.3650     0.5430  -2.514   0.0145 *\nConditionCondA  -0.8042     0.7679  -1.047   0.2989  \nConditionCondB   0.8348     0.7679   1.087   0.2810  \nConditionCondC   1.4725     0.7679   1.918   0.0596 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 64 degrees of freedom\nMultiple R-squared:  0.1351,    Adjusted R-squared:  0.09455 \nF-statistic: 3.332 on 3 and 64 DF,  p-value: 0.02487\n\n\nNow, as you can see, the coefficients changes, and there is only a very marginally significant coefficient (ConditionCondC ), indicating that no Condition is significantly different from the control D.\nMarginal means calculation\nLet‚Äôs look at the marginal means using the modelbased library and plot them.\n\nemmFz &lt;- estimate_means(mFz, by=\"Condition\")\nemmFz\n\nEstimated Marginal Means\n\nCondition |  Mean |   SE |         95% CI | t(64)\n-------------------------------------------------\nCondD     | -1.36 | 0.54 | [-2.45, -0.28] | -2.51\nCondA     | -2.17 | 0.54 | [-3.25, -1.08] | -4.00\nCondB     | -0.53 | 0.54 | [-1.61,  0.55] | -0.98\nCondC     |  0.11 | 0.54 | [-0.98,  1.19] |  0.20\n\nVariable predicted: AvgVoltage\nPredictors modulated: Condition\n\nplot(emmFz) + theme_minimal()\n\n\n\n\n\n\n\nAs we can see from the marginal means and plot, it looks like Condition A and D are slightly different than condition A.\nPost-hoc analysis\nWe understand the differences between all values, we run a post-hoc analysis with all pairwise comparisons. Let‚Äôs use for example Tukey correction (less strict than Bonferroni)\n\nestimate_contrasts(mFz, contrast=\"Condition\", p_adjust = \"tukey\", backend=\"emmeans\")\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |         95% CI |   SE | t(64) |     p\n--------------------------------------------------------------------\nCondA  | CondB  |      -1.64 | [-3.66,  0.39] | 0.77 | -2.13 | 0.153\nCondA  | CondC  |      -2.28 | [-4.30, -0.25] | 0.77 | -2.96 | 0.022\nCondB  | CondC  |      -0.64 | [-2.66,  1.39] | 0.77 | -0.83 | 0.840\nCondD  | CondA  |       0.80 | [-1.22,  2.83] | 0.77 |  1.05 | 0.722\nCondD  | CondB  |      -0.83 | [-2.86,  1.19] | 0.77 | -1.09 | 0.699\nCondD  | CondC  |      -1.47 | [-3.50,  0.55] | 0.77 | -1.92 | 0.231\n\nVariable predicted: AvgVoltage\nPredictors contrasted: Condition\np-value adjustment method: Tukey\n\n\nWe see a few interesting things in the output above:\n\nOnly the contrast between condition C and Condition A seems to be significant, when applying the relevant correction for multiple comparisons. As a comparison, look at what would have been the result if the correction was not applied:\n\n\nestimate_contrasts(mFz, contrast=\"Condition\", backend=\"emmeans\")\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |         95% CI |   SE | t(64) |     p\n--------------------------------------------------------------------\nCondA  | CondB  |      -1.64 | [-3.17, -0.11] | 0.77 | -2.13 | 0.037\nCondA  | CondC  |      -2.28 | [-3.81, -0.74] | 0.77 | -2.96 | 0.004\nCondB  | CondC  |      -0.64 | [-2.17,  0.90] | 0.77 | -0.83 | 0.409\nCondD  | CondA  |       0.80 | [-0.73,  2.34] | 0.77 |  1.05 | 0.299\nCondD  | CondB  |      -0.83 | [-2.37,  0.70] | 0.77 | -1.09 | 0.281\nCondD  | CondC  |      -1.47 | [-3.01,  0.06] | 0.77 | -1.92 | 0.060\n\nVariable predicted: AvgVoltage\nPredictors contrasted: Condition\np-values are uncorrected.\n\n\n\nIn this case also the comparison between Conditions A and B is signficant, but it is likely a Type I error due to the use of multiple comparisons.\n\n\n\n\n\n\n\nAnalysis using emmeans\n\n\n\nConsidering that the modelbased package is relatively new and the wide usage of emmeans , I include here the same calculation using emmeans for reference, as the nomenclature and output is a bit different. Remember though that modelbased is a wrapper that calls emmeans on the background.\n\nemmFz_alt &lt;- emmeans(mFz,~Condition)\nemmFz_alt\n\n Condition emmean    SE df lower.CL upper.CL\n CondD     -1.365 0.543 64   -2.450   -0.280\n CondA     -2.169 0.543 64   -3.254   -1.085\n CondB     -0.530 0.543 64   -1.615    0.555\n CondC      0.107 0.543 64   -0.977    1.192\n\nConfidence level used: 0.95 \n\n\nUsing plot directly on the emmeans output produces the same plot in a different format but with the same information.\n\nplot(emmFz_alt, horizontal = FALSE)\n\n\n\n\n\n\n\nFinally, to perform pairwise comparisons, the same function emmeans can be used, but using the pairwise word in the formula as below:\n\nemmeans(mFz,pairwise~Condition)\n\n$emmeans\n Condition emmean    SE df lower.CL upper.CL\n CondD     -1.365 0.543 64   -2.450   -0.280\n CondA     -2.169 0.543 64   -3.254   -1.085\n CondB     -0.530 0.543 64   -1.615    0.555\n CondC      0.107 0.543 64   -0.977    1.192\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate    SE df t.ratio p.value\n CondD - CondA    0.804 0.768 64   1.047  0.7224\n CondD - CondB   -0.835 0.768 64  -1.087  0.6986\n CondD - CondC   -1.472 0.768 64  -1.918  0.2310\n CondA - CondB   -1.639 0.768 64  -2.135  0.1533\n CondA - CondC   -2.277 0.768 64  -2.965  0.0216\n CondB - CondC   -0.638 0.768 64  -0.830  0.8398\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nAs you can see from the output, the function automatically applied a Tukey correction, without the need to specify it. Results are the same we obtained before\n\n\nModel assumption checking\nFinally, let‚Äôs check the model assumptions for validity. First on the linearity of the residuals\n\nqqnorm(mFz$residuals)\nqqline(mFz$residuals)\n\n\n\n\n\n\nhist(mFz$residuals, breaks = 25)\n\n\n\n\n\n\nshapiro.test(mFz$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mFz$residuals\nW = 0.93818, p-value = 0.002184\n\n\n\nThe qqplot and histogram, show that the values are quite normal, however outliers are affecting the results. In the real data set, with additional subjects this will not appear, and the model will show to be valid. For the purpose of this exercise, we will report the observations.\n\nFor the homogeneity of variance we use the test that confirms the model meets the assumptions\n\nncvTest(mFz)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.6019959, Df = 1, p = 0.43782\n\n\nTask#3.2: Cz Electrode\nWe repeat the above process for the Cz electrode:\nModel fit\n\nSelect the data in the Cz electrode\nCreate a model to investigate if the AverageVoltage on the Fz electrode in the N400 window is related to the Condition factor.\n\n\neegData_Cz = eegDataAveraged %&gt;% filter(Electrode==\"Cz\") %&gt;% mutate(Condition=relevel(Condition,\"CondD\"))\nmCz &lt;- lm(AvgVoltage~Condition, data=eegData_Cz)\nsummary(mCz)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition, data = eegData_Cz)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3318 -0.9362  0.1938  1.0596  5.1886 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -2.0151     0.5120  -3.936 0.000207 ***\nConditionCondA  -0.2577     0.7241  -0.356 0.723063    \nConditionCondB   1.8329     0.7241   2.531 0.013834 *  \nConditionCondC   2.2794     0.7241   3.148 0.002499 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.111 on 64 degrees of freedom\nMultiple R-squared:  0.2263,    Adjusted R-squared:   0.19 \nF-statistic: 6.239 on 3 and 64 DF,  p-value: 0.0008772\n\nconfint(mCz)\n\n                    2.5 %     97.5 %\n(Intercept)    -3.0379764 -0.9921956\nConditionCondA -1.7043200  1.1888510\nConditionCondB  0.3863119  3.2794828\nConditionCondC  0.8327691  3.7259401\n\n\nMarginal means calculation and post-hoc analysis\n\nemmCz &lt;- estimate_means(mCz, by=\"Condition\")\nemmCz\n\nEstimated Marginal Means\n\nCondition |  Mean |   SE |         95% CI | t(64)\n-------------------------------------------------\nCondD     | -2.02 | 0.51 | [-3.04, -0.99] | -3.94\nCondA     | -2.27 | 0.51 | [-3.30, -1.25] | -4.44\nCondB     | -0.18 | 0.51 | [-1.21,  0.84] | -0.36\nCondC     |  0.26 | 0.51 | [-0.76,  1.29] |  0.52\n\nVariable predicted: AvgVoltage\nPredictors modulated: Condition\n\nplot(emmCz) + theme_minimal()\n\n\n\n\n\n\n\n\nestimate_contrasts(mCz, contrast=\"Condition\", p_adjust = \"tukey\", backend=\"emmeans\")\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | Difference |         95% CI |   SE | t(64) |     p\n--------------------------------------------------------------------\nCondA  | CondB  |      -2.09 | [-4.00, -0.18] | 0.72 | -2.89 | 0.027\nCondA  | CondC  |      -2.54 | [-4.45, -0.63] | 0.72 | -3.50 | 0.005\nCondB  | CondC  |      -0.45 | [-2.36,  1.46] | 0.72 | -0.62 | 0.926\nCondD  | CondA  |       0.26 | [-1.65,  2.17] | 0.72 |  0.36 | 0.984\nCondD  | CondB  |      -1.83 | [-3.74,  0.08] | 0.72 | -2.53 | 0.065\nCondD  | CondC  |      -2.28 | [-4.19, -0.37] | 0.72 | -3.15 | 0.013\n\nVariable predicted: AvgVoltage\nPredictors contrasted: Condition\np-value adjustment method: Tukey\n\n\nIn this case, there are three significant comparisons: between Conditions A and B, A and C and also C and D.\nModel assumptions checking\n\nqqnorm(mCz$residuals)\nqqline(mCz$residuals)\n\n\n\n\n\n\nhist(mCz$residuals, breaks = 25)\n\n\n\n\n\n\nshapiro.test(mCz$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mCz$residuals\nW = 0.9589, p-value = 0.02476\n\n\n\nncvTest(mCz)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.000360312, Df = 1, p = 0.98486\n\n\nThe results are similar to the Fz calculations, were the model failed on the assumption related to the normality of residuals due to a few outliers that would likely disappear when using the data of more subjects.\nTask #3.3: Report the models for both Fz and Cz:\nReport the results of the analysis performed in the APA format described in the lecture including if necessary any post-hoc analysis performed if needed to compare against the reference Condition D.\n\n\n\n\n\n\nWarning\n\n\n\nWe saw on the output from the different comparisons that a number of them show significant differences. Which ones to report however depend on your experimental design. The assignment asked to report against a reference condition D, which would imply that the other comparisons not against D are not relevant or interpretable for the experimental manipulation.\nThis can often be the case in linguistics experiments, where the different conditions are to be compared to a control. Based on this assumption we write the reporting below\n\n\n\nIn an Event Related Potential (ERP) study, the effect of experimental manipulation &lt;&lt;&lt;HERE THE EXPERIMENT DESCRIPTION&gt;&gt;&gt; was assessed using a linear model analysis to investivate the presence of a N400 component. The analysis was performed on two sites (Electrodes Fz and Cz) on the averaged voltage in the time window [250ms, 450ms]. Three conditions (A, B, C) were compared to a a control condition D.\nLinear model results showed a significant effect for Condition both in Electrode Fz (\\(F(3,64)=3.33,p=.0.25,R^2_{adj}=0.09\\)) and Cz (\\(F(3,64)=6.24,p&lt;.001,R^2{adj}=0.19\\)).\nIn the Frontal region, as measured in the site Fz, the fitted model was:\n\\[AverageVoltage_{Fz} = -1.36 -0.80\\times CondA+0.83\\times CondB+1.47\\times CondC\\]\nThe model did not reveal any significant effect of Condition with respect to the reference Condition D.\n\n\n\n\n\n\n\n\n\nCoefficient\nbeta\nConfidence Interval\nt\np\n\n\n\nIntercept (corresponding to Condition D)\n-1.36\n[-2.45, -0.28]\nt(64) = -2.51\np = 0.014\n\n\nCondition A\n-0.80\n[-2.34, 0.73]\nt(64) = -1.05\np = 0.299\n\n\nCondition B\n0.83\n[-0.70, 2.37]\nt(64) = 1.09\np = 0.281\n\n\nCondition C\n1.47\n[-0.06, 3.01]\nt(64) = 1.92\np = 0.060\n\n\n\nIn the Central Region, measured at the site Cz, the fitted model was:\n\\[AverageVoltage_{Cz} = -2.02 -0.26\\times CondA+1.83\\times CondB+2.28\\times CondC\\]\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nbeta\nConfidence Interval\nt\np\n\n\n\nIntercept (corresponding to Condition D)\n-2.02\n[-3.04, -0.99]\nt(64) = -3.94\np&lt;.001\n\n\nCondition A\n-0.26\n[-1.70, 1.19]\nt(64) = -0.36\np = 0.723\n\n\nCondition B\n1.83\n[0.39, 3.28]\nt(64) = 2.53\np = 0.014\n\n\nCondition C\n2.28\n[0.83, 3.73]\nt(64) = 3.15\np = 0.002\n\n\n\nPost-hoc testing based on estimation of the marginal means of the model showed a significant positive Average Voltage in Conditions C with respect to Control Condition D (\\(Diff_{C-D}=2.28, 95\\%CI[0.37,4.19],SE=0.72,t(64)3.15,p=0.013\\)). The other two conditions showed a marginal significant positive difference in Condition B (\\(Diff_{B-D}= 1.83, 95\\%CI[-0.08,3.74],SE=0.72,t(64)=-2.53,p=0.065\\)) and not a significant difference in Condition A (\\(Diff_{A-D}= -0.26, 95\\%CI[-2.17,1.65],SE=0.72,t(64)=0.36,p=0.984\\)).\nPairwise post-hoc comparisons were performed using Tukey correction to account for multiple comparisons.\nModel assumptions checking revealed deviations from the normality assumptions on both models that would require a more detailed look at the data outliers or likely an increase of the number of subjects in the study.\n\nEXTRA CREDIT: Task #4 Create factor encoding Region of interest.\nTask 4.1: Create new factor\nCreate a new variable with a factor to encode the Region of Interest called ROI based on the criteria defined above and two levels:\n\nAnterior: electrodes F1, F2, F3, F4, F5, F6, Fz\nCentral: electrodes C1, C2, C3, C4, C5, C6, Cz\n\nTip: You can create a variable as a function of the value of another variable using case_when() . Read the following page for instructions : https://www.statology.org/conditional-mutating-r/\n\neegDataAveraged&lt;- eegDataAveraged %&gt;% mutate(Condition=relevel(Condition,\"CondD\"))\n\neegDataAveraged$roi &lt;- case_when(\n  eegDataAveraged$Electrode %in% c(\"F1\",\"F2\",\"F3\",\"F4\",\"F5\",\"F6\",\"Fz\") ~ \"Anterior\",\n  eegDataAveraged$Electrode %in% c(\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"Cz\") ~ \"Central\"\n)\neegDataAveraged$roi&lt;-as.factor(eegDataAveraged$roi)\n\nTask 4.2: Multiple linear regression\n\nCreate a model to investigate if the AverageVoltage in the N400 window is related to the Condition and ROI predictors.\n\n\nmCA &lt;-lm(AvgVoltage~Condition+roi, data = eegDataAveraged)\nsummary(mCA)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition + roi, data = eegDataAveraged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9257 -0.8371  0.0603  1.0342  6.4014 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.45219    0.15511  -9.362  &lt; 2e-16 ***\nConditionCondA -0.61770    0.19301  -3.200  0.00142 ** \nConditionCondB  1.12764    0.19301   5.842 7.24e-09 ***\nConditionCondC  1.55151    0.19301   8.039 2.92e-15 ***\nroiCentral      0.03094    0.13688   0.226  0.82124    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.029 on 879 degrees of freedom\nMultiple R-squared:  0.1548,    Adjusted R-squared:  0.151 \nF-statistic: 40.25 on 4 and 879 DF,  p-value: &lt; 2.2e-16\n\n\nIf we calculate the marginal means and plot we can see that there are no major differences between the two ROIs.\n\nemm&lt;-estimate_means(mCA,by=c(\"Condition\",\"roi\"))\nemm\n\nEstimated Marginal Means\n\nCondition | roi      |  Mean |   SE |         95% CI | t(879)\n-------------------------------------------------------------\nCondD     | Anterior | -1.45 | 0.16 | [-1.76, -1.15] |  -9.36\nCondA     | Anterior | -2.07 | 0.16 | [-2.37, -1.77] | -13.34\nCondB     | Anterior | -0.32 | 0.16 | [-0.63, -0.02] |  -2.09\nCondC     | Anterior |  0.10 | 0.16 | [-0.21,  0.40] |   0.64\nCondD     | Central  | -1.42 | 0.15 | [-1.72, -1.13] |  -9.45\nCondA     | Central  | -2.04 | 0.15 | [-2.33, -1.74] | -13.56\nCondB     | Central  | -0.29 | 0.15 | [-0.59,  0.00] |  -1.95\nCondC     | Central  |  0.13 | 0.15 | [-0.16,  0.43] |   0.87\n\nVariable predicted: AvgVoltage\nPredictors modulated: Condition, roi\n\nplot(emm)\n\n\n\n\n\n\n\n\nestimate_contrasts(mCA, \"Condition\", by=c(\"roi\"),p_adjust = \"tukey\",backend = \"emmeans\")\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2 | roi      | Difference |         95% CI |   SE | t(879) |      p\n---------------------------------------------------------------------------------\nCondA  | CondB  | Anterior |      -1.75 | [-2.24, -1.25] | 0.19 |  -9.04 | &lt; .001\nCondA  | CondC  | Anterior |      -2.17 | [-2.67, -1.67] | 0.19 | -11.24 | &lt; .001\nCondB  | CondC  | Anterior |      -0.42 | [-0.92,  0.07] | 0.19 |  -2.20 |  0.125\nCondD  | CondA  | Anterior |       0.62 | [ 0.12,  1.11] | 0.19 |   3.20 |  0.008\nCondD  | CondB  | Anterior |      -1.13 | [-1.62, -0.63] | 0.19 |  -5.84 | &lt; .001\nCondD  | CondC  | Anterior |      -1.55 | [-2.05, -1.05] | 0.19 |  -8.04 | &lt; .001\nCondA  | CondB  | Central  |      -1.75 | [-2.24, -1.25] | 0.19 |  -9.04 | &lt; .001\nCondA  | CondC  | Central  |      -2.17 | [-2.67, -1.67] | 0.19 | -11.24 | &lt; .001\nCondB  | CondC  | Central  |      -0.42 | [-0.92,  0.07] | 0.19 |  -2.20 |  0.125\nCondD  | CondA  | Central  |       0.62 | [ 0.12,  1.11] | 0.19 |   3.20 |  0.008\nCondD  | CondB  | Central  |      -1.13 | [-1.62, -0.63] | 0.19 |  -5.84 | &lt; .001\nCondD  | CondC  | Central  |      -1.55 | [-2.05, -1.05] | 0.19 |  -8.04 | &lt; .001\n\nVariable predicted: AvgVoltage\nPredictors contrasted: Condition\np-value adjustment method: Tukey\n\n\nWhat you can see from the results and the graphs is that the marginal means show difference between conditions, but same behavior is observed across Regions of Interest, without particular differences between Anterior and Central regions.\nHowever, you can see that a number of comparisons that were not significant before are now that we have included more data (more sites). We can compare model with and without a parameter.\nPlease note that a model with only the Region of Interest roi does not make theoretical sense in general, as it will imply that the activity is different independently of the manipulation.\n\nmCA_0&lt;-lm(AvgVoltage~Condition, data=eegDataAveraged)\nsummary(mCA_0)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition, data = eegDataAveraged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9114 -0.8330  0.0626  1.0369  6.3848 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -1.4355     0.1364 -10.524  &lt; 2e-16 ***\nConditionCondA  -0.6177     0.1929  -3.202  0.00141 ** \nConditionCondB   1.1276     0.1929   5.846 7.11e-09 ***\nConditionCondC   1.5515     0.1929   8.043 2.82e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.028 on 880 degrees of freedom\nMultiple R-squared:  0.1548,    Adjusted R-squared:  0.1519 \nF-statistic: 53.71 on 3 and 880 DF,  p-value: &lt; 2.2e-16\n\nanova(mCA_0,mCA)\n\nAnalysis of Variance Table\n\nModel 1: AvgVoltage ~ Condition\nModel 2: AvgVoltage ~ Condition + roi\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    880 3618.5                           \n2    879 3618.3  1   0.21027 0.0511 0.8212\n\n\nAs expected, the region of interest does not improve the model.\nWe will see that introducing an interaction between the two factors can change this interpretation after the Task#5 in Assignment #6.",
    "crumbs": [
      "Workgroup 5: Multiple Regression with R",
      "Assignment #5 - An EEG Processing Pipeline - Answer Key"
    ]
  },
  {
    "objectID": "Workgroup6.html",
    "href": "Workgroup6.html",
    "title": "Workgroup 6: Multiple Regression with R II",
    "section": "",
    "text": "Workgroup 6 material to be published in the coming weeks\n\n\n\n\n\n\nThis workgropu introduces:\n\nInteractions in multiple linear regression\nInterpretation of interaction coefficients\nSum coding (a.k.a. deviation/effect coding)\nMarginal means and simple slopes (emmeans)\n\nIn this session we will examine how to execute and interpret multiple linear regression models with interactions.\nWe will look at examples of models with a combination of interactions between categorical and continuous predictor and the interpretation of its coefficients in each of the cases with two different types of dummy variable coding: treatment (dummy) coding and sum (deviation/effect) coding.\n\nüß† Learning Objectives\nBy the end of this lesson, you will be able to:\n\nFit and interpret a multiple linear model with interactions using lm() .\nReview concept of marginal means in the presence of interactions.\nVisualize interactions in models.\nUnderstand concept of sum coding.\nReport interactions in multiple linear models.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II"
    ]
  },
  {
    "objectID": "model_interactions.html",
    "href": "model_interactions.html",
    "title": "Multiple Linear Regression: Interactions",
    "section": "",
    "text": "Interaction with one categorical and one continuous predictor (Categorical*Continuous)\nAs described during the last lecture, interactions in a model reflect that the effect of one predictor depends on the level (categorical) or value (continuous) of one or more other predictors.\nWith two predictors, we discussed a model including and interaction between two predictors \\(X_1\\) and \\(X_2\\) would look as follows:\n\\[\nY=b_0+b_1X_1+b_2X_2+b_3X_1X_2\n\\]\nwith \\(b_3\\) coefficient indicating the strength of the interaction.\nLet‚Äôs look at a few examples of two combinations of predictors.\nTo illustrate this, we use the Iconicity model as described in [Winter (2019) Chapter 8.2, based on the study in (Winter et al. 2017). Read through the book chapters to see a similar analysis.\nBodo Winter made all data used in the book available in an Open Source Foundation (OSF) directory, from which it can be downloaded directly from R.\nlibrary(tidyverse)\n\niconicity&lt;-read.csv('https://osf.io/43btm/download')\nDescription of the dataset (see full details in Chapter 6 of (Winter 2019):\nFor the analysis we will look at a subset of Noun and Verb.\nicon&lt;-iconicity %&gt;% filter(POS %in% c(\"Noun\",\"Verb\"))\nIf we fit a model with both SER and POS as predictors as a reference and look at the effects:\nlibrary(ggeffects)\nlibrary(ggiraphExtra)\n\nm1&lt;-lm(Iconicity~POS+SER,data=icon)\nsummary(m1)\n\n\nCall:\nlm(formula = Iconicity ~ POS + SER, data = icon)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0956 -0.7210 -0.1233  0.6146  3.6902 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.11935    0.10040  -1.189    0.235    \nPOSVerb      0.60159    0.06388   9.418   &lt;2e-16 ***\nSER          0.23319    0.02789   8.362   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.045 on 1476 degrees of freedom\n  (782 observations deleted due to missingness)\nMultiple R-squared:  0.08226,   Adjusted R-squared:  0.08102 \nF-statistic: 66.15 on 2 and 1476 DF,  p-value: &lt; 2.2e-16\n\nggPredict(m1, terms=c(\"POS\",\"SER\"),se = TRUE) + theme_classic()\nInterpretation of the plot and coefficients are as per last lecture, with same slope (effect of SER) for both levels of POS but with a shift between them.\nLet‚Äôs look now at what happens if we introduce the interaction in the model:\nm2&lt;-lm(Iconicity~POS*SER,data=icon)\nsummary(m2)\n\n\nCall:\nlm(formula = Iconicity ~ POS * SER, data = icon)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8448 -0.7043 -0.1257  0.5864  3.5845 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.27394    0.11065   2.476  0.01341 *  \nPOSVerb     -0.95542    0.20971  -4.556 5.65e-06 ***\nSER          0.11817    0.03108   3.801  0.00015 ***\nPOSVerb:SER  0.50838    0.06535   7.780 1.36e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 1475 degrees of freedom\n  (782 observations deleted due to missingness)\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1166 \nF-statistic: 66.05 on 3 and 1475 DF,  p-value: &lt; 2.2e-16\n\nggPredict(m2, terms=c(\"POS\",\"SER\"),se = TRUE) + theme_classic()\nAs you can see, allowing the model fit to account for an interaction allows a variability of the slope or effect of SER for each of the levels of the other predictor. The two lines are not parallel now. The sensory experience has a different impact on Iconicity for Nouns compared to Verbs.\nIgnoring a data interaction when present in the data leads to incorrect interpretation of the fit results.\nThe fitted model is:\n\\[ Iconicity=0.274-0.955\\times POS_{Verb}+0.118\\times SER+0.508\\times POS_{Verb}\\times SER\\]\nLet‚Äôs check what every coefficient means:",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multiple Linear Regression: Interactions"
    ]
  },
  {
    "objectID": "model_interactions.html#interaction-with-one-categorical-and-one-continuous-predictor-categoricalcontinuous",
    "href": "model_interactions.html#interaction-with-one-categorical-and-one-continuous-predictor-categoricalcontinuous",
    "title": "Multiple Linear Regression: Interactions",
    "section": "",
    "text": "Iconicity: degree to which a word form resembles the meaning of a word, based on a rating scale. E.g: onomatopoeic words such as bang and beep are iconic because they imitate the sounds these words describe. This is the measured or outcome variable.\nSER: Sensory experience rating for a particular word as extracted from a rating study.\nPOS (Part of Speech): Lexical categoryPosition in the sentence where the word was used.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(b_0\\) - intercept: corresponds to the Iconicity for a Noun (POS=0) for a word with Sensory Experience Rating = 0. Note that this is not directly interpretable, since SER=0 is not even possible for this rating.\n\\(b_1\\) - slope coefficient for POS. Represents the effect in the Noun-Verb difference for words with SER=0. If you were to extend the plot above to the left to SER = 0, you can see that the lines for Verb would be below the line for Noun.\n\\(b_2\\) - slope coefficient for SER. Represents the slope of the SER only in the case that POS=0 (Noun). So 0.118 is the slope of the red curve\n\\(b_3\\) - indicates the change in the slope of SER when POS=1 (Verb), so the slope of the green curve is \\(b_2+b_3=0.118+0.508=0.626\\).\n\nCentering continuous predictors\n\nA simpler way to interpret intercepts when involving continuous predictors can be done if the predictors are centered.\nCentering predictors involves substracting the mean from all the values. It does not affect the model fit quality, but the coefficients become directly interpretable. Let‚Äôs create a new variable centering SER called SER_c and fit a new model with the centered variable\n\nicon&lt;-icon %&gt;%\n  mutate(SER_c=SER-mean(SER, na.rm=TRUE))\n\nm2_c&lt;-lm(Iconicity~POS*SER_c, data=icon)\nsummary(m2_c)\n\n\nCall:\nlm(formula = Iconicity ~ POS * SER_c, data = icon)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8448 -0.7043 -0.1257  0.5864  3.5845 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.66423    0.03102  21.414  &lt; 2e-16 ***\nPOSVerb        0.72371    0.06456  11.209  &lt; 2e-16 ***\nSER_c          0.11817    0.03108   3.801  0.00015 ***\nPOSVerb:SER_c  0.50838    0.06535   7.780 1.36e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 1475 degrees of freedom\n  (782 observations deleted due to missingness)\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1166 \nF-statistic: 66.05 on 3 and 1475 DF,  p-value: &lt; 2.2e-16\n\nggPredict(m2_c, terms=c(\"POS\",\"SER\"),se = TRUE) + theme_classic()\n\n\n\n\n\n\n\nThe fitted model is:\n\\[ Iconicity=0.664+0.724\\times POS_{Verb}+0.118\\times SER_c+0.508\\times POS_{Verb}\\times SER_c\\]\nLet‚Äôs check what every coefficient means:\n\n\\(b_0\\) - intercept: corresponds to the Iconicity for a Noun (POS=0) for a word with average SER (SER_c=0). This has not a direct interpretation.\n\\(b_1\\) - slope coefficient for POS. Represents the effect in the Noun-Verb difference for words with average SER.\n\\(b_2\\) - slope coefficient for SER. Represents the slope of the SER only in the case that POS=0 (Noun). Same as before.\n\\(b_3\\) - indicates the change in the slope of SER when POS=1 (Verb). Same as before.\nModel selection\n\nThe model coefficients show significant simple effects and interactions, and it shows to improve the variance explained compared with a model without interactions.\nTo compare them we can also perform a model comparison as shown in previous workgroups:\n\n# model without interaction with centered variable\nm1_c&lt;-lm(Iconicity~POS+SER_c,data=icon)\n\nanova(m1_c,m2_c)\n\nAnalysis of Variance Table\n\nModel 1: Iconicity ~ POS + SER_c\nModel 2: Iconicity ~ POS * SER_c\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1476 1611.9                                  \n2   1475 1548.3  1    63.533 60.523 1.358e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC(m1_c,m2_c)\n\n     df      AIC\nm1_c  4 4332.468\nm2_c  5 4274.993\n\nBIC(m1_c,m2_c)\n\n     df      BIC\nm1_c  4 4353.664\nm2_c  5 4301.488\n\n\nAll three test identify a significant improvement of the model fit including the interaction.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multiple Linear Regression: Interactions"
    ]
  },
  {
    "objectID": "model_interactions.html#interaction-with-two-categorical-predictors-categoricalcategorical",
    "href": "model_interactions.html#interaction-with-two-categorical-predictors-categoricalcategorical",
    "title": "Multiple Linear Regression: Interactions",
    "section": "Interaction with two categorical predictors (Categorical*Categorical)",
    "text": "Interaction with two categorical predictors (Categorical*Categorical)\n\nFor this example, we will use again the subset of the real data from a Self-Paced Reading study on Negative Polarity Items and complementizer agreement in Basque (Pablos and Saddy 2009).\n\nload('./data/BasqueNPISampleEx5.Rda')\n\nThe basquenpi_Ex5 data frame contains only the data from Region Number 8 (complementizer position), which was considered the ‚Äúcritical region‚Äù for analysis in the experiment.\nFrom the dataframe, we will select a subset of the data columns for the analysis and introduce a transformation to inverse Reading Time (1/RWRT) to normalize the data as discussed in the assignment\n\n\nSubject ‚Äì Factor identifying the participant on the experiment (coded as a number from 1 to 32)\n\nEmbeddedSubject ‚Äì Factor/predictor indicating the nature of the embedded subject with the following levels:\n\nNP ‚Äì for target sentences with a Noun Phrase as subject\nNPI ‚Äì for target sentences with a Negative Polarity Item as subject\n\n\n\nAgreement Morphology ‚Äì Factor/predictor indicating the nature of agreement with the following levels:\n\nDeclarative ‚Äì for target sentences that contained a complementizer with declarative morphology\nPartitive ‚Äì for target sentences that contained a complementizer with partitive morphology\n\n\n\nRWRT ‚Äì Raw (recorded) Reading Time of the word [milliseconds]\n\nInvRWRT - Inverse Raw Reading Time [in seconds]\n\n\ndataBasque&lt;-basquenpi_Ex5 %&gt;% \n  select(Item, Subject, EmbeddedSubject, AgreementMorphology, RWRT) %&gt;%\n  mutate(InvRWRT=1/(RWRT/1000))\n\nAgain, let‚Äôs fit a model with interaction and a model without and compare them.\n\nlibrary(modelbased)\n\nmbasque_1&lt;-lm(InvRWRT~EmbeddedSubject+AgreementMorphology, data=dataBasque)\nsummary(mbasque_1)\n\n\nCall:\nlm(formula = InvRWRT ~ EmbeddedSubject + AgreementMorphology, \n    data = dataBasque)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.72727 -0.42095  0.01369  0.38429  2.70877 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    2.1050     0.0394  53.422  &lt; 2e-16 ***\nEmbeddedSubjectNPI            -0.1757     0.0455  -3.862 0.000122 ***\nAgreementMorphologyPartitive  -0.1537     0.0455  -3.378 0.000766 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6304 on 765 degrees of freedom\nMultiple R-squared:  0.03327,   Adjusted R-squared:  0.03074 \nF-statistic: 13.16 on 2 and 765 DF,  p-value: 2.394e-06\n\nemm_basque_1&lt;-estimate_means(mbasque_1, by=c(\"EmbeddedSubject\",\"AgreementMorphology\"))\nemm_basque_1\n\nEstimated Marginal Means\n\nEmbeddedSubject | AgreementMorphology | Mean |   SE |       95% CI | t(765)\n---------------------------------------------------------------------------\nNP              | Declarative         | 2.10 | 0.04 | [2.03, 2.18] |  53.42\nNPI             | Declarative         | 1.93 | 0.04 | [1.85, 2.01] |  48.96\nNP              | Partitive           | 1.95 | 0.04 | [1.87, 2.03] |  49.52\nNPI             | Partitive           | 1.78 | 0.04 | [1.70, 1.85] |  45.06\n\nVariable predicted: InvRWRT\nPredictors modulated: EmbeddedSubject, AgreementMorphology\n\nplot(emm_basque_1) + theme_classic()\n\n\n\n\n\n\n\nAnd now with the interaction:\n\nmbasque_2&lt;-lm(InvRWRT~EmbeddedSubject*AgreementMorphology, data=dataBasque)\nsummary(mbasque_2)\n\n\nCall:\nlm(formula = InvRWRT ~ EmbeddedSubject * AgreementMorphology, \n    data = dataBasque)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.69400 -0.44454  0.03515  0.39684  2.67551 \n\nCoefficients:\n                                                Estimate Std. Error t value\n(Intercept)                                      2.13822    0.04546  47.031\nEmbeddedSubjectNPI                              -0.24224    0.06430  -3.768\nAgreementMorphologyPartitive                    -0.22025    0.06430  -3.426\nEmbeddedSubjectNPI:AgreementMorphologyPartitive  0.13307    0.09093   1.463\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEmbeddedSubjectNPI                              0.000177 ***\nAgreementMorphologyPartitive                    0.000646 ***\nEmbeddedSubjectNPI:AgreementMorphologyPartitive 0.143754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.63 on 764 degrees of freedom\nMultiple R-squared:  0.03597,   Adjusted R-squared:  0.03219 \nF-statistic: 9.503 on 3 and 764 DF,  p-value: 3.618e-06\n\nconfint(mbasque_2)\n\n                                                      2.5 %      97.5 %\n(Intercept)                                      2.04897153  2.22747080\nEmbeddedSubjectNPI                              -0.36846214 -0.11602605\nAgreementMorphologyPartitive                    -0.34646508 -0.09402899\nEmbeddedSubjectNPI:AgreementMorphologyPartitive -0.04542944  0.31156911\n\nemm_basque_2&lt;-estimate_means(mbasque_2, by=c(\"EmbeddedSubject\",\"AgreementMorphology\"))\nemm_basque_2\n\nEstimated Marginal Means\n\nEmbeddedSubject | AgreementMorphology | Mean |   SE |       95% CI | t(764)\n---------------------------------------------------------------------------\nNP              | Declarative         | 2.14 | 0.05 | [2.05, 2.23] |  47.03\nNPI             | Declarative         | 1.90 | 0.05 | [1.81, 1.99] |  41.70\nNP              | Partitive           | 1.92 | 0.05 | [1.83, 2.01] |  42.19\nNPI             | Partitive           | 1.81 | 0.05 | [1.72, 1.90] |  39.79\n\nVariable predicted: InvRWRT\nPredictors modulated: EmbeddedSubject, AgreementMorphology\n\nplot(emm_basque_2) + theme_classic()\n\n\n\n\n\n\n\nAs you can see, the ‚Äúslopes‚Äù of the effect and coefficient values have change, and so is the interpretation.\nThe fitted model is:\n\\[ (1/RWRT)=2.138-2.422\\times EmbeddedSubject_{NPI}-2.220\\times AgreementMorphology_{Partitive}+0.133\\times EmbeddedSubject_{NPI}\\times AgreementMorphology_{Partitive}\\]\nLet‚Äôs check what every coefficient means:\n\n\\(b_0\\) - intercept: corresponds to the Inverse Raw Reading Time for the reference cell in the design: EmbeddedSubject = NP (EmbeddedSubjectNPI=0) and AgreementMorphology = Declarative (AgreementMorphologyPartitive=0).\n\\(b_1\\) - slope coefficient for EmbeddedSubject. Represents the effect in the InvRWRT for EmbeddedSubject=NPI compared with the reference cell (NP, Declarative), while AgreementMorphology = Declarative\n\\(b_2\\) - slope coefficient for AgreementMorphology. Represents the change in InvRWRT for AgreementMorphology = Partitive compared with the reference cell (NP, Declarative), while EmbeddedSubject = NP\n\\(b_3\\) - indicates the change with respect to other coefficients when EmbeddedSubject=NPI and AgreementMorphology = Partitive\n\nSo the estimated values are the following:\n\n\n\n\n\n\n\n\n\nAgreement Morphology = Declarative\nAgreement Morphology = Partitive\n\n\nEmbedded Subject = NP\n\\(b_0\\)\n\\(b_0+b_2\\)\n\n\nEmbedded Subject = NPI\n\\(b_0+b_1\\)\n\\(b_0+b_1+b_2+b_3\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSimple effects vs Main effects\nThe effects defined by \\(b_1\\) and \\(b_2\\) are called simple effects as they represent the impact of a particular variable for a specific level of the other predictor.\nIn general, in research what is of interest is the effect of a predictor on the average response independently of the other predictors, which is referred to as main effects.\nWe look at how to evaluate main effects in the next section on Contrast Coding.\n\n\nModel reporting\n\nFinally, to wrap-up we describe what should be included when reporting a model with interactions:\n\nReport Overall Model: State the model‚Äôs significance and fit (\\(F, df, p, R^2,R^2_{adj}\\))\n\nProvide Table of Coefficients: Use a table for detailed results.\n\nColumns: Unstandardized Coefficients (b), Standard Error (SE), t, p, 95% Confidence Interval (CI).\nRows: Intercept, predictors, and the Interaction Terms\nItalics: Italicize statistical symbols like b, SE, t, p, M, SD, R¬≤.\n\n\n\nSignificant Interaction: If the interaction p-value is significant (e.g., &lt; .05), explain that the effect.\n\nReport the unstandardized coefficients for the interaction (Binteraction) and the main effect it modifies (BIV1).\n\n\n\nExplain effects:\n\nWhen including a continuous predictor and a categorical predictor as moderator describe and plot the slopes at meaningful predicted values to illustrate the interaction\nWith categorical predictors report the Estimated Marginal Means at the specific categories of the predictors.\n\n\nIf using data transformation, report coefficients directly from the model in transformed units, but Marginal Means in back-transformed units.\n\nExample:\n\nMultiple linear regression was performed to assess the relationship between the Reading Time measured in milliseconds and the nature of the Embedded Subject (NPI or NP) and the Morphological Agreement (Declarative or Partitive).\nRaw Reading Time (RWRT) data was transformed using an inverse transformation to address deviations from linear model assumptions observed after fit of RWRT. The model‚Äôs intercept\nThe model (formula: 1/RWRT ~ EmbeddedSubject * AgreementMorphology), explains a statistically significant although weak proportion of the data variance (\\(F(3,764)=9.50,p&lt;.001,R^2=0.03\\)). The model was fit using treatment coding, with the model‚Äôs intercept corresponding to the reference condition EmbeddedSubject = NP and AgreementMorphology = Declarative.\nThe table below summarizes the coefficients of the model:\n\n\n\n\n\n\n\n\n\n\nterm\nb*\nSE\nt\np\n95%CI\n\n\n\n(Intercept)\n2.14\n0.045\n47.0\n&lt;.001\n[2.05, 2.23]\n\n\nEmbeddedSubject_NPI\n-0.24\n0.064\n-3.77\n&lt;.001\n[-0.37, -0.12]\n\n\nAgreementMorphology_Partitive\n-0.22\n0.064\n-3.43\n&lt;.001\n[-0.35, -0.09]\n\n\nEmbeddedSubject_NPI:AgreementMorphology_Partitive\n0.13\n0.09\n1.45\n0.144\n[-0.05, 0.31]\n\n\n\n\n\n\n\n\nPablos, Leticia, and Douglas Saddy. 2009. ‚ÄúNegative Polarity Items and Complementizer Agreement in Basque.‚Äù Brain Talk, 61.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using r. Routledge.\n\n\nWinter, Bodo, Marcus Perlman, Lynn K Perry, and Gary Lupyan. 2017. ‚ÄúWhich Words Are Most Iconic? Iconicity in English Sensory Words.‚Äù Interaction Studies 18 (3): 443‚Äì64.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multiple Linear Regression: Interactions"
    ]
  },
  {
    "objectID": "model_contrasts.html",
    "href": "model_contrasts.html",
    "title": "Treatment Coding vs.¬†Sum Coding",
    "section": "",
    "text": "Setting contrasts\nWe discussed briefly in Workgroup 5 the concept of contrast coding of categorical variables.\nIn all the work up to now we have looked at dummy or treatment coding, where categorical predictors are coded considering a reference value against which the others are compared.\nThis coding is useful in many experimental designs where we have a control condition. It is however more difficult to assess main effects in the case of multiple predictors, i.e., the individual effect of a predictor independently from the value of other predictors. This can be done using Sum coding (also called effect coding).\nIn Sum coding, categorical variables are encoded in such a way that the intercept is the grand mean across all predictors, and comparisons between different levels of a given predictor are comparisons to the grand mean. This allows to interpret the coefficients directly as main effects.\nTreatment contrasts can be set using the function contrasts() together with the functions contr.treatment() and contr.sum() respectively for treatment and sum contrasts. The functions take as argument the number of levels fo the variables. You have to set the contrasts before model fitting.\nIn the previous example based on the Basque NPI study:\nTo set sum contrasts to the EmbeddedSubject and AgreementMorphology predictors we will do the following:\ncontrasts(dataBasque$EmbeddedSubject) &lt;- contr.sum(2)\ncontrasts(dataBasque$AgreementMorphology) &lt;- contr.sum(2)\ncontrasts(dataBasque$EmbeddedSubject)\n\n    [,1]\nNP     1\nNPI   -1\nThis will code the dummy variables as follows:\nWith that coding, a model as follows:\n\\[\n1/RWRT = b_0+b_1\\times EmbeddedSubject_{NPI}+b_2\\times AgreementMorphology_{Partitive}+b_3\\times EmbeddedSubject_{NPI}\\times AgreementMorphology_{Partitive}\n\\]\nIf now we fit a model and compare with the previous output:\nm_sum &lt;- lm(InvRWRT~EmbeddedSubject*AgreementMorphology, data=dataBasque)\nsummary(m_sum)\n\n\nCall:\nlm(formula = InvRWRT ~ EmbeddedSubject * AgreementMorphology, \n    data = dataBasque)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.694e-03 -4.445e-04  3.515e-05  3.968e-04  2.676e-03 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           1.940e-03  2.273e-05  85.353  &lt; 2e-16 ***\nEmbeddedSubject1                      8.785e-05  2.273e-05   3.865 0.000121 ***\nAgreementMorphology1                  7.686e-05  2.273e-05   3.381 0.000759 ***\nEmbeddedSubject1:AgreementMorphology1 3.327e-05  2.273e-05   1.463 0.143754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00063 on 764 degrees of freedom\nMultiple R-squared:  0.03597,   Adjusted R-squared:  0.03219 \nF-statistic: 9.503 on 3 and 764 DF,  p-value: 3.618e-06\nInterpretation of coefficients",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Treatment Coding vs. Sum Coding"
    ]
  },
  {
    "objectID": "model_contrasts.html#setting-contrasts",
    "href": "model_contrasts.html#setting-contrasts",
    "title": "Treatment Coding vs.¬†Sum Coding",
    "section": "",
    "text": "Embedded Subject = NP\nEmbedded Subject = NPI\n\n\n\nEmbeddedSubjectNPI\n+1\n-1\n\n\n\nAgreementMorphology = Declarative\nAgreementMorphology = Partitive\n\n\nAgreementMorphologyPartitive\n+1\n-1\n\n\n\n\n\n\n\n\n\n\\(b_0\\) - Intercept = grand mean across all four cells (NP-Declarative, NP-Partitive, NPI-Declarative, NPI-Partitive)\n\\(b_1\\) - EmbeddedSubject main effect= half the difference between two levels of EmbeddedSubject\n\\(b_2\\) - AgreementMorphology main effect= half the difference between two levels of AgreementMorphology\n\\(b_3\\) - Effect of interaction: 1/4th of the full interaction.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Treatment Coding vs. Sum Coding"
    ]
  },
  {
    "objectID": "model_contrasts.html#recovering-cell-means",
    "href": "model_contrasts.html#recovering-cell-means",
    "title": "Treatment Coding vs.¬†Sum Coding",
    "section": "Recovering Cell Means",
    "text": "Recovering Cell Means\nPredicted means can be calculated based on the encoding:\n\n\nCondition\nPredicted Mean (Marginal Means)\n\n\n\nNP (+1), Declarative (+1)\n\\(b_0+b_1+b_2+b_3\\)\n\n\nNP (+1), Partitive (‚àí1)\n\\(b_0+b_1-b_2-b_3\\)\n\n\nNPI (‚àí1), Declarative (+1)\n\\(b_0-b_1+b_2-b_3\\)\n\n\nNPI (‚àí1), Partitive (‚àí1)\n\\(b_0-b_1-b_2+b_3\\)\n\n\n\nAs per the table you can recover the interpretation of the coefficients:\n\n\nThe grand mean is the average of the four means above is the sum of all, divided by 4:\n\\[\n\\frac{(b_0+b_1+b_2+b_3)+(b_0+b_1-b_2-b_3)+(b_0-b_1+b_2-b_3)+(b_0-b_1-b_2+b_3)}{4}=\\frac{4b_0}{4}=b_0\n\\]\nSo this confirms the interpretation of the intercept.\n\n\nSimilarly, if we want to see the effect of the EmbeddedSubject, we can calculate the difference between the means of NP conditions and NPI conditions:\n\\[\n\\frac{(b_0+b_1+b_2+b_3)+(b_0+b_1-b_2-b_3)}{2}-\\frac{(b_0-b_1+b_2-b_3)+(b_0-b_1-b_2+b_3)}{2}=2b_1\n\\]\nAgain, that shows that \\(b_1\\) is half the difference between the average of the two levels.\n\n\nA similar calculation can be done for the other elements.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Treatment Coding vs. Sum Coding"
    ]
  },
  {
    "objectID": "model_multicollinearity.html",
    "href": "model_multicollinearity.html",
    "title": "Multicollinearity",
    "section": "",
    "text": "Correlation assessment using VIF\nMulticollinearity occurs when predictors in a regression model are highly correlated. This is common in variables as for example Frequency, Word Length or Predictability (shorted words are less frequent and more predictable than long words).\nFitting a model with two predictors that vary together creates problems\nLet‚Äôs examine examine relationships among using the lexdec dataset Frequency, FamilySize, WordLength, and logRT.\nVariance Inflation Factor (VIF) quantifies how much variance of a coefficient is inflated due to correlation. The VIF is calculated trying to create a regression against other predictors VIF = 1 / (1 - R¬≤).\nInterpretation guidelines:\nWe calculate it with the vif() function in the car model.\nlibrary(car)\n\nmod1 &lt;- lm(RT ~ Frequency + FamilySize + Length, data = lexdec)\n\nvif(mod1)\n\n Frequency FamilySize     Length \n  2.002928   2.733769   1.675048",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multicollinearity"
    ]
  },
  {
    "objectID": "model_multicollinearity.html#correlation-assessment-using-vif",
    "href": "model_multicollinearity.html#correlation-assessment-using-vif",
    "title": "Multicollinearity",
    "section": "",
    "text": "A high R¬≤ for a predictor means it‚Äôs predictable by others (high multicollinearity), which inflates its VIF, making its coefficient unstable.¬†\n\n\n\nVIF = 1: No multicollinearity; the predictor is independent.\n1 &lt; VIF &lt; 5: Moderate multicollinearity; generally not a serious problem.\nVIF &gt; 5: Potentially severe multicollinearity; coefficients and p-values may be unreliable, warranting investigation.\nVIF &gt; 10: Serious multicollinearity; the model is likely unstable, and the predictor might need removal or transformation.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multicollinearity"
    ]
  },
  {
    "objectID": "model_multicollinearity.html#multicollinearity-in-interaction-models",
    "href": "model_multicollinearity.html#multicollinearity-in-interaction-models",
    "title": "Multicollinearity",
    "section": "Multicollinearity in Interaction Models",
    "text": "Multicollinearity in Interaction Models\nInteraction terms are inherently correlated with their main components, and values would be higher.\n\nmod2&lt;-lm(RT~FamilySize+Frequency*Length, data=lexdec)\n\nvif(mod2)\n\n      FamilySize        Frequency           Length Frequency:Length \n        3.965852        22.759448        19.434428        23.051442 \n\n\nAs the interactions are introduced, the variance inflation increases. In those cases, it is recommended to center all continuous variables. Centering removes the impact on the variance.\n\nlexdec$Frequency_c &lt;- lexdec$Frequency - mean(lexdec$Frequency)\nlexdec$Length_c &lt;- lexdec$Length - mean(lexdec$Length)\n\nmod3 &lt;- lm(RT ~ Frequency_c * Length_c, data = lexdec)\nvif(mod3)\n\n         Frequency_c             Length_c Frequency_c:Length_c \n            1.234799             1.234283             1.010100",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Multicollinearity"
    ]
  },
  {
    "objectID": "assignment6_key.html",
    "href": "assignment6_key.html",
    "title": "Assignment #6 - An EEG Processing Pipeline II - Answer Key",
    "section": "",
    "text": "Task #5 (Assignment#6 / After Workgroup 6): Multiple Regression with two factors\nIn this key, we continue the exercise from assignment#5 with the final task to perform a model with interactions.\nFor the steps to create the required data frame, see the Assignment#5 key.\nThe data frame eegDataAveraged created in Assignment$5 contains the following columns:",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Assignment #6 - An EEG Processing Pipeline II - Answer Key"
    ]
  },
  {
    "objectID": "assignment6_key.html#task-5-assignment6-after-workgroup-6-multiple-regression-with-two-factors",
    "href": "assignment6_key.html#task-5-assignment6-after-workgroup-6-multiple-regression-with-two-factors",
    "title": "Assignment #6 - An EEG Processing Pipeline II - Answer Key",
    "section": "",
    "text": "Task #5.1: Model with interactions\nTo model the N400 window average voltage from the previous tasks we include now the two predictors: Condition and ROI and their interaction.\n\nmEEG_interaction&lt;-lm(AvgVoltage~Condition*roi,data=eegDataAveraged)\nsummary(mEEG_interaction)\n\n\nCall:\nlm(formula = AvgVoltage ~ Condition * roi, data = eegDataAveraged)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.8272 -0.8436  0.0566  1.0315  6.2423 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -1.2112     0.2007  -6.034 2.36e-09 ***\nConditionCondA             -0.8395     0.2839  -2.957  0.00319 ** \nConditionCondB              0.7862     0.2839   2.769  0.00573 ** \nConditionCondC              1.1510     0.2839   4.054 5.47e-05 ***\nroiCentral                 -0.4165     0.2736  -1.523  0.12821    \nConditionCondA:roiCentral   0.4119     0.3869   1.065  0.28728    \nConditionCondB:roiCentral   0.6341     0.3869   1.639  0.10156    \nConditionCondC:roiCentral   0.7439     0.3869   1.923  0.05483 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.027 on 876 degrees of freedom\nMultiple R-squared:  0.159, Adjusted R-squared:  0.1523 \nF-statistic: 23.65 on 7 and 876 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the resulting model fit, we can make the following observations:\n\nThe intercept represents the reference case: Condition D in the Anterior region.\nThe coefficients for the Condition dummy variables represent the change from the reference Condition D in the Anterior Region.\nThe coefficient for roiCentral represents the change of Condition D from the anterior to the central region.\nThe interaction coefficients represent the additional change with respect to the Condition coefficients from the Anterior to the Central region.\nThe values indicate a significant change between the Conditions and the reference, without a significant impact on whether it is in the anterior or central region, with the exception of Condition C, that shows a nearly significant difference.\n\nLet‚Äôs visualize this interaction using the ggeffects package and the predict response function:\n\nlibrary(ggeffects)\n\n# predict responses based on the fitted model:\npred_voltage&lt;-predict_response(mEEG_interaction,terms=c(\"Condition\",\"roi\"))\nplot(pred_voltage)\n\n\n\n\n\n\n\nThe plot shows that there are differences between conditions, mostly independent from the region although some larger differences are apparent in the reference Condition D and Condition C.\nTo test these differences we perform post-hoc tests. I illustrate how to do it below with emmeans package:\n\nlibrary(emmeans)\n\nemmeans(mEEG_interaction,pairwise~Condition,by=\"roi\")\n\n$emmeans\nroi = Anterior:\n Condition  emmean    SE  df lower.CL upper.CL\n CondD     -1.2112 0.201 876  -1.6052  -0.8173\n CondA     -2.0507 0.201 876  -2.4447  -1.6568\n CondB     -0.4250 0.201 876  -0.8190  -0.0311\n CondC     -0.0603 0.201 876  -0.4543   0.3337\n\nroi = Central:\n Condition  emmean    SE  df lower.CL upper.CL\n CondD     -1.6278 0.186 876  -1.9925  -1.2630\n CondA     -2.0554 0.186 876  -2.4201  -1.6906\n CondB     -0.2075 0.186 876  -0.5722   0.1573\n CondC      0.2671 0.186 876  -0.0977   0.6318\n\nConfidence level used: 0.95 \n\n$contrasts\nroi = Anterior:\n contrast      estimate    SE  df t.ratio p.value\n CondD - CondA    0.840 0.284 876   2.957  0.0168\n CondD - CondB   -0.786 0.284 876  -2.769  0.0292\n CondD - CondC   -1.151 0.284 876  -4.054  0.0003\n CondA - CondB   -1.626 0.284 876  -5.727  &lt;.0001\n CondA - CondC   -1.990 0.284 876  -7.012  &lt;.0001\n CondB - CondC   -0.365 0.284 876  -1.285  0.5728\n\nroi = Central:\n contrast      estimate    SE  df t.ratio p.value\n CondD - CondA    0.428 0.263 876   1.627  0.3640\n CondD - CondB   -1.420 0.263 876  -5.404  &lt;.0001\n CondD - CondC   -1.895 0.263 876  -7.209  &lt;.0001\n CondA - CondB   -1.848 0.263 876  -7.031  &lt;.0001\n CondA - CondC   -2.322 0.263 876  -8.836  &lt;.0001\n CondB - CondC   -0.475 0.263 876  -1.806  0.2714\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nWith this function, we asked emmeans to perform a pairwise comparison between the levels of condition (pairwise~Condition) at each of the levels of the roi variable.\nConsidering only the comparisons to the reference level D, we can observe:\n\nThere is a significant difference between Condition C and the control condition D both in Central and Anterior region.\nThere is a significant difference between Condition B and the control condition D in both Central and Anterior region.\nThe difference between Condition A and Condition D is only significant in the Anterior region (\\(D=0.84,SE=0.284,t(876)=2.957, p=.017\\)).\n\nThe last contrast indicates a potential weak interaction between roi and Condition however, this is not clearly visible in the model.\n\n\n\n\n\n\nImportant\n\n\n\nWhat do we do in these cases, when we have a non-significant interaction, but potentially significant post-hoc test?\nYou should report the interaction as non-significant or nearly significant. Using the pairwise comparisons only as a justification for the effect will be considered inadequate, and probably more power will be required in the experiment to ensure the effect is real.\n\n\nTo illustrate this last point, let‚Äôs compare the model above with roi factor and a model considering only the Condition, independent from the region on the scalp.\n\nmCond&lt;-lm(AvgVoltage~Condition, data=eegDataAveraged)\n\nanova(mEEG_interaction,mCond)\n\nAnalysis of Variance Table\n\nModel 1: AvgVoltage ~ Condition * roi\nModel 2: AvgVoltage ~ Condition\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    876 3600.5                           \n2    880 3618.5 -4   -18.015 1.0958 0.3574\n\nAIC(mEEG_interaction,mCond)\n\n                 df      AIC\nmEEG_interaction  9 3768.136\nmCond             5 3764.548\n\nBIC(mEEG_interaction,mCond)\n\n                 df      BIC\nmEEG_interaction  9 3811.196\nmCond             5 3788.470\n\n\nAll three test favor the simple model with only Condition as a predictor, so no topological effect was present.\nFinally, we check the model assumptions:\n\nqqnorm(mEEG_interaction$residuals)\nqqline(mEEG_interaction$residuals)\n\n\n\n\n\n\nshapiro.test(mEEG_interaction$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mEEG_interaction$residuals\nW = 0.94714, p-value &lt; 2.2e-16\n\nlibrary(car)\nncvTest(mEEG_interaction)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.227006, Df = 1, p = 0.63375\n\n\nThe qqplot indicates a violation of the normality of residuals. See below how you can visualize the residuals compared to a normal distribution with same mean and standard deviation.\n\nlibrary(tidyverse)\n\nx&lt;-mEEG_interaction$residuals\nggplot(data.frame(x), aes(x=x)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 fill=\"grey\", \n                 color=\"black\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(x), \n                            sd = sd(x)), \n                color = \"red\", \n                size = 1) +\n  labs(title=\"ggplot2: Histogram with Normal Curve\", \n       x=\"AvgVoltage\", \n       y=\"Density\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe data is more peaked than a normal distribution and would likely require a data transformation data filtering to truncate outliers. Very often work performed to analyze data (even published) ignore or don‚Äôt indicate a caveat on the model assumptions, which could lead to incorrect conclusions or invalid interpretation of the results.\n\n\nTask #5.2: Report the analysis\nWe could report the results as follows considering the confidence interval for the coefficients extracted with confint(mEEG_interaction)\n\nWe assessed the presence of a significant N400 ERP component in the manipulated experimental conditions (CondA, CondB and CondC) in comparison to a reference control (CondD). In order to do that we fitted a linear model to predict the measured voltage in mV averaged in the time window -250 to 450ms over a series of electrodes based on the Condition and including a topographical predictor encoding the Region of Interest (roi) to assess the location of the effect in the brain. roi factor was encoded with two levels: Anterior (corresponding to 6 frontal electrodes - F1, F3, F4, F5, F6, Fz) and Central (corresponding to 7 central electrodes - C1, C2, C3, C4, C5, C6, Cz).\nThe model explains a statistical significant and moderate proportion of variance (\\(F(7,876)=23.65,p&lt;.001,R^2=0.16,R^2_{adj}=0.15\\)). The variables were coded using treatment coding with the intercept corresponding to control Condition = D and roi = Anterior, and fitted at \\(b = -1.21,95\\%CI[-1.61,-0.82], t(876)=-6.03,p&lt;.001\\).\nThe table below lists the model parameters:\nCoefficient\n\n\n\n\n\n\n\n\n\n\nbeta\nConfidence Interval\nt\np\n\n\n\nIntercept (corresponding to Condition D, roi = Anterior)\n-1.21\n[-1.61, -0.82]\nt(876) = -6.03\np&lt;.001\n\n\nConditionCondA\n-0.84\n[-1.40, -0.28]\nt(876) = -2.96\np = .003\n\n\nConditionCondB\n0.79\n[0.23, 1.34]\nt(876) = 2.77\np = .006\n\n\nConditionCondC\n1.15\n[0.59, 1.71]\nt(876) = 4.05\np &lt;.001\n\n\nroiCentral\n-0.42\n[-0.95, 0.12]\nt(876) = -1.52\np=.128\n\n\nConditionCondA:roiCentral\n0.41\n[-0.35, 1.17]\nt(876) = 1.06\np=.287\n\n\nConditionCondB:roiCentral\n0.63\n[-0.12, 1.39]\nt(876) = 1.64\np=.102\n\n\nConditionCondC:roiCentral\n0.74\n[-0.02, 1.50]\nt(876) = 1.92\np=.054\n\n\n\nThe model did not reveal a significant interaction between roi and Condition. Comparison with a model with only Condition as predictor, confirmed that the introduction of the roi predictor and interaction did not improve the model fit (\\(F(4,880)=1.10,p=.357, AIC_{ConditionOnlyModel} = 3764.44, AIC_{interactionModel}=3768.13\\)\nA main effect of Condition was identified with all three Conditions A (\\(M=-2.05ms,SE=0.14,95\\%CI=[-2.3,-1.78]\\)), B (\\(M=-0.32ms,SE=0.14,95\\%CI=[-0.58,-0.05]\\)), C ( \\(M=0.10ms,SE=0.14,95\\%CI=[-0.16,0.37]\\)) showed a significant difference to the reference condition D (\\(M=-1.42ms,SE=0.14,95\\%CI=[-1.69,-1.15]\\)) as verified by post-hoc t-tests corrected for multiple comparisons using Tukey correction.\nThe nature of the effect, however displayed a different nature on the three conditions. While in Condition A was more negative than in the Control condition D, as expected for a N400 negativity, this was not the case in conditions B and C, where the data was more positive than in Condition A.\n\n\n\nDifference\nt\np\n\n\n\nConD - CondA\n0.63\nt(876) = 3.275\np=.006\n\n\nConD - CondB\n-1.10\nt(876) = -5.70\np &lt;.001\n\n\nConD - CondC\n-1.52\nt(876) = -7.87\np &lt;.001\n\n\n\n\n\nModel assumptions checking revealed deviations from the normality assumptions on both models that would require a more detailed look at the data outliers or likely an increase of the number of subjects in the study.",
    "crumbs": [
      "Workgroup 6: Multiple Regression with R II",
      "Assignment #6 - An EEG Processing Pipeline II - Answer Key"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baayen, R. H. 2008. Analyzing Linguistic Data: A Practical\nIntroduction to Statistics Using r. Cambridge University Press.\n\n\nField, Andy P. 2026. Discovering Statistics Using R and\nRStudio. London: SAGE Publications.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. \"\nO‚ÄôReilly Media, Inc.\".\n\n\nLenth, Russell V., and Julia Piaskowski. 2025. Emmeans: Estimated\nMarginal Means, Aka Least-Squares Means. https://rvlenth.github.io/emmeans/.\n\n\nL√ºdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M.\nWiernik, Etienne Bacher, R√©mi Th√©riault, and Dominique Makowski. 2022.\n‚ÄúEasystats: Framework for Easy Statistical Modeling,\nVisualization, and Reporting.‚Äù CRAN. https://doi.org/10.32614/CRAN.package.easystats.\n\n\nMakowski, Dominique, Mattan S. Ben-Shachar, Brenton M. Wiernik,\nIndrajeet Patil, R√©mi Th√©riault, and Daniel L√ºdecke. 2025. ‚Äúmodelbased: An R Package to Make the\nMost Out of Your Statistical Models Through Marginal Means, Marginal\nEffects, and Model Predictions.‚Äù Journal of Open Source\nSoftware 10 (109): 7969. https://doi.org/10.21105/joss.07969.\n\n\nPablos, Leticia, and Douglas Saddy. 2009. ‚ÄúNegative Polarity Items\nand Complementizer Agreement in Basque.‚Äù Brain Talk, 61.\n\n\nPluymaekers, Mark, Mirjam Ernestus, and R Harald Baayen. 2005.\n‚ÄúLexical Frequency and Acoustic Reduction in Spoken Dutch.‚Äù\nThe Journal of the Acoustical Society of America 118 (4):\n2561‚Äì69.\n\n\nRaftery, Adrian E. 1995. ‚ÄúBayesian Model Selection in Social\nResearch.‚Äù Sociological Methodology, 111‚Äì63.\n\n\nSearle, S. R., F. M. Speed, and G. A. Milliken. 1980. ‚ÄúPopulation\nMarginal Means in the Linear Model: An Alternative to Least Squares\nMeans.‚Äù The American Statistician 34 (4): 216‚Äì21. https://doi.org/10.1080/00031305.1980.10483031.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. 2nd ed. Springer Publishing Company, Incorporated.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2024.\nR for Data Science. O‚ÄôReilly.\n\n\nWilkinson, Leland. 2011. ‚ÄúThe Grammar of Graphics.‚Äù In\nHandbook of Computational Statistics: Concepts and Methods,\n375‚Äì414. Springer.\n\n\nWinter, Bodo. 2019. Statistics for Linguists: An Introduction Using\nr. Routledge.\n\n\nWinter, Bodo, Marcus Perlman, Lynn K Perry, and Gary Lupyan. 2017.\n‚ÄúWhich Words Are Most Iconic? Iconicity in English Sensory\nWords.‚Äù Interaction Studies 18 (3): 443‚Äì64.",
    "crumbs": [
      "References"
    ]
  }
]